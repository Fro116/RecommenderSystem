{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdec301e-ef42-4fbc-8889-eb6dc67d255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"../../TrainingAlphas/Transformer/Transformer.py\").read())\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import h5py\n",
    "from captum.attr import IntegratedGradients\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.filename = file\n",
    "        f = h5py.File(file, \"r\")\n",
    "        self.length = f[\"anime\"].shape[0]\n",
    "        self.embeddings = [\n",
    "            f[\"anime\"][:] - 1,\n",
    "            f[\"manga\"][:] - 1,\n",
    "            f[\"rating\"][:].reshape(*f[\"rating\"].shape, 1).astype(np.float32),\n",
    "            f[\"timestamp\"][:].reshape(*f[\"timestamp\"].shape, 1).astype(np.float32),\n",
    "            f[\"status\"][:] - 1,\n",
    "            f[\"completion\"][:].reshape(*f[\"completion\"].shape, 1).astype(np.float32),\n",
    "            f[\"position\"][:] - 1,\n",
    "        ]\n",
    "        self.mask = f[\"user\"][:]\n",
    "\n",
    "        def process_position(x):\n",
    "            return x[:].flatten().astype(np.int64) - 1\n",
    "\n",
    "        self.positions = process_position(f[\"positions\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        embeds = tuple(x[i, :] for x in self.embeddings)\n",
    "        mask = self.mask[i, :]\n",
    "        mask = mask.reshape(1, mask.size) != mask.reshape(mask.size, 1)\n",
    "        positions = self.positions[i]\n",
    "        return embeds, mask, positions\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "def get_model(medium, task):\n",
    "    device = torch.device(\"cuda\")\n",
    "    source_dir = get_data_path(\n",
    "        os.path.join(\"alphas\", medium, task, \"Transformer\", \"v1\")\n",
    "    )\n",
    "    model_file = os.path.join(source_dir, \"model.pt\")\n",
    "    config_file = os.path.join(source_dir, \"config.json\")\n",
    "    training_config = create_training_config(config_file, 1)\n",
    "    model_config = create_model_config(training_config)\n",
    "    model = TransformerModel(model_config)\n",
    "    model.load_state_dict(load_model(model_file, map_location=\"cpu\"))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_data(username, medium, task):\n",
    "    device = torch.device(\"cuda\")\n",
    "    outdir = get_data_path(\n",
    "        f\"recommendations/{username}/alphas/{medium}/{task}/Transformer/v1\"\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        InferenceDataset(os.path.join(outdir, \"inference.h5\")),\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    it = iter(dataloader)\n",
    "    data = tuple(next(it))\n",
    "    return to_device(data, device)\n",
    "\n",
    "\n",
    "def get_baseline(medium, task, inputs, positions, fields):\n",
    "    # load configs\n",
    "    source_dir = get_data_path(\n",
    "        os.path.join(\"alphas\", medium, task, \"Transformer\", \"v1\")\n",
    "    )\n",
    "    config_file = os.path.join(source_dir, \"config.json\")\n",
    "    config = json.load(open(config_file, \"r\"))\n",
    "    training_config = create_training_config(config_file, 1)\n",
    "\n",
    "    # get mask tokens\n",
    "    assert len(training_config[\"vocab_types\"]) == 8\n",
    "    assert training_config[\"vocab_types\"][6] == None\n",
    "    mask_tokens = config[\"mask_tokens\"][:6] + config[\"mask_tokens\"][7:]\n",
    "    empty_tokens = config[\"empty_tokens\"][:6] + config[\"mask_tokens\"][7:]    \n",
    "    vocab_types = (\n",
    "        training_config[\"vocab_types\"][:6] + training_config[\"vocab_types\"][7:]\n",
    "    )\n",
    "    for i in range(len(vocab_types)):\n",
    "        mask_tokens[i] -= vocab_types[i] == int\n",
    "        empty_tokens[i] -= vocab_types[i] == int\n",
    "\n",
    "    # # mask out item inputs\n",
    "    # baseline is a new user with no watched items\n",
    "    end_of_seq_pos = positions.cpu().numpy()[0]\n",
    "    baseline_inputs = [x.clone() for x in inputs]\n",
    "    for i in fields:\n",
    "        for j in range(1, end_of_seq_pos):\n",
    "            if baseline_inputs[0][0, j] != empty_tokens[i]:\n",
    "                baseline_inputs[i][0, j] = np.random.randint(mask_tokens[i])\n",
    "                # baseline_inputs[i][0, j] = mask_tokens[i]\n",
    "    return baseline_inputs\n",
    "    \n",
    "\n",
    "def cpu(x):\n",
    "    return x.detach().cpu().numpy().squeeze().tolist()\n",
    "\n",
    "\n",
    "def utility_model(\n",
    "    anime_embedding,\n",
    "    manga_embedding,\n",
    "    rating_embedding,\n",
    "    timestamp_embedding,\n",
    "    status_embedding,\n",
    "    completion_embedding,\n",
    "    position_embedding,\n",
    "    model=None,\n",
    "    positions=None,\n",
    "    mask=None,\n",
    "    coefs=None,\n",
    "    medium=None,\n",
    "):\n",
    "    embedding = (\n",
    "        anime_embedding\n",
    "        + manga_embedding\n",
    "        + rating_embedding\n",
    "        + timestamp_embedding\n",
    "        + status_embedding\n",
    "        + completion_embedding\n",
    "        + position_embedding\n",
    "    )\n",
    "    embedding = model.embed.postprocessor(embedding)\n",
    "    hidden = model.transformers(embedding, mask)\n",
    "    output = hidden[range(len(positions)), positions, :]\n",
    "    if medium == \"anime\":\n",
    "        idxp = 0\n",
    "        idxr = 1\n",
    "    elif medium == \"manga\":\n",
    "        idxp = 2\n",
    "        idxr = 3\n",
    "    else:\n",
    "        assert False\n",
    "    p = model.classifier[idxp](output)\n",
    "    r = model.classifier[idxr](output)\n",
    "    # transform into MLE utility\n",
    "    p = torch.exp(p)\n",
    "    r = torch.relu((r + 10) / 10)\n",
    "    u = (\n",
    "        np.exp(coefs[0]) * p\n",
    "        + np.exp(coefs[1]) * r\n",
    "        + np.exp(coefs[2]) * torch.log(p)\n",
    "        + np.exp(coefs[3]) * torch.log(r)\n",
    "    )\n",
    "    return u\n",
    "\n",
    "def zero_out(x, pos):\n",
    "    x[:, 1:pos, :] = 0\n",
    "    return x\n",
    "\n",
    "def identity(x, pos):\n",
    "    return x\n",
    "\n",
    "\n",
    "def compute_attributions(username, medium, task, coefs, items, fields):\n",
    "    model = get_model(medium, task)\n",
    "    inputs, mask, positions = get_data(username, medium, task)\n",
    "    baseline_inputs = get_baseline(medium, task, inputs, positions, fields)\n",
    "\n",
    "    embedding = tuple(embed(x) for (embed, x) in zip(model.embed.embeddings, inputs))\n",
    "    baseline_embedding = tuple(\n",
    "        embed(x) for (embed, x) in zip(model.embed.embeddings, baseline_inputs)\n",
    "    )\n",
    "\n",
    "    ig = IntegratedGradients(utility_model)\n",
    "    attributions = {}\n",
    "    for item in items:\n",
    "        attrs = ig.attribute(\n",
    "            embedding,\n",
    "            baseline_embedding,\n",
    "            target=item,\n",
    "            internal_batch_size=8,\n",
    "            additional_forward_args=(model, positions, mask, coefs, medium),\n",
    "            n_steps=20,\n",
    "        )\n",
    "        return cpu(sum(attrs[:2]).sum(dim=2))        \n",
    "        # return cpu(sum(attrs).sum(dim=2))\n",
    "        attrs = sum(attrs)\n",
    "        attributions[item] = cpu(attrs.sum(dim=2))\n",
    "    return attributions, cpu(inputs[0]), cpu(inputs[1])\n",
    "\n",
    "\n",
    "def save_attributions(username, medium, task, coefs, items):\n",
    "    cache = {}\n",
    "    attrs, anime, manga = compute_attributions(username, medium, task, coefs, items)\n",
    "    cache[\"anime\"] = anime\n",
    "    cache[\"manga\"] = manga\n",
    "    cache[\"attributions\"] = attrs\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f3f45b-f4f8-416e-bfad-65ad1a7f34b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2092076523.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    function get_perturbations(medium)\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e2ea28d-3088-493c-9721-4be22864a4b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "username = \"Fro116\"\n",
    "medium = \"anime\"\n",
    "task = \"temporal_causal\"\n",
    "coefs = [-5.481506, 1.2974571, -1.084365, 0.24383727]\n",
    "# save_attributions(username, medium, task, coefs, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac36aff-c65c-4f3f-8448-ac99db100922",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, mask, positions = get_data(username, medium, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368a5267-1dbb-4cb8-b4ed-9b44f4ca2ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6661, device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0][0, 585]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab08cebd-795f-4c9e-a9d4-a3bd6122bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "bocchi = 6437\n",
    "kanojo = 22311\n",
    "ojamajo = 5802\n",
    "bunny = 19765\n",
    "tengoku = 5667\n",
    "fullmoon = 8706\n",
    "touch = 23762\n",
    "items = [touch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4038117-9b22-43d0-8b3f-93cf328e3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bunny -> bunny\n",
    "# kanojo -> kanojo\n",
    "# full moon -> kodocha (boo!)\n",
    "# bocchi -> akb0048 \n",
    "# ojamajo -> aria\n",
    "# touch -> kodocha (boo!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "226398d3-7879-4767-92a1-0ed55f9610ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime_attrs = compute_attributions(username, medium, task, coefs, items, [0, 1, 2, 3, 4, 5, 6])\n",
    "# rating_attrs = compute_attributions(username, medium, task, coefs, items, [2])\n",
    "# ts_attrs = compute_attributions(username, medium, task, coefs, items, [3])\n",
    "# pos_attrs = compute_attributions(username, medium, task, coefs, items, [1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42817b38-b646-4c8a-be74-318a010c1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1cdf74-c1c3-4d0f-8c3c-310122cf44ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:46<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "attrs = []\n",
    "for i in tqdm(range(30)):\n",
    "    attrs.append(compute_attributions(username, medium, task, coefs, items, [0, 1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c52738-a6a7-4202-af93-013aa9da0853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(821, device='cuda:0'),\n",
       " tensor(24957, device='cuda:0'),\n",
       " tensor(11349, device='cuda:0'),\n",
       " tensor(11961, device='cuda:0'),\n",
       " tensor(6040, device='cuda:0')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_attrs = sum(np.array(x) for x in attrs)\n",
    "[inputs[0][0, np.argsort(avg_attrs)[-1-x]] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee8045d-d20e-47df-9259-7146afd4fdce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(66823, device='cuda:0'),\n",
       " tensor(66823, device='cuda:0'),\n",
       " tensor(66823, device='cuda:0'),\n",
       " tensor(66823, device='cuda:0'),\n",
       " tensor(66823, device='cuda:0')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inputs[1][0, np.argsort(avg_attrs)[-1-x]] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cbbd9db-e5de-4aea-a52e-e68ad7a2400f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05014963, -0.04879337, -0.04778021, ...,  0.05706015,\n",
       "         0.06754141,  0.10395877],\n",
       "       [-0.16695583, -0.09790334, -0.06586245, ...,  0.04741236,\n",
       "         0.05375742,  0.14209028],\n",
       "       [-0.17932507, -0.08462202, -0.05074595, ...,  0.09995314,\n",
       "         0.15306959,  0.26410297],\n",
       "       ...,\n",
       "       [-0.14195417, -0.08618962, -0.0652843 , ...,  0.07789235,\n",
       "         0.08925848,  0.16869851],\n",
       "       [-0.04940343, -0.04839007, -0.03370804, ...,  0.06859194,\n",
       "         0.08550637,  0.12960392],\n",
       "       [-0.14423118, -0.07191206, -0.06692154, ...,  0.06403629,\n",
       "         0.06978307,  0.09930972]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(attrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
