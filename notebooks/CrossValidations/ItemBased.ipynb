{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "@functools.wraps(smf.ols)\n",
    "def lm(*args, **kwargs):\n",
    "    return smf.ols(*args, **kwargs).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f2112377eb58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UserAnimeList.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raw_df = pd.read_csv(\"UserAnimeList.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = raw_df[[\"username\", \"anime_id\", \"my_score\"]].loc[\n",
    "    lambda x: x[\"my_score\"] != 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(file, username):\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    xml_data = open(file, \"r\").read()  # Read file\n",
    "    root = ET.XML(xml_data)  # Parse XML\n",
    "\n",
    "    data = []\n",
    "    cols = []\n",
    "    for i, child in enumerate(root):\n",
    "        data.append([subchild.text for subchild in child])\n",
    "        cols.append(child.tag)\n",
    "    new_list = pd.DataFrame(data).T\n",
    "    new_list.columns = cols\n",
    "\n",
    "    df = new_list.loc[[0, 9]].T.dropna().rename({0: \"anime_id\", 9: \"my_score\"}, axis=1)\n",
    "    df[\"username\"] = username\n",
    "    df[\"anime_id\"] = df[\"anime_id\"].astype(int)\n",
    "    df[\"my_score\"] = df[\"my_score\"].astype(int)\n",
    "    df[\"username\"] = df[\"username\"].astype(str)\n",
    "    df = df.loc[lambda x: x[\"my_score\"] != 0]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_user(full_df, xml_file, username):\n",
    "    user_df = read_xml(xml_file, username)\n",
    "    without_user = full_df.loc[lambda x: x[\"username\"] != username]\n",
    "    return pd.concat([without_user, user_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = add_user(filtered_df, \"user_profiles/Fro116.xml\", \"Fro116\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rating = filtered_df[\"my_score\"].mean()\n",
    "user_bias = (\n",
    "    pd.DataFrame(filtered_df.groupby(\"username\")[\"my_score\"].mean()).rename(\n",
    "        {\"my_score\": \"user_bias\"}, axis=1\n",
    "    )\n",
    "    - average_rating\n",
    ")\n",
    "anime_bias = (\n",
    "    pd.DataFrame(filtered_df.groupby(\"anime_id\")[\"my_score\"].mean()).rename(\n",
    "        {\"my_score\": \"anime_bias\"}, axis=1\n",
    "    )\n",
    "    - average_rating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.merge(anime_bias, on=[\"anime_id\"]).merge(\n",
    "    user_bias, on=[\"username\"]\n",
    ")\n",
    "filtered_df[\"normalized_score\"] = (\n",
    "    filtered_df[\"my_score\"]\n",
    "    - filtered_df[\"anime_bias\"]\n",
    "    - filtered_df[\"user_bias\"]\n",
    "    - average_rating\n",
    ")\n",
    "filtered_df[\"orig_normalized_score\"] = filtered_df[\"normalized_score\"]\n",
    "filtered_df = filtered_df.set_index(\"username\")\n",
    "filtered_df = filtered_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prediction(recommendee, neighborhood):\n",
    "    pred_df = pd.DataFrame()\n",
    "    pred_df[\"delta\"] = neighborhood.groupby(\"anime_id\").apply(\n",
    "        lambda x: np.dot(x[\"normalized_score\"], x[\"corr\"]) / x[\"corr\"].abs().sum()\n",
    "    )\n",
    "    pred_df[\"blp\"] = anime_bias + user_bias.loc[recommendee].squeeze() + average_rating\n",
    "    pred_df = pred_df.dropna()\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squared_error(df, pred_df, recommendee):\n",
    "    recommendee_df = pred_df.loc[\n",
    "        pred_df.index.intersection(df.loc[recommendee].anime_id)\n",
    "    ]\n",
    "    recommendee_df = recommendee_df.merge(\n",
    "        df.loc[recommendee].set_index(\"anime_id\")[\"my_score\"], on=\"anime_id\"\n",
    "    )\n",
    "    errors = recommendee_df[\"my_score\"] - recommendee_df[\"score\"]\n",
    "    return np.dot(errors, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(\n",
    "    is_df, oos_df, recommendee, neighborhood_sizes, full_neighborhoods\n",
    "):\n",
    "\n",
    "    all_corrs = pickle.load(open(\"item_correlations/correlations.pkl\", \"rb\"))\n",
    "    all_corrs[\"similarity\"] = all_corrs[\"corr\"].abs()\n",
    "    all_corrs = all_corrs.dropna()\n",
    "    all_corrs = all_corrs.loc[\n",
    "        lambda x: x.index.get_level_values(\"anime_id_x\")\n",
    "        != x.index.get_level_values(\"anime_id_y\")\n",
    "    ]\n",
    "\n",
    "    anime_var = (\n",
    "        pd.DataFrame(is_df.groupby(\"anime_id\")[\"normalized_score\"].var())\n",
    "        .rename({\"normalized_score\": \"anime_var\"}, axis=1)\n",
    "        .dropna()\n",
    "    )\n",
    "    user_var = (\n",
    "        pd.DataFrame(is_df.groupby(\"username\")[\"normalized_score\"].var())\n",
    "        .rename({\"normalized_score\": \"user_var\"}, axis=1)\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "    for neighborhood_size in neighborhood_sizes:\n",
    "        # extract model features\n",
    "        corrs = all_corrs.copy()\n",
    "        if full_neighborhoods:\n",
    "            corrs = corrs.groupby(\"anime_id_x\").apply(\n",
    "                lambda x: x.sort_values(by=\"similarity\")\n",
    "            )\n",
    "        else:\n",
    "            corrs = corrs.groupby(\"anime_id_x\").apply(\n",
    "                lambda x: x.sort_values(by=\"similarity\")[-neighborhood_size:]\n",
    "            )\n",
    "        corrs.index = corrs.index.droplevel()\n",
    "\n",
    "        score = is_df.loc[recommendee].merge(\n",
    "            corrs.reset_index(\"anime_id_x\"), left_on=\"anime_id\", right_on=\"anime_id_y\"\n",
    "        )\n",
    "        score[\"user_var\"] = user_var.loc[\"Fro116\"].squeeze()\n",
    "        score = score.merge(anime_var, on=\"anime_id\")\n",
    "        score = score.drop(\"anime_id\", axis=1).rename(\n",
    "            {\"anime_id_x\": \"anime_id\"}, axis=1\n",
    "        )\n",
    "\n",
    "        if full_neighborhoods:\n",
    "            score = (\n",
    "                score.groupby(\"anime_id\")\n",
    "                .apply(lambda x: x.sort_values(by=\"similarity\")[-neighborhood_size:])\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "        pred_df = prepare_prediction(recommendee, score)\n",
    "\n",
    "        # train linear model\n",
    "        recomendee_seen_shows = is_df.loc[recommendee].merge(pred_df, on=[\"anime_id\"])\n",
    "        recomendee_seen_shows[\"target\"] = (\n",
    "            recomendee_seen_shows[\"my_score\"] - recomendee_seen_shows[\"blp\"]\n",
    "        )\n",
    "        model = lm(\"target ~ delta + 0\", recomendee_seen_shows)\n",
    "\n",
    "        # inference\n",
    "        pred_df[\"score\"] = model.predict(pred_df) + pred_df[\"blp\"]\n",
    "        is_pred_df = pred_df.loc[\n",
    "            lambda x: x.index.isin(is_df.loc[recommendee].anime_id)\n",
    "        ]\n",
    "        oos_pred_df = pred_df.loc[lambda x: x.index.isin(oos_df.anime_id)]\n",
    "\n",
    "        # compute coverage\n",
    "        is_coverage = len(is_pred_df) / len(is_df.loc[recommendee])\n",
    "        oos_coverage = len(oos_pred_df) / len(oos_df)\n",
    "\n",
    "        # compute rmse\n",
    "        missing_is = is_df.loc[recommendee].loc[\n",
    "            lambda x: ~x.anime_id.isin(is_pred_df.index)\n",
    "            & ~x.anime_id.isin(oos_df.anime_id)\n",
    "        ]\n",
    "        missing_oos = oos_df.loc[lambda x: ~x.anime_id.isin(oos_pred_df.index)]\n",
    "        is_se = get_squared_error(is_df, is_pred_df, recommendee)\n",
    "        oos_se = get_squared_error(oos_df, oos_pred_df, recommendee)\n",
    "        missing_is_se = np.dot(\n",
    "            missing_is[\"orig_normalized_score\"], missing_is[\"orig_normalized_score\"]\n",
    "        )\n",
    "        missing_oos_se = np.dot(\n",
    "            missing_oos[\"orig_normalized_score\"], missing_oos[\"orig_normalized_score\"]\n",
    "        )\n",
    "        is_rmse = np.sqrt((is_se + missing_is_se) / len(is_df.loc[recommendee]))\n",
    "        oos_rmse = np.sqrt((oos_se + missing_oos_se) / len(oos_df))\n",
    "        metrics = metrics.append(\n",
    "            {\n",
    "                \"neighborhood_size\": neighborhood_size,\n",
    "                \"full_neighborhoods\": full_neighborhoods,\n",
    "                \"is_rmse\": is_rmse,\n",
    "                \"is_coverage\": is_coverage,\n",
    "                \"oos_rmse\": oos_rmse,\n",
    "                \"oos_coverage\": oos_coverage,\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "recommendee = \"Fro116\"\n",
    "K = 10\n",
    "base = np.sqrt(2)\n",
    "\n",
    "errors_by_neighborhood_size = []\n",
    "max_size = len(filtered_df.anime_id.unique())\n",
    "neighborhood_sizes = [\n",
    "    int(base ** i) for i in range(int(np.log(max_size) / np.log(base)) + 1)\n",
    "] + [max_size]\n",
    "neighborhood_sizes = sorted(list(set(neighborhood_sizes)))\n",
    "splits = np.array_split(filtered_df.loc[recommendee].sample(frac=1), K)\n",
    "for split in splits:\n",
    "    display(split.head())\n",
    "    \n",
    "for split in tqdm(splits):\n",
    "    oos_df = split\n",
    "    is_df = filtered_df.loc[\n",
    "        lambda x: ~(\n",
    "            (x.index.get_level_values(\"username\") == recommendee)\n",
    "            & x.anime_id.isin(oos_df.anime_id)\n",
    "        )\n",
    "    ]\n",
    "    # we take copies as a safety precaution\n",
    "    errors_by_neighborhood_size.append(\n",
    "        compute_accuracy_metrics(\n",
    "            is_df.copy(), oos_df.copy(), \"Fro116\", neighborhood_sizes, False\n",
    "        )\n",
    "    )\n",
    "    errors_by_neighborhood_size.append(\n",
    "        compute_accuracy_metrics(\n",
    "            is_df.copy(), oos_df.copy(), \"Fro116\", neighborhood_sizes, True\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors = pd.concat(errors_by_neighborhood_size, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_data = pd.melt(allerrors, [\"neighborhood_size\", \"full_neighborhoods\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_data.loc[lambda x: x.full_neighborhoods == True, \"variable\"] = (\n",
    "    \"full_\" + wide_data.loc[lambda x: x.full_neighborhoods == True, \"variable\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "_ = sns.lineplot(\n",
    "    x=\"neighborhood_size\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    data=wide_data.loc[lambda x: x.variable.str.contains(\"coverage\")],\n",
    ").set(xscale=\"log\", title=\"Prediction Coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "_ = sns.lineplot(\n",
    "    x=\"neighborhood_size\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    data=wide_data.loc[lambda x: x.variable.str.contains(\"rmse\")],\n",
    ").set(xscale=\"log\", title=\"Root Mean Squared Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "allerrors.groupby([\"full_neighborhoods\", \"neighborhood_size\"]).mean().sort_values(\n",
    "    by=\"oos_rmse\"\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My takeaway from this is that it doesn't matter if we normalize variance or not\n",
    "# For simplicity, let's forgo normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "allerrors.groupby([\"full_neighborhoods\", \"neighborhood_size\"]).mean().xs(\n",
    "    0, level=\"full_neighborhoods\"\n",
    ").sort_values(by=\"oos_rmse\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "allerrors.groupby([\"full_neighborhoods\", \"neighborhood_size\"]).mean().xs(\n",
    "    1, level=\"full_neighborhoods\"\n",
    ").sort_values(by=\"oos_rmse\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
