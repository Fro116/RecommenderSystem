{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Document\n",
    "# TODO: Cleanup\n",
    "* Note, this script may take several days to run\n",
    "* TODO: Document the related-signal residualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS PARAMETER\n",
    "username = \"taapaye\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "\n",
    "@functools.wraps(smf.ols)\n",
    "def lm(*args, **kwargs):\n",
    "    return smf.ols(*args, **kwargs).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(f\"../../data/recommendations/{username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = pickle.load(open(\"../../processed_data/user_anime_lists.pkl\", \"rb\"))\n",
    "user_df = pickle.load(open(\"user_anime_list.pkl\", \"rb\"))\n",
    "filtered_df = filtered_df.loc[lambda x: x['username'].str.lower() != username.lower()]\n",
    "filtered_df = pd.concat([filtered_df, user_df], ignore_index=True)\n",
    "filtered_df = filtered_df.set_index(\"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_df = pd.DataFrame()\n",
    "residual_df['anime_id'] = user_df['anime_id']\n",
    "residual_df['residual'] = 0\n",
    "residual_df = residual_df.set_index('anime_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache()\n",
    "def get_item_corrs(nonneg_corrs=False):\n",
    "    corrs = pickle.load(open(\"../../processed_data/item_correlations.pkl\", \"rb\"))\n",
    "    return corrs\n",
    "\n",
    "\n",
    "def get_item_corrs_wrapper(df, username):\n",
    "    return get_item_corrs()\n",
    "\n",
    "\n",
    "def get_item_scores(df, corrs, username, neighborhood_size):\n",
    "    corrs = corrs.groupby(\"anime_id_x\").tail(neighborhood_size)\n",
    "    score = df.loc[username].merge(\n",
    "        corrs.reset_index(\"anime_id_x\"), left_on=\"anime_id\", right_on=\"anime_id_y\",\n",
    "    )\n",
    "    score = score.drop(\"anime_id\", axis=1).rename({\"anime_id_x\": \"anime_id\"}, axis=1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_corrs(df, username):\n",
    "    user_subset = df.loc[[username]].merge(df.reset_index(), on=\"anime_id\")\n",
    "    corr_numerator = user_subset.groupby(\"username\").apply(\n",
    "        lambda x: np.dot(x[\"score_x\"], x[\"score_y\"])\n",
    "    )\n",
    "    corr_denom = df.groupby(\"username\").apply(\n",
    "        lambda x: np.sqrt(np.dot(x[\"score\"], x[\"score\"]))\n",
    "    )\n",
    "    corr_denom *= corr_denom.loc[username]\n",
    "    corrs = pd.DataFrame((corr_numerator / corr_denom), columns=[\"corr\"])\n",
    "    corrs[\"similarity\"] = corrs[\"corr\"].abs()\n",
    "    corrs[\"corr_size\"] = user_subset.groupby(\"username\").size()\n",
    "    corrs = corrs.drop(username)\n",
    "    corrs = corrs.dropna()\n",
    "    corrs = corrs.loc[lambda x: x[\"corr_size\"] > 2]\n",
    "    corrs = corrs.sort_values(by=\"similarity\")\n",
    "    return corrs\n",
    "\n",
    "\n",
    "def get_user_scores(df, corrs, recommendee, neighborhood_size):\n",
    "    corrs = corrs[-neighborhood_size:]\n",
    "    score = (df.merge(pd.DataFrame(corrs), on=\"username\")).dropna()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta(score):\n",
    "    return score.groupby(\"anime_id\").apply(\n",
    "        lambda x: np.dot(x[\"score\"], x[\"corr\"]) / x[\"corr\"].abs().sum()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squared_error(df, pred_df, username):\n",
    "    pred_df = pred_df.loc[pred_df.index.intersection(df.loc[username].anime_id)]\n",
    "    pred_df = pred_df.merge(\n",
    "        df.loc[username].set_index(\"anime_id\")[\"score\"], on=\"anime_id\"\n",
    "    )\n",
    "    errors = pred_df[\"pred_score\"] - pred_df[\"score\"]\n",
    "    return np.dot(errors, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(\n",
    "    is_df, oos_df, score_fn, corrs_fn, username, neighborhood_sizes,\n",
    "):\n",
    "    corrs = corrs_fn(is_df, username)\n",
    "    metrics = pd.DataFrame()\n",
    "    for neighborhood_size in tqdm(\n",
    "        reversed(sorted(neighborhood_sizes)), total=len(neighborhood_sizes),\n",
    "    ):\n",
    "        score = score_fn(is_df, corrs, username, neighborhood_size)\n",
    "        pred_df = pd.DataFrame()\n",
    "        pred_df[\"residual\"] = residual_df['residual']        \n",
    "        pred_df[\"delta\"] = get_delta(score)\n",
    "        pred_df = pred_df.fillna(0)\n",
    "\n",
    "        # train linear model\n",
    "        seen_shows = is_df.loc[username].merge(pred_df, on='anime_id')\n",
    "        model = lm(\"score ~ delta + residual\", seen_shows)\n",
    "\n",
    "        # inference\n",
    "        pred_df[\"pred_score\"] = model.predict(pred_df)\n",
    "        is_pred_df = pred_df.loc[lambda x: x.index.isin(is_df.loc[username].anime_id)]\n",
    "        oos_pred_df = pred_df.loc[lambda x: x.index.isin(oos_df.anime_id)]\n",
    "\n",
    "        # compute coverage\n",
    "        is_coverage = len(is_pred_df.loc[lambda x: ~np.isclose(x['delta'], 0)]) / len(is_df.loc[username])\n",
    "        oos_coverage = len(oos_pred_df.loc[lambda x: ~np.isclose(x['delta'], 0)]) / len(oos_df)\n",
    "\n",
    "        # compute rmse\n",
    "        # TODO delete the 'missing' components as nothing should be missing anymore\n",
    "        missing_is = is_df.loc[username].loc[\n",
    "            lambda x: ~x.anime_id.isin(is_pred_df.index)\n",
    "            & ~x.anime_id.isin(oos_df.anime_id)\n",
    "        ]\n",
    "        missing_oos = oos_df.loc[lambda x: ~x.anime_id.isin(oos_pred_df.index)]\n",
    "        is_se = get_squared_error(is_df, is_pred_df, username)\n",
    "        oos_se = get_squared_error(oos_df, oos_pred_df, username)\n",
    "        missing_is_se = np.dot(missing_is[\"score\"], missing_is[\"score\"])\n",
    "        missing_oos_se = np.dot(missing_oos[\"score\"], missing_oos[\"score\"])\n",
    "        is_rmse = np.sqrt((is_se + missing_is_se) / len(is_df.loc[username]))\n",
    "        oos_rmse = np.sqrt((oos_se + missing_oos_se) / len(oos_df))\n",
    "        metrics = metrics.append(\n",
    "            {\n",
    "                \"neighborhood_size\": neighborhood_size,\n",
    "                \"is_rsquared\": model.rsquared_adj,\n",
    "                \"is_rmse\": is_rmse,\n",
    "                \"is_coverage\": is_coverage,\n",
    "                \"oos_rmse\": oos_rmse,\n",
    "                \"oos_coverage\": oos_coverage,\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/29 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "base = np.sqrt(2)\n",
    "\n",
    "errors_by_neighborhood_size = []\n",
    "item_max_size = len(filtered_df.anime_id.unique())\n",
    "item_neighborhood_sizes = [\n",
    "    int(base ** i) for i in range(int(np.log(item_max_size) / np.log(base)) + 1)\n",
    "] + [item_max_size]\n",
    "user_max_size = len(filtered_df.index.unique())\n",
    "user_neighborhood_sizes = [\n",
    "    int(base ** i) for i in range(int(np.log(user_max_size) / np.log(base)) + 1)\n",
    "] + [user_max_size]\n",
    "splits = np.array_split(filtered_df.loc[username].sample(frac=1), K)\n",
    "\n",
    "for split in splits:\n",
    "    oos_df = split\n",
    "    is_df = filtered_df.loc[\n",
    "        lambda x: ~(\n",
    "            (x.index.get_level_values(\"username\") == username)\n",
    "            & x.anime_id.isin(oos_df.anime_id)\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    item_metrics = compute_accuracy_metrics(\n",
    "        is_df.copy(),\n",
    "        oos_df.copy(),\n",
    "        get_item_scores,\n",
    "        get_item_corrs_wrapper,\n",
    "        username,\n",
    "        item_neighborhood_sizes,\n",
    "    )\n",
    "    item_metrics[\"signal\"] = \"item\"\n",
    "    errors_by_neighborhood_size.append(item_metrics)    \n",
    "\n",
    "    user_metrics = compute_accuracy_metrics(\n",
    "        is_df.copy(),\n",
    "        oos_df.copy(),\n",
    "        get_user_scores,\n",
    "        get_user_corrs,\n",
    "        username,\n",
    "        user_neighborhood_sizes,\n",
    "    )\n",
    "    user_metrics[\"signal\"] = \"user\"\n",
    "    errors_by_neighborhood_size.append(user_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors = pd.concat(errors_by_neighborhood_size, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_data = pd.melt(allerrors, [\"neighborhood_size\", \"signal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for signal in wide_data[\"signal\"].unique():\n",
    "    wide_data.loc[lambda x: x[\"signal\"] == signal, \"variable\"] = (\n",
    "        f\"{signal}_\" + wide_data.loc[lambda x: x[\"signal\"] == signal, \"variable\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "_ = sns.lineplot(\n",
    "    x=\"neighborhood_size\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    data=wide_data.loc[lambda x: x.variable.str.contains(\"coverage\")],\n",
    ").set(xscale=\"log\", title=\"Prediction Coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "_ = sns.lineplot(\n",
    "    x=\"neighborhood_size\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    data=wide_data.loc[lambda x: x.variable.str.contains(\"rmse\")],\n",
    ").set(xscale=\"log\", title=\"Root Mean Squared Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors.groupby([\"signal\", \"neighborhood_size\"]).mean().sort_values(\n",
    "    by=\"oos_rmse\"\n",
    ").head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors.groupby([\"signal\", \"neighborhood_size\"]).mean().sort_values(\n",
    "    by=\"oos_rmse\"\n",
    ").reset_index().groupby(\"signal\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"parameters\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "os.chdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors.groupby([\"signal\", \"neighborhood_size\"]).mean().sort_values(\n",
    "    by=\"oos_rmse\"\n",
    ").reset_index().groupby(\"signal\").first().to_pickle(\"neighborhoodcf.best.pkl\")\n",
    "allerrors.to_pickle(\"neighborhoodcf.all.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
