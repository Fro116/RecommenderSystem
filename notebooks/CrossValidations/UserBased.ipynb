{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Document\n",
    "# TODO: Cleanup\n",
    "# TODO: Merge with ItemCF if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS PARAMETER\n",
    "recommendee = \"taapaye\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "@functools.wraps(smf.ols)\n",
    "def lm(*args, **kwargs):\n",
    "    return smf.ols(*args, **kwargs).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(f\"../../data/recommendations/{recommendee}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pickle.load(open(\"../../processed_data/user_anime_lists.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pickle.load(open(\"user_anime_list.pkl\", \"rb\"))\n",
    "filtered_df = filtered_df.loc[lambda x: ~x[\"username\"].isin(user_df.username)]\n",
    "filtered_df = pd.concat([filtered_df, user_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.set_index(\"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_rating = filtered_df[\"my_score\"].mean()\n",
    "# user_bias = (\n",
    "#     pd.DataFrame(filtered_df.groupby(\"username\")[\"my_score\"].mean()).rename(\n",
    "#         {\"my_score\": \"user_bias\"}, axis=1\n",
    "#     )\n",
    "#     - average_rating\n",
    "# )\n",
    "# anime_bias = (\n",
    "#     pd.DataFrame(filtered_df.groupby(\"anime_id\")[\"my_score\"].mean()).rename(\n",
    "#         {\"my_score\": \"anime_bias\"}, axis=1\n",
    "#     )\n",
    "#     - average_rating\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = filtered_df.merge(anime_bias, on=[\"anime_id\"]).merge(\n",
    "#     user_bias, on=[\"username\"]\n",
    "# )\n",
    "# filtered_df[\"normalized_score\"] = (\n",
    "#     filtered_df[\"my_score\"]\n",
    "#     - filtered_df[\"anime_bias\"]\n",
    "#     - filtered_df[\"user_bias\"]\n",
    "#     - average_rating\n",
    "# )\n",
    "# filtered_df[\"orig_normalized_score\"] = filtered_df[\"normalized_score\"]\n",
    "# filtered_df = filtered_df.set_index(\"username\")\n",
    "# filtered_df = filtered_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prediction(recommendee, neighborhood):\n",
    "    pred_df = pd.DataFrame()\n",
    "    pred_df[\"delta\"] = neighborhood.groupby(\"anime_id\").apply(\n",
    "        lambda x: np.dot(x[\"score\"], x[\"corr\"]) / x[\"corr\"].abs().sum()\n",
    "    )\n",
    "    pred_df = pred_df.dropna()\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(df, recommendee):\n",
    "    user_subset = df.loc[[recommendee]].merge(df.reset_index(), on=\"anime_id\")\n",
    "    adj_cos_corr_numerator = user_subset.groupby(\"username\").apply(\n",
    "        lambda x: np.dot(x[\"score_x\"], x[\"score_y\"])\n",
    "    )\n",
    "    adj_cos_corr_denom = df.groupby(\"username\").apply(\n",
    "        lambda x: np.sqrt(np.dot(x[\"score\"], x[\"score\"]))\n",
    "    )\n",
    "    adj_cos_corr_denom *= adj_cos_corr_denom.loc[recommendee]\n",
    "    adj_cos_corrs = pd.DataFrame(\n",
    "        (adj_cos_corr_numerator / adj_cos_corr_denom), columns=[\"corr\"]\n",
    "    )\n",
    "    adj_cos_corrs = adj_cos_corrs.dropna()\n",
    "    return adj_cos_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squared_error(df, pred_df, recommendee):\n",
    "    recommendee_df = pred_df.loc[\n",
    "        pred_df.index.intersection(df.loc[recommendee].anime_id)\n",
    "    ]\n",
    "    recommendee_df = recommendee_df.merge(\n",
    "        df.loc[recommendee].set_index(\"anime_id\")[\"score\"], on=\"anime_id\"\n",
    "    )\n",
    "    errors = recommendee_df[\"pred_score\"] - recommendee_df[\"score\"]\n",
    "    return np.dot(errors, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_id</th>\n",
       "      <th>score</th>\n",
       "      <th>score_var</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>username</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>karthiga</th>\n",
       "      <td>21</td>\n",
       "      <td>0.605479</td>\n",
       "      <td>2.031881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>karthiga</th>\n",
       "      <td>59</td>\n",
       "      <td>-0.474160</td>\n",
       "      <td>1.593917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>karthiga</th>\n",
       "      <td>74</td>\n",
       "      <td>-0.750239</td>\n",
       "      <td>1.456926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>karthiga</th>\n",
       "      <td>120</td>\n",
       "      <td>-0.743815</td>\n",
       "      <td>1.658394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>karthiga</th>\n",
       "      <td>178</td>\n",
       "      <td>-0.206618</td>\n",
       "      <td>1.556040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          anime_id     score  score_var\n",
       "username                               \n",
       "karthiga        21  0.605479   2.031881\n",
       "karthiga        59 -0.474160   1.593917\n",
       "karthiga        74 -0.750239   1.456926\n",
       "karthiga       120 -0.743815   1.658394\n",
       "karthiga       178 -0.206618   1.556040"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(\n",
    "    is_df, oos_df, recommendee, neighborhood_sizes, nonneg_corrs\n",
    "):\n",
    "\n",
    "    # compute correlations\n",
    "    corrs = get_correlation(is_df, recommendee)\n",
    "    if nonneg_corrs:\n",
    "        corrs[\"similarity\"] = corrs[\"corr\"]\n",
    "    else:\n",
    "        corrs[\"similarity\"] = corrs[\"corr\"].abs()\n",
    "    corrs = corrs.sort_values(by=\"similarity\").dropna()\n",
    "    corrs = corrs.drop(recommendee)  # makes insample score more meaningful\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "    for neighborhood_size in tqdm(neighborhood_sizes):\n",
    "        # extract model features\n",
    "        neighborhood = (\n",
    "            is_df.merge(pd.DataFrame(corrs[-neighborhood_size:]), on=\"username\")\n",
    "        ).dropna()\n",
    "        pred_df = prepare_prediction(recommendee, neighborhood)\n",
    "\n",
    "        # train linear model\n",
    "        recomendee_seen_shows = is_df.loc[recommendee].merge(pred_df, on=[\"anime_id\"])\n",
    "        model = lm(\"score ~ delta + 0\", recomendee_seen_shows)\n",
    "\n",
    "        # inference\n",
    "        pred_df[\"pred_score\"] = model.predict(pred_df)\n",
    "        is_pred_df = pred_df.loc[\n",
    "            lambda x: x.index.isin(is_df.loc[recommendee].anime_id)\n",
    "        ]\n",
    "        oos_pred_df = pred_df.loc[lambda x: x.index.isin(oos_df.anime_id)]\n",
    "\n",
    "        # compute coverage\n",
    "        is_coverage = len(is_pred_df) / len(is_df.loc[recommendee])\n",
    "        oos_coverage = len(oos_pred_df) / len(oos_df)\n",
    "\n",
    "        # compute rmse\n",
    "        missing_is = is_df.loc[recommendee].loc[\n",
    "            lambda x: ~x.anime_id.isin(is_pred_df.index)\n",
    "            & ~x.anime_id.isin(oos_df.anime_id)\n",
    "        ]\n",
    "        missing_oos = oos_df.loc[lambda x: ~x.anime_id.isin(oos_pred_df.index)]\n",
    "        is_se = get_squared_error(is_df, is_pred_df, recommendee)\n",
    "        oos_se = get_squared_error(oos_df, oos_pred_df, recommendee)\n",
    "        missing_is_se = np.dot(missing_is[\"score\"], missing_is[\"score\"])\n",
    "        missing_oos_se = np.dot(missing_oos[\"score\"], missing_oos[\"score\"])\n",
    "        is_rmse = np.sqrt((is_se + missing_is_se) / len(is_df.loc[recommendee]))\n",
    "        oos_rmse = np.sqrt((oos_se + missing_oos_se) / len(oos_df))\n",
    "        metrics = metrics.append(\n",
    "            {\n",
    "                \"neighborhood_size\": neighborhood_size,\n",
    "                \"nonneg_corrs\": nonneg_corrs,\n",
    "                \"is_rmse\": is_rmse,\n",
    "                \"is_coverage\": is_coverage,\n",
    "                \"oos_rmse\": oos_rmse,\n",
    "                \"oos_coverage\": oos_coverage,\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:08<06:34,  8.06s/it]\u001b[A\n",
      "  4%|▍         | 2/50 [00:14<05:35,  6.99s/it]\u001b[A\n",
      "  6%|▌         | 3/50 [00:20<05:15,  6.71s/it]\u001b[A\n",
      "  8%|▊         | 4/50 [00:26<04:59,  6.51s/it]\u001b[A\n",
      " 10%|█         | 5/50 [00:33<04:53,  6.53s/it]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:40<04:56,  6.73s/it]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:48<05:01,  7.02s/it]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:55<04:52,  6.96s/it]\u001b[A\n",
      " 18%|█▊        | 9/50 [01:01<04:40,  6.85s/it]\u001b[A\n",
      " 20%|██        | 10/50 [01:08<04:28,  6.71s/it]\u001b[A\n",
      " 22%|██▏       | 11/50 [01:14<04:21,  6.70s/it]\u001b[A\n",
      " 24%|██▍       | 12/50 [01:22<04:27,  7.05s/it]\u001b[A\n",
      " 26%|██▌       | 13/50 [01:29<04:15,  6.89s/it]\u001b[A\n",
      " 28%|██▊       | 14/50 [01:35<04:05,  6.82s/it]\u001b[A\n",
      " 30%|███       | 15/50 [01:42<03:57,  6.78s/it]\u001b[A\n",
      " 32%|███▏      | 16/50 [01:49<03:58,  7.01s/it]\u001b[A\n",
      " 34%|███▍      | 17/50 [01:56<03:48,  6.94s/it]\u001b[A\n",
      " 36%|███▌      | 18/50 [02:04<03:45,  7.04s/it]\u001b[A\n",
      " 38%|███▊      | 19/50 [02:11<03:44,  7.24s/it]\u001b[A\n",
      " 40%|████      | 20/50 [02:20<03:50,  7.68s/it]\u001b[A\n",
      " 42%|████▏     | 21/50 [02:27<03:40,  7.60s/it]\u001b[A\n",
      " 44%|████▍     | 22/50 [02:37<03:54,  8.37s/it]\u001b[A\n",
      " 46%|████▌     | 23/50 [02:47<03:53,  8.65s/it]\u001b[A\n",
      " 48%|████▊     | 24/50 [02:56<03:50,  8.85s/it]\u001b[A\n",
      " 50%|█████     | 25/50 [03:07<04:00,  9.61s/it]\u001b[A\n",
      " 52%|█████▏    | 26/50 [03:18<04:00, 10.02s/it]\u001b[A\n",
      " 54%|█████▍    | 27/50 [03:29<03:54, 10.21s/it]\u001b[A\n",
      " 56%|█████▌    | 28/50 [03:40<03:50, 10.46s/it]\u001b[A\n",
      " 58%|█████▊    | 29/50 [03:54<04:02, 11.53s/it]\u001b[A\n",
      " 60%|██████    | 30/50 [04:10<04:16, 12.84s/it]\u001b[A\n",
      " 62%|██████▏   | 31/50 [04:28<04:31, 14.31s/it]\u001b[A\n",
      " 64%|██████▍   | 32/50 [04:48<04:48, 16.02s/it]\u001b[A\n",
      " 66%|██████▌   | 33/50 [05:14<05:23, 19.01s/it]\u001b[A\n",
      " 68%|██████▊   | 34/50 [05:45<06:03, 22.71s/it]\u001b[A\n",
      " 70%|███████   | 35/50 [06:26<07:03, 28.23s/it]\u001b[A\n",
      " 72%|███████▏  | 36/50 [07:01<07:02, 30.15s/it]\u001b[A\n",
      " 74%|███████▍  | 37/50 [07:34<06:42, 30.98s/it]\u001b[A\n",
      " 76%|███████▌  | 38/50 [08:07<06:20, 31.71s/it]\u001b[A\n",
      " 78%|███████▊  | 39/50 [08:49<06:23, 34.83s/it]\u001b[A\n",
      " 80%|████████  | 40/50 [09:45<06:51, 41.12s/it]\u001b[A\n",
      " 82%|████████▏ | 41/50 [10:48<07:08, 47.64s/it]\u001b[A\n",
      " 84%|████████▍ | 42/50 [11:36<06:22, 47.83s/it]\u001b[A\n",
      " 86%|████████▌ | 43/50 [12:31<05:48, 49.83s/it]\u001b[A\n",
      " 88%|████████▊ | 44/50 [13:12<04:42, 47.15s/it]\u001b[A\n",
      " 90%|█████████ | 45/50 [13:47<03:37, 43.55s/it]\u001b[A\n",
      " 92%|█████████▏| 46/50 [14:21<02:42, 40.72s/it]\u001b[A\n",
      " 94%|█████████▍| 47/50 [14:54<01:55, 38.49s/it]\u001b[A\n",
      " 96%|█████████▌| 48/50 [15:47<01:25, 42.82s/it]\u001b[A\n",
      " 98%|█████████▊| 49/50 [16:44<00:47, 47.16s/it]\u001b[A\n",
      "100%|██████████| 50/50 [17:42<00:00, 21.25s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:06<05:11,  6.35s/it]\u001b[A\n",
      "  4%|▍         | 2/50 [00:12<04:46,  5.98s/it]\u001b[A\n",
      "  6%|▌         | 3/50 [00:18<04:57,  6.33s/it]\u001b[A\n",
      "  8%|▊         | 4/50 [00:26<05:08,  6.70s/it]\u001b[A\n",
      " 10%|█         | 5/50 [00:32<04:59,  6.65s/it]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:38<04:41,  6.39s/it]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:44<04:23,  6.12s/it]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:49<04:13,  6.04s/it]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:56<04:13,  6.19s/it]\u001b[A\n",
      " 20%|██        | 10/50 [01:03<04:17,  6.44s/it]\u001b[A\n",
      " 22%|██▏       | 11/50 [01:09<04:02,  6.22s/it]\u001b[A\n",
      " 24%|██▍       | 12/50 [01:15<03:54,  6.16s/it]\u001b[A\n",
      " 26%|██▌       | 13/50 [01:21<03:49,  6.21s/it]\u001b[A\n",
      " 28%|██▊       | 14/50 [01:28<03:46,  6.30s/it]\u001b[A\n",
      " 30%|███       | 15/50 [01:35<03:48,  6.54s/it]\u001b[A\n",
      " 32%|███▏      | 16/50 [01:42<03:53,  6.85s/it]\u001b[A\n",
      " 34%|███▍      | 17/50 [01:49<03:47,  6.90s/it]\u001b[A\n",
      " 36%|███▌      | 18/50 [01:58<03:54,  7.33s/it]\u001b[A\n",
      " 38%|███▊      | 19/50 [02:07<04:07,  7.99s/it]\u001b[A\n",
      " 40%|████      | 20/50 [02:15<04:03,  8.11s/it]\u001b[A\n",
      " 42%|████▏     | 21/50 [02:26<04:13,  8.74s/it]\u001b[A\n",
      " 44%|████▍     | 22/50 [02:37<04:30,  9.65s/it]\u001b[A\n",
      " 46%|████▌     | 23/50 [02:47<04:15,  9.48s/it]\u001b[A\n",
      " 48%|████▊     | 24/50 [02:55<04:02,  9.31s/it]\u001b[A\n",
      " 50%|█████     | 25/50 [03:06<04:01,  9.64s/it]\u001b[A\n",
      " 52%|█████▏    | 26/50 [03:15<03:47,  9.47s/it]\u001b[A\n",
      " 54%|█████▍    | 27/50 [03:25<03:41,  9.64s/it]\u001b[A\n",
      " 56%|█████▌    | 28/50 [03:40<04:10, 11.37s/it]\u001b[A\n",
      " 58%|█████▊    | 29/50 [03:55<04:19, 12.34s/it]\u001b[A\n",
      " 60%|██████    | 30/50 [04:09<04:16, 12.83s/it]\u001b[A\n",
      " 62%|██████▏   | 31/50 [04:44<02:54,  9.18s/it]\u001b[A\n",
      "  0%|          | 0/10 [25:07<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "K = 10\n",
    "base = np.sqrt(2)\n",
    "\n",
    "errors_by_neighborhood_size = []\n",
    "neighborhood_sizes = [\n",
    "    int(base ** i) for i in range(int(np.log(len(filtered_df)) / np.log(base)) + 1)\n",
    "] + [len(filtered_df)]\n",
    "neighborhood_sizes = sorted(list(set(neighborhood_sizes)))\n",
    "splits = np.array_split(filtered_df.loc[recommendee].sample(frac=1), K)\n",
    "\n",
    "for split in tqdm(splits):\n",
    "    oos_df = split\n",
    "    is_df = filtered_df.loc[\n",
    "        lambda x: ~(\n",
    "            (x.index.get_level_values(\"username\") == recommendee)\n",
    "            & x.anime_id.isin(oos_df.anime_id)\n",
    "        )\n",
    "    ]\n",
    "    errors_by_neighborhood_size.append(\n",
    "        compute_accuracy_metrics(\n",
    "            is_df.copy(), oos_df.copy(), recommendee, neighborhood_sizes, False\n",
    "        )\n",
    "    )\n",
    "    errors_by_neighborhood_size.append(\n",
    "        compute_accuracy_metrics(\n",
    "            is_df.copy(), oos_df.copy(), recommendee, neighborhood_sizes, True\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors = pd.concat(errors_by_neighborhood_size, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_data = pd.melt(allerrors, [\"neighborhood_size\", \"nonneg_corrs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_data.loc[lambda x: x.nonneg_corrs == True, \"variable\"] = (\n",
    "    \"nonneg_\" + wide_data.loc[lambda x: x.nonneg_corrs == True, \"variable\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors.loc[lambda x: x[\"neighborhood_size\"] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "_ = sns.lineplot(\n",
    "    x=\"neighborhood_size\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    data=wide_data.loc[lambda x: x.variable.str.contains(\"coverage\")],\n",
    ").set(xscale=\"log\", title=\"Prediction Coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "_ = sns.lineplot(\n",
    "    x=\"neighborhood_size\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    data=wide_data.loc[lambda x: x.variable.str.contains(\"rmse\")],\n",
    ").set(xscale=\"log\", title=\"Root Mean Squared Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "allerrors.groupby([\"nonneg_corrs\", \"neighborhood_size\"]).mean().sort_values(\n",
    "    by=\"oos_rmse\"\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "allerrors.groupby([\"nonneg_corrs\", \"neighborhood_size\"]).mean().xs(\n",
    "    0, level=\"nonneg_corrs\"\n",
    ").sort_values(by=\"oos_rmse\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "allerrors.groupby([\"nonneg_corrs\", \"neighborhood_size\"]).mean().xs(\n",
    "    0, level=\"nonneg_corrs\"\n",
    ").sort_values(by=\"is_rmse\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "allerrors.groupby([\"nonneg_corrs\", \"neighborhood_size\"]).mean().xs(\n",
    "    0, level=\"nonneg_corrs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
