{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "@functools.wraps(smf.ols)\n",
    "def lm(*args, **kwargs):\n",
    "    return smf.ols(*args, **kwargs).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"UserAnimeList.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = raw_df[[\"username\", \"anime_id\", \"my_score\"]].loc[\n",
    "    lambda x: x[\"my_score\"] != 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(file, username):\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    xml_data = open(file, \"r\").read()  # Read file\n",
    "    root = ET.XML(xml_data)  # Parse XML\n",
    "\n",
    "    data = []\n",
    "    cols = []\n",
    "    for i, child in enumerate(root):\n",
    "        data.append([subchild.text for subchild in child])\n",
    "        cols.append(child.tag)\n",
    "    new_list = pd.DataFrame(data).T\n",
    "    new_list.columns = cols\n",
    "\n",
    "    df = new_list.loc[[0, 9]].T.dropna().rename({0: \"anime_id\", 9: \"my_score\"}, axis=1)\n",
    "    df[\"username\"] = username\n",
    "    df[\"anime_id\"] = df[\"anime_id\"].astype(int)\n",
    "    df[\"my_score\"] = df[\"my_score\"].astype(int)\n",
    "    df[\"username\"] = df[\"username\"].astype(str)\n",
    "    df = df.loc[lambda x: x[\"my_score\"] != 0]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_user(full_df, xml_file, username):\n",
    "    user_df = read_xml(xml_file, username)\n",
    "    without_user = full_df.loc[lambda x: x[\"username\"] != username]\n",
    "    return pd.concat([without_user, user_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = add_user(filtered_df, \"user_profiles/Fro116.xml\", \"Fro116\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rating = filtered_df[\"my_score\"].mean()\n",
    "user_bias = (\n",
    "    pd.DataFrame(filtered_df.groupby(\"username\")[\"my_score\"].mean()).rename(\n",
    "        {\"my_score\": \"user_bias\"}, axis=1\n",
    "    )\n",
    "    - average_rating\n",
    ")\n",
    "anime_bias = (\n",
    "    pd.DataFrame(filtered_df.groupby(\"anime_id\")[\"my_score\"].mean()).rename(\n",
    "        {\"my_score\": \"anime_bias\"}, axis=1\n",
    "    )\n",
    "    - average_rating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.merge(anime_bias, on=[\"anime_id\"]).merge(\n",
    "    user_bias, on=[\"username\"]\n",
    ")\n",
    "filtered_df[\"normalized_score\"] = (\n",
    "    filtered_df[\"my_score\"]\n",
    "    - filtered_df[\"anime_bias\"]\n",
    "    - filtered_df[\"user_bias\"]\n",
    "    - average_rating\n",
    ")\n",
    "filtered_df[\"orig_normalized_score\"] = filtered_df[\"normalized_score\"]\n",
    "filtered_df = filtered_df.set_index(\"username\")\n",
    "filtered_df = filtered_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prediction(recommendee, neighborhood):\n",
    "    pred_df = pd.DataFrame()\n",
    "    pred_df[\"delta\"] = neighborhood.groupby(\"anime_id\").apply(\n",
    "        lambda x: np.dot(x[\"normalized_score\"], x[\"corr\"]) / x[\"corr\"].abs().sum()\n",
    "    )\n",
    "    pred_df[\"blp\"] = anime_bias + user_bias.loc[recommendee].squeeze() + average_rating\n",
    "    pred_df = pred_df.dropna()\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(df, recommendee):\n",
    "    user_subset = df.loc[[recommendee]].merge(df.reset_index(), on=\"anime_id\")\n",
    "    adj_cos_corr_numerator = user_subset.groupby(\"username\").apply(\n",
    "        lambda x: np.dot(x[\"normalized_score_x\"], x[\"normalized_score_y\"])\n",
    "    )\n",
    "    adj_cos_corr_denom = df.groupby(\"username\").apply(\n",
    "        lambda x: np.sqrt(np.dot(x[\"normalized_score\"], x[\"normalized_score\"]))\n",
    "    )\n",
    "    adj_cos_corr_denom *= adj_cos_corr_denom.loc[recommendee]\n",
    "    adj_cos_corrs = pd.DataFrame(\n",
    "        (adj_cos_corr_numerator / adj_cos_corr_denom), columns=[\"corr\"]\n",
    "    )\n",
    "    adj_cos_corrs = adj_cos_corrs.dropna()\n",
    "    return adj_cos_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squared_error(df, pred_df, recommendee):\n",
    "    recommendee_df = pred_df.loc[\n",
    "        pred_df.index.intersection(df.loc[recommendee].anime_id)\n",
    "    ]\n",
    "    recommendee_df = recommendee_df.merge(\n",
    "        df.loc[recommendee].set_index(\"anime_id\")[\"my_score\"], on=\"anime_id\"\n",
    "    )\n",
    "    errors = recommendee_df[\"my_score\"] - recommendee_df[\"score\"]\n",
    "    return np.dot(errors, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(\n",
    "    is_df, oos_df, recommendee, neighborhood_sizes, normalize_variance\n",
    "):\n",
    "\n",
    "    # Should we normalize by variance?\n",
    "    # Note that normalize_variance=True modifies the data\n",
    "    if normalize_variance:\n",
    "        user_stds = (\n",
    "            is_df.groupby(\"username\")[[\"normalized_score\"]]\n",
    "            .std()\n",
    "            .rename({\"normalized_score\": \"user_std\"}, axis=1)\n",
    "        )\n",
    "        is_df = is_df.merge(user_stds, on=\"username\")\n",
    "        is_df[\"normalized_score\"] /= is_df[\"user_std\"]\n",
    "        is_df = is_df.drop(\"user_std\", axis=1)\n",
    "\n",
    "    # compute correlations\n",
    "    corrs = get_correlation(is_df, recommendee)\n",
    "    corrs[\"similarity\"] = corrs[\"corr\"].abs()\n",
    "    corrs = corrs.sort_values(by=\"similarity\").dropna()\n",
    "    corrs = corrs.drop(recommendee)  # makes insample score more meaningful\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "    for neighborhood_size in neighborhood_sizes:\n",
    "        # extract model features\n",
    "        neighborhood = (\n",
    "            is_df.merge(pd.DataFrame(corrs[-neighborhood_size:]), on=\"username\")\n",
    "        ).dropna()\n",
    "        pred_df = prepare_prediction(recommendee, neighborhood)\n",
    "\n",
    "        # train linear model\n",
    "        recomendee_seen_shows = is_df.loc[recommendee].merge(pred_df, on=[\"anime_id\"])\n",
    "        recomendee_seen_shows[\"target\"] = (\n",
    "            recomendee_seen_shows[\"my_score\"] - recomendee_seen_shows[\"blp\"]\n",
    "        )\n",
    "        model = lm(\"target ~ delta + 0\", recomendee_seen_shows)\n",
    "\n",
    "        # inference\n",
    "        pred_df[\"score\"] = model.predict(pred_df) + pred_df[\"blp\"]\n",
    "        is_pred_df = pred_df.loc[\n",
    "            lambda x: x.index.isin(is_df.loc[recommendee].anime_id)\n",
    "        ]\n",
    "        oos_pred_df = pred_df.loc[lambda x: x.index.isin(oos_df.anime_id)]\n",
    "\n",
    "        # compute coverage\n",
    "        is_coverage = len(is_pred_df) / len(is_df.loc[recommendee])\n",
    "        oos_coverage = len(oos_pred_df) / len(oos_df)\n",
    "\n",
    "        # compute rmse\n",
    "        missing_is = is_df.loc[recommendee].loc[\n",
    "            lambda x: ~x.anime_id.isin(is_pred_df.index)\n",
    "            & ~x.anime_id.isin(oos_df.anime_id)\n",
    "        ]\n",
    "        missing_oos = oos_df.loc[lambda x: ~x.anime_id.isin(oos_pred_df.index)]\n",
    "        is_se = get_squared_error(is_df, is_pred_df, recommendee)\n",
    "        oos_se = get_squared_error(oos_df, oos_pred_df, recommendee)\n",
    "        missing_is_se = np.dot(\n",
    "            missing_is[\"orig_normalized_score\"], missing_is[\"orig_normalized_score\"]\n",
    "        )\n",
    "        missing_oos_se = np.dot(\n",
    "            missing_oos[\"orig_normalized_score\"], missing_oos[\"orig_normalized_score\"]\n",
    "        )\n",
    "        is_rmse = np.sqrt((is_se + missing_is_se) / len(is_df.loc[recommendee]))\n",
    "        oos_rmse = np.sqrt((oos_se + missing_oos_se) / len(oos_df))\n",
    "        metrics = metrics.append(\n",
    "            {\n",
    "                \"neighborhood_size\": neighborhood_size,\n",
    "                \"normalize_variance\": normalize_variance,\n",
    "                \"is_rmse\": is_rmse,\n",
    "                \"is_coverage\": is_coverage,\n",
    "                \"oos_rmse\": oos_rmse,\n",
    "                \"oos_coverage\": oos_coverage,\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "recommendee = \"Fro116\"\n",
    "K = 10\n",
    "base = np.sqrt(2)\n",
    "\n",
    "errors_by_neighborhood_size = []\n",
    "neighborhood_sizes = [\n",
    "    int(base ** i) for i in range(int(np.log(len(filtered_df)) / np.log(base)) + 1)\n",
    "] + [len(filtered_df)]\n",
    "splits = np.array_split(filtered_df.loc[recommendee].sample(frac=1), K)\n",
    "for split in splits:\n",
    "    display(split.head())\n",
    "\n",
    "for split in tqdm(splits):\n",
    "    oos_df = split\n",
    "    is_df = filtered_df.loc[\n",
    "        lambda x: ~(\n",
    "            (x.index.get_level_values(\"username\") == recommendee)\n",
    "            & x.anime_id.isin(oos_df.anime_id)\n",
    "        )\n",
    "    ]\n",
    "    # we take copies because setting normalize_variance=True modifies the data\n",
    "    errors_by_neighborhood_size.append(\n",
    "        compute_accuracy_metrics(\n",
    "            is_df.copy(), oos_df.copy(), \"Fro116\", neighborhood_sizes, True\n",
    "        )\n",
    "    )\n",
    "    errors_by_neighborhood_size.append(\n",
    "        compute_accuracy_metrics(\n",
    "            is_df.copy(), oos_df.copy(), \"Fro116\", neighborhood_sizes, False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors = pd.concat(errors_by_neighborhood_size, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_data = pd.melt(allerrors, [\"neighborhood_size\", \"normalize_variance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_data.loc[lambda x: x.normalize_variance == True, \"variable\"] = (\n",
    "    \"normalized_\" + wide_data.loc[lambda x: x.normalize_variance == True, \"variable\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "_ = sns.lineplot(\n",
    "    x=\"neighborhood_size\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    data=wide_data.loc[lambda x: x.variable.str.contains(\"coverage\")],\n",
    ").set(xscale=\"log\", title=\"Prediction Coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "_ = sns.lineplot(\n",
    "    x=\"neighborhood_size\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    data=wide_data.loc[lambda x: x.variable.str.contains(\"rmse\")],\n",
    ").set(xscale=\"log\", title=\"Root Mean Squared Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "allerrors.groupby([\"normalize_variance\", \"neighborhood_size\"]).mean().sort_values(\n",
    "    by=\"oos_rmse\"\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My takeaway from this is that it doesn't matter if we normalize variance or not\n",
    "# For simplicity, let's forgo normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "allerrors.groupby([\"normalize_variance\", \"neighborhood_size\"]).mean().xs(0, level='normalize_variance').sort_values(\n",
    "    by=\"oos_rmse\"\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "allerrors.groupby([\"normalize_variance\", \"neighborhood_size\"]).mean().xs(0, level='normalize_variance').sort_values(\n",
    "    by=\"is_rmse\"\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "allerrors.groupby([\"normalize_variance\", \"neighborhood_size\"]).mean().xs(0, level='normalize_variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allerrors.groupby([\"normalize_variance\", \"neighborhood_size\"]).apply(lambda x: np.sqrt(np.sum(x**2) / K)).xs(0, level='normalize_variance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
