{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1915b44a-3a42-431f-bd6c-94edf9e5bc36",
   "metadata": {},
   "source": [
    "# Getting MAL anime-lists\n",
    "* We collect the anime-list for each username in `data/mal/user_facts/usernames.txt` and `data/mal/user_facts/recent_usernames.txt` and `data/mal/user_facts/usernames_from_id.txt`\n",
    "* You can terminate or restart the notebook at any point without losing progress. All anime-lists found so far will be stored at `data/mal/user_anime_facts/user_anime_list.csv` \n",
    "* This notebook will run indefinitely. You must manually terminate once an acceptable number of anime-lists have been found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a55928-bc2d-4dfb-974f-2480e13c1f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65822b0-3bec-4869-94e1-d956684c4bba",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50d2d6-3aa2-4ae2-b6cb-8daa401f6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdir\n",
    "data_path = \"../../data/mal/user_anime_facts\"\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8b363-578a-44d6-a82a-1005d9ca3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging\n",
    "logger = logging.getLogger(\"GetUserAnimeLists\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\n",
    "    \"%(name)s:%(levelname)s:%(asctime)s: %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "for stream in [\n",
    "    logging.FileHandler(\"get_user_anime_lists.log\"),\n",
    "    logging.StreamHandler(),\n",
    "]:\n",
    "    stream.setFormatter(formatter)\n",
    "    logger.addHandler(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd55c16-a8c1-4b3b-87c1-2b8aef672917",
   "metadata": {},
   "source": [
    "## Parse MAL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36dd4e4-5e93-4e57-bfca-8f271e4afce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = json.load(open(\"../mal_authentication/token.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a872e694-1db1-4d9c-b84f-6d6ed5700f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply rate limiting with exponential backoff for unexpected errors\n",
    "@sleep_and_retry\n",
    "@limits(calls=1, period=0.75)\n",
    "def call_api(url, retry_timeout=1):\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url, headers={\"Authorization\": f'Bearer {token[\"access_token\"]}'}\n",
    "        )\n",
    "        if response.status_code in [500, 504] and retry_timeout < 3600:\n",
    "            # This can occur if MAL servers go down or if the page doesnt exist\n",
    "            raise Exception(f\"{response.status_code}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(\n",
    "            f\"Received error {str(e)} while accessing {url}. Retrying in {retry_timeout} seconds\"\n",
    "        )\n",
    "        time.sleep(retry_timeout)\n",
    "        retry_timeout = min(retry_timeout * 2, 3600)\n",
    "        return call_api(url, retry_timeout)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab2a76-a736-4d48-92dc-6d422fdf08a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for extracting an anime list from the MAL API\n",
    "\n",
    "\n",
    "def parse_json_node(x):\n",
    "    ls = x[\"list_status\"]\n",
    "    entry = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"uid\": [x[\"node\"][\"id\"]],\n",
    "            \"status\": [ls.get(\"status\", \"\")],\n",
    "            \"score\": [ls.get(\"score\", -1)],\n",
    "            \"num_episodes_watched\": [ls.get(\"num_episodes_watched\", -1)],\n",
    "            \"is_rewatching\": [ls.get(\"is_rewatching\", False)],\n",
    "            \"start_date\": [ls.get(\"start_date\", \"\")],\n",
    "            \"finish_date\": [ls.get(\"finish_date\", \"\")],\n",
    "            \"priority\": [ls.get(\"priority\", -1)],\n",
    "            \"num_times_rewatched\": [ls.get(\"num_times_rewatched\", -1)],\n",
    "            \"rewatch_value\": [ls.get(\"rewatch_value\", -1)],\n",
    "            \"updated_at\": [ls.get(\"updated_at\", \"\")],\n",
    "        }\n",
    "    )\n",
    "    return entry\n",
    "\n",
    "\n",
    "def process_json(json):\n",
    "    entries = [parse_json_node(x) for x in json[\"data\"]]\n",
    "    if entries:\n",
    "        return pd.concat(entries, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"uid\": [],\n",
    "                \"status\": [],\n",
    "                \"score\": [],\n",
    "                \"num_episodes_watched\": [],\n",
    "                \"is_rewatching\": [],\n",
    "                \"start_date\": [],\n",
    "                \"finish_date\": [],\n",
    "                \"priority\": [],\n",
    "                \"num_times_rewatched\": [],\n",
    "                \"rewatch_value\": [],\n",
    "                \"updated_at\": [],\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "def get_user_anime_list(username):\n",
    "    anime_lists = []\n",
    "    more_pages = True\n",
    "    url = f\"https://api.myanimelist.net/v2/users/{username}/animelist?limit=1000&fields=list_status&nsfw=true\"\n",
    "    while more_pages:\n",
    "        response = call_api(url)\n",
    "        if response.status_code in [403, 404]:\n",
    "            # 403: This can occur if the user privated their list\n",
    "            # 404: This can occur if the user deleted their account\n",
    "            return pd.DataFrame(), False\n",
    "        if not response.ok:\n",
    "            logger.warning(f\"Error {response} received when handling {url}\")\n",
    "            return pd.DataFrame(), False\n",
    "\n",
    "        json = response.json()\n",
    "        anime_lists.append(process_json(json))\n",
    "        more_pages = \"next\" in json[\"paging\"]\n",
    "        if more_pages:\n",
    "            url = json[\"paging\"][\"next\"]\n",
    "    user_anime_list = pd.concat(anime_lists, ignore_index=True)\n",
    "    user_anime_list[\"username\"] = username\n",
    "    return user_anime_list, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2c1b8-99ed-4bb3-8c15-338d0709db5a",
   "metadata": {},
   "source": [
    "## Sort users by recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d5273-5795-41ce-a967-c91f63c0631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_usernames_from_path(base):\n",
    "    path = f\"../user_facts/{base}.txt\"\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    with open(path, \"r\") as f:\n",
    "        return {x.strip() for x in f.readlines()}\n",
    "\n",
    "\n",
    "def read_usernames():\n",
    "    usernames = set()\n",
    "    usernames |= read_usernames_from_path(\"usernames\")\n",
    "    usernames |= read_usernames_from_path(\"recent_usernames\")\n",
    "    usernames |= read_usernames_from_path(\"usernames_from_id\")\n",
    "    return list(usernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b6b41-a93b-434a-9408-261b11ba7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_user_status():\n",
    "    user_status_file = \"user_status.csv\"\n",
    "    if os.path.exists(user_status_file):\n",
    "        return pd.read_csv(user_status_file, keep_default_na=False)\n",
    "    else:\n",
    "        return pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"username\": [],\n",
    "                \"access_timestamp\": [],\n",
    "                \"success\": [],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279a574-9d37-489a-9366-12cf46399574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritize_users():\n",
    "    usernames = read_usernames()\n",
    "    user_status = read_user_status()\n",
    "    new_users = list(set(usernames) - set(user_status[\"username\"]))\n",
    "    random.shuffle(new_users)\n",
    "    existing_users = list(user_status.sort_values(by=\"access_timestamp\")[\"username\"])\n",
    "    logger.info(\n",
    "        f\"Getting the anime lists of {len(new_users)} new users and refreshing \"\n",
    "        f\"the anime lists of {len(existing_users)} existing users!\"\n",
    "    )\n",
    "    return new_users + existing_users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f61b54-8179-4dda-8e9a-a4d8a0b9234f",
   "metadata": {},
   "source": [
    "## Continuously refresh anime lists\n",
    "* We take the least recently refreshed users and refresh their anime lists\n",
    "* These anime lists are stored in a temporary block\n",
    "* Once the block is big enough, we atomically merge it with the existing anime lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098f0a2-fb1e-4022-a154-ad82a362d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_block(file, user_field, users):\n",
    "    outfile = file + \"~\"\n",
    "    blockfile = file + \".block\"\n",
    "    first_run = not os.path.exists(file)\n",
    "    with open(outfile, \"w\") as out_file:\n",
    "        # copy over all the unchaged users\n",
    "        if not first_run:\n",
    "            with open(file, \"r\") as in_file:\n",
    "                header = False\n",
    "                for line in tqdm(in_file):\n",
    "                    if not header:\n",
    "                        header = True\n",
    "                        out_file.write(line)\n",
    "                        continue\n",
    "                    fields = line.strip().split(\",\")\n",
    "                    if fields[user_field] not in users:\n",
    "                        out_file.write(line)\n",
    "\n",
    "        # copy over the new block\n",
    "        with open(blockfile, \"r\") as in_file:\n",
    "            header = False\n",
    "            for line in tqdm(in_file):\n",
    "                if not header:\n",
    "                    if first_run:\n",
    "                        out_file.write(line)\n",
    "                    header = True\n",
    "                    continue\n",
    "                out_file.write(line)\n",
    "    os.replace(outfile, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f788a-4908-417d-b4d8-c76f1e18d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_blocks():\n",
    "    users = set(pd.read_csv(\"user_status.csv.block\", keep_default_na=False)[\"username\"])\n",
    "    merge_block(\"user_anime_list.csv\", -1, users)\n",
    "    merge_block(\"user_status.csv\", 0, users)\n",
    "    logger.info(f\"Merging block of {len(users)} users into the main database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff37af-acda-491d-9f30-2deec9e4c6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the anime list for each new user and write to disk\n",
    "while True:\n",
    "    usernames = prioritize_users()[:50000]\n",
    "    block = set()\n",
    "    for username in tqdm(usernames):\n",
    "        user_anime_list, ok = get_user_anime_list(username)\n",
    "        user_anime_list.to_csv(\n",
    "            \"user_anime_list.csv.block\",\n",
    "            index=False,\n",
    "            mode=\"w\" if not block else \"a+\",\n",
    "            header=not block,\n",
    "        )\n",
    "        \n",
    "        user_status_entry = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"username\": [username],\n",
    "                \"access_timestamp\": [int(datetime.datetime.now().timestamp())],\n",
    "                \"success\": [ok],\n",
    "            }\n",
    "        )\n",
    "        user_status_entry.to_csv(\n",
    "            \"user_status.csv.block\",\n",
    "            index=False,\n",
    "            mode=\"w\" if not block else \"a+\",\n",
    "            header=not block,\n",
    "        )\n",
    "        block.add(username)\n",
    "    merge_blocks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
