{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a list of MAL usernames\n",
    "* We start with a list of starting usernames, which can either be stored in `data/mal/user_facts/queue.txt` or specified in the notebook\n",
    "* You can optionally run `notebooks/API/GetRecentUsernames` to seed the search \n",
    "* Then, we do a breadth-first search of their friends until the friend graph is spanned\n",
    "* You can terminate or restart the notebook at any point without losing progress. All users found so far will be stored at `data/mal/user_facts/usernames.txt`. The adjacency list of the friend graph will be stored at `data/mal/user_facts/friends_list.csv`.\n",
    "* This notebook will run indefinitely. You must manually terminate once an acceptable number of users have been found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdir\n",
    "data_path = \"../../data/mal/user_facts\"\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging\n",
    "logger = logging.getLogger(\"GetUserFriends\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\n",
    "    \"%(name)s:%(levelname)s:%(asctime)s: %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "for stream in [logging.FileHandler(\"get_user_friends.log\"), logging.StreamHandler()]:\n",
    "    stream.setFormatter(formatter)\n",
    "    logger.addHandler(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse MAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply rate limiting with exponential backoff for unexpected errors\n",
    "@sleep_and_retry\n",
    "@limits(calls=1, period=3)\n",
    "def call_api(url, retry_timeout=1):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code in [403, 429, 500, 503]:\n",
    "            # This can occur if MAL servers go down\n",
    "            raise Exception(f\"{response.status_code}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(\n",
    "            f\"Received error {str(e)} while accessing {url}. Retrying in {retry_timeout} seconds\"\n",
    "        )\n",
    "        time.sleep(retry_timeout)\n",
    "        retry_timeout = min(retry_timeout * 2, 3600)\n",
    "        return call_api(url, retry_timeout)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse a user's MAL profile page for their friends\n",
    "def get_friends(username):\n",
    "    url = f\"https://myanimelist.net/profile/{username}/friends\"\n",
    "    response = call_api(url)\n",
    "    if response.status_code in [404]:\n",
    "        # the user may have deleted their account\n",
    "        return set()\n",
    "    if not response.ok:\n",
    "        logger.warning(f\"Error {response} received when handling {url}\")\n",
    "        return set()\n",
    "    friend_urls = re.findall(\n",
    "        '''https://myanimelist.net/profile/[^\"/%#]+\"''', response.text\n",
    "    )\n",
    "    friends = {\n",
    "        x[len(\"https://myanimelist.net/profile/\") : -len('\"')] for x in friend_urls\n",
    "    }\n",
    "    return friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage username database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atomic saving utilities\n",
    "@contextlib.contextmanager\n",
    "def atomic_overwrite(filename):\n",
    "    temp = filename + \"~\"\n",
    "    with open(temp, \"w\") as f:\n",
    "        yield f\n",
    "    os.replace(temp, filename)\n",
    "\n",
    "\n",
    "def atomic_to_csv(collection, filename):\n",
    "    with atomic_overwrite(filename) as f:\n",
    "        pd.Series(collection).to_csv(f, header=False, index=False)\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def atomic_append(filename):\n",
    "    temp = filename + \"~\"\n",
    "    with open(temp, \"w\") as f:\n",
    "        yield f\n",
    "\n",
    "    temp2 = temp + \"~\"\n",
    "    with open(temp2, \"wb\") as wfd:\n",
    "        for f in [filename, temp]:\n",
    "            with open(f, \"rb\") as fd:\n",
    "                shutil.copyfileobj(fd, wfd)\n",
    "    os.remove(temp)\n",
    "    os.replace(temp2, filename)\n",
    "\n",
    "\n",
    "def atomic_append_dataframe_to_csv(df, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        with atomic_overwrite(filename) as f:\n",
    "            df.to_csv(f, index=False)\n",
    "    else:\n",
    "        with atomic_append(filename) as f:\n",
    "            df.to_csv(f, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snapshot hourly to amortize the cost of the disk I/O\n",
    "def should_save(reason):\n",
    "    should_save = False\n",
    "    if reason not in save_reasons:\n",
    "        save_reasons[reason] = (0, 1)\n",
    "    iterations_since_last_write, iterations_until_next_write = save_reasons[reason]\n",
    "    iterations_since_last_write += 1\n",
    "    if iterations_since_last_write >= iterations_until_next_write:\n",
    "        iterations_since_last_write = 0\n",
    "        iterations_until_next_write = min(2 * iterations_until_next_write, 1200)\n",
    "        should_save = True\n",
    "        logger.info(\n",
    "            f\"Writing data for {reason}. Will next write data \"\n",
    "            f\"after {iterations_until_next_write} iterations\"\n",
    "        )\n",
    "    save_reasons[reason] = (iterations_since_last_write, iterations_until_next_write)\n",
    "    return should_save\n",
    "\n",
    "\n",
    "save_reasons = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed the graph search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reseed_search():\n",
    "    # reasonable defaults if this is the first time running the notebook\n",
    "    queue = [\"Fro116\"]\n",
    "    usernames = set()\n",
    "    open_nodes = set()\n",
    "    closed_nodes = set()\n",
    "    shuffle_on_rerun = True\n",
    "\n",
    "    # if we rerunning the notebook, then resume execution where we last left off\n",
    "    if os.path.exists(\"queue.txt\"):\n",
    "        with open(\"queue.txt\") as f:\n",
    "            queue = [x.strip() for x in f.readlines() if x.strip()]\n",
    "    for username_fn in [\"usernames.txt\", \"recent_usernames.txt\"]:\n",
    "        if os.path.exists(username_fn):\n",
    "            with open(username_fn) as f:\n",
    "                usernames |= {x.strip() for x in f.readlines() if x.strip()}\n",
    "    if os.path.exists(\"closed_nodes.txt\"):\n",
    "        with open(\"closed_nodes.txt\") as f:\n",
    "            closed_nodes = {x.strip() for x in f.readlines() if x.strip()}\n",
    "\n",
    "    # verify consistency of loaded data structures\n",
    "    # they might be inconsistent if the notebook crashed mid-save\n",
    "    queue = [x for x in queue if x not in closed_nodes]\n",
    "    queue = queue + list(usernames - closed_nodes - set(queue))\n",
    "    usernames |= set(queue)\n",
    "    open_nodes = set(queue)\n",
    "    if shuffle_on_rerun:\n",
    "        np.random.shuffle(queue)\n",
    "\n",
    "    for x in [queue, usernames, open_nodes, closed_nodes]:\n",
    "        x = filter_pct_sign(x)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Starting with {len(queue)} users in queue and {len(closed_nodes)} processed \"\n",
    "        f\"users for a total of {len(usernames)} users!\"\n",
    "    )\n",
    "    return queue, usernames, open_nodes, closed_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuously query user friends\n",
    "* TODO documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we use a generator for profiling with tqdm\n",
    "def generate_true():\n",
    "    while True:\n",
    "        yield\n",
    "\n",
    "\n",
    "def atomic_save_progress(usernames, friends_list, queue, closed_nodes):\n",
    "    atomic_to_csv(sorted(list(usernames)), \"usernames.txt\")\n",
    "    atomic_append_dataframe_to_csv(friends_list, \"friends_list.csv\")\n",
    "    atomic_to_csv(queue, \"queue.txt\")\n",
    "    atomic_to_csv(sorted(list(closed_nodes)), \"closed_nodes.txt\")\n",
    "    logger.info(\n",
    "        f\"Successfully wrote {len(queue)} users in queue and {len(closed_nodes)} \"\n",
    "        f\"processed users for a total of {len(usernames)} users!\"\n",
    "    )\n",
    "\n",
    "\n",
    "def search_friend_graph():\n",
    "    # Breadth first search\n",
    "    queue, usernames, open_nodes, closed_nodes = reseed_search()\n",
    "    friends_list = pd.DataFrame()\n",
    "    for _ in tqdm(generate_true()):\n",
    "        if not queue:\n",
    "            break\n",
    "        username = queue[0]\n",
    "        queue = queue[1:]\n",
    "        friends = get_friends(username)\n",
    "\n",
    "        usernames |= friends\n",
    "        new_friends = [\n",
    "            x for x in friends if x not in closed_nodes and x not in open_nodes\n",
    "        ]\n",
    "        queue = queue + new_friends\n",
    "        open_nodes |= set(new_friends)\n",
    "        open_nodes.remove(username)\n",
    "        closed_nodes.add(username)\n",
    "        friends |= {username}\n",
    "        friends_list = friends_list.append(\n",
    "            pd.DataFrame.from_dict(\n",
    "                {\"Username\": [username] * len(friends), \"Friend\": list(friends)}\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if should_save(\"users\"):\n",
    "            atomic_save_progress(usernames, friends_list, queue, closed_nodes)\n",
    "            friends_list = pd.DataFrame()\n",
    "\n",
    "    # Final save\n",
    "    atomic_save_progress(usernames, friends_list, queue, closed_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    search_friend_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
