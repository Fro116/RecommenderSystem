{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1915b44a-3a42-431f-bd6c-94edf9e5bc36",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting user lists\n",
    "* You can terminate or restart the notebook at any point without losing progress. All lists found so far will be stored at `data/{NAME}/user_media_facts/user_{MEDIATYPE}_list.{PARTITION}.csv`\n",
    "* The notebook will only collect data for users whose hash is equal to PARTITION_NUMBER mod NUM_PARTITIONS\n",
    "* This notebook will run indefinitely. You must manually terminate once an acceptable number of lists have been found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a55928-bc2d-4dfb-974f-2480e13c1f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from hashlib import sha256\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65822b0-3bec-4869-94e1-d956684c4bba",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef1d18-5e5f-4aaf-9a38-2c305d735240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_notebook(nb):\n",
    "    cwd = os.getcwd()\n",
    "    try:\n",
    "        os.chdir(os.path.dirname(nb))\n",
    "        script = os.path.basename(nb)\n",
    "        %run $script\n",
    "    finally:\n",
    "        os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653d794-df6b-47ca-b8de-dacf4fafe95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_notebook(f\"../API/{NAME.capitalize()}Api.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50d2d6-3aa2-4ae2-b6cb-8daa401f6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdir\n",
    "data_path = f\"../../../data/{NAME}/user_media_facts\"\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b89df-ed09-412c-adc7-ef0cb605e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_FILE = f\"user_status.{PARTITION}.csv\"\n",
    "LOG_FILE = f\"get_user_media_lists.{PARTITION}.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc7e24-ee38-4735-81b8-9d04f6a95e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def media_list_file(medium):\n",
    "    return f\"user_{medium}_list.{PARTITION}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0016eb-533e-43c9-b294-6b4eb417e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_per_batch = 25000\n",
    "if NAME == \"animeplanet\":\n",
    "    users_per_batch = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8b363-578a-44d6-a82a-1005d9ca3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging\n",
    "logger = logging.getLogger(f\"get_user_media_lists\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\n",
    "    \"%(name)s:%(levelname)s:%(asctime)s: %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "for stream in [\n",
    "    logging.handlers.RotatingFileHandler(\n",
    "        LOG_FILE, \"w+\", maxBytes=1000000, backupCount=1\n",
    "    ),\n",
    "]:\n",
    "    stream.setFormatter(formatter)\n",
    "    logger.addHandler(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2c1b8-99ed-4bb3-8c15-338d0709db5a",
   "metadata": {},
   "source": [
    "## Sort users by recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a66a2a-5b92-4906-9a64-e83c80ba9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portable_hash(username):\n",
    "    return int(sha256(username.encode(\"utf-8\")).hexdigest(), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007b0d7f-32cd-40bd-9fb4-56a602113872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for rebalancing when NUM_PARTITIONS changes, but not necessary\n",
    "def repartition(fn, N, M):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with open(f\"{fn}.unified.csv\", \"w\") as f:\n",
    "        for t in range(N):\n",
    "            header = False\n",
    "            with open(fn + f\".{t}.csv\") as infile:\n",
    "                for line in tqdm(infile):\n",
    "                    if not header:\n",
    "                        header = True\n",
    "                        if t == 0:\n",
    "                            f.write(line)\n",
    "                        continue\n",
    "                    f.write(line)\n",
    "        os.remove(fn + f\".{t}.csv\")\n",
    "\n",
    "    with open(f\"{fn}.unified.csv\") as infile:\n",
    "        files = [open(fn + f\".{t}.csv\", \"w\") for t in range(M)]\n",
    "        for t in range(N):\n",
    "            header = False\n",
    "            for line in tqdm(infile):\n",
    "                if not header:\n",
    "                    header = True\n",
    "                    for f in files:\n",
    "                        f.write(line)\n",
    "                        usercol = line.strip().split(\",\").index(\"username\")\n",
    "                    continue\n",
    "                username = line.strip().split(\",\")[usercol]\n",
    "                files[portable_hash(username) % M].write(line)\n",
    "        for f in files:\n",
    "            f.close()\n",
    "    os.remove(f\"{fn}.unified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c74bb-804d-4d40-aed3-524c6c6a09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorts users by how recent their last refresh was and returns the K oldest\n",
    "# only considers users in the given partition\n",
    "# TODO generational mark-sweep prioritization\n",
    "def prioritize_users(K):\n",
    "    logger.info(\n",
    "        f\"Prioritizing users from partition {PARTITION+1} out of {NUM_PARTITIONS}\"\n",
    "    )\n",
    "    usernames = {\n",
    "        x for x in read_usernames() if (portable_hash(x) % NUM_PARTITIONS) == PARTITION\n",
    "    }\n",
    "    user_status_file = STATUS_FILE\n",
    "    if not os.path.exists(user_status_file):\n",
    "        logger.info(\n",
    "            f\"Getting the lists of {len(usernames)} new users, \"\n",
    "            f\"refreshing the lists of {0} existing users, \"\n",
    "            f\"and skipping the lists of {0} broken users!\"\n",
    "        )\n",
    "        new_users = list(usernames)\n",
    "        random.shuffle(new_users)\n",
    "        return {x: 0 for x in new_users[:K]}\n",
    "    else:\n",
    "        oldest_existing_users = {}\n",
    "        num_existing_users = 0\n",
    "        num_broken_users = 0\n",
    "        with open(user_status_file, \"r\") as in_file:\n",
    "            header = False\n",
    "            for line in in_file:\n",
    "                if not header:\n",
    "                    header = True\n",
    "                    user_col = line.strip().split(\",\").index(\"username\")\n",
    "                    attempts_col = line.strip().split(\",\").index(\"failed_attempts\")\n",
    "                    continue\n",
    "                fields = line.strip().split(\",\")\n",
    "                username = fields[user_col]\n",
    "                attempts = int(fields[attempts_col])\n",
    "                if username not in usernames:\n",
    "                    continue\n",
    "                usernames.remove(username)\n",
    "                if attempts >= 3:\n",
    "                    num_broken_users += 1\n",
    "                else:\n",
    "                    num_existing_users += 1\n",
    "                    if len(oldest_existing_users) < K:\n",
    "                        oldest_existing_users[username] = attempts\n",
    "        new_users = list(usernames)\n",
    "        random.shuffle(new_users)\n",
    "        logger.info(\n",
    "            f\"Getting the lists of {len(new_users)} new users, \"\n",
    "            f\"refreshing the lists of {num_existing_users} existing users, \"\n",
    "            f\"and skipping the lists of {num_broken_users} broken users!\"\n",
    "        )\n",
    "        prioritized_users = {}\n",
    "        for user in new_users:\n",
    "            if len(prioritized_users) >= K:\n",
    "                break\n",
    "            prioritized_users[user] = 0\n",
    "        for user, value in oldest_existing_users.items():\n",
    "            if len(prioritized_users) >= K:\n",
    "                break\n",
    "            prioritized_users[user] = value\n",
    "        return prioritized_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbe5f8-eb51-482c-9b2b-0d833d42c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_failure_count = 0\n",
    "\n",
    "\n",
    "def monitor_failures(ok):\n",
    "    global consecutive_failure_count\n",
    "    if ok:\n",
    "        consecutive_failure_count = 0\n",
    "    else:\n",
    "        consecutive_failure_count += 1\n",
    "    if consecutive_failure_count >= 20:\n",
    "        logger.info(\n",
    "            f\"The most recent {consecutive_failure_count} attempts failed, pausing collection\"\n",
    "        )\n",
    "        time.sleep(3600)\n",
    "        consecutive_failure_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f61b54-8179-4dda-8e9a-a4d8a0b9234f",
   "metadata": {},
   "source": [
    "## Continuously refresh lists\n",
    "* We take the least recently refreshed users and refresh their lists\n",
    "* These lists are stored in a temporary block\n",
    "* Once the block is big enough, we atomically merge it with the existing lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098f0a2-fb1e-4022-a154-ad82a362d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_block(file, user_field, users):\n",
    "    outfile = file + \"~\"\n",
    "    blockfile = file + \".block\"\n",
    "    first_run = not os.path.exists(file)\n",
    "    with open(outfile, \"w\") as out_file:\n",
    "        # copy over all the unchaged users\n",
    "        if not first_run:\n",
    "            valid_usernames = {\n",
    "                x\n",
    "                for x in read_usernames()\n",
    "                if (portable_hash(x) % NUM_PARTITIONS) == PARTITION\n",
    "            }\n",
    "            with open(file, \"r\") as in_file:\n",
    "                header = False\n",
    "                for line in tqdm(in_file):\n",
    "                    if not header:\n",
    "                        header = True\n",
    "                        out_file.write(line)\n",
    "                        continue\n",
    "                    fields = line.strip().split(\",\")\n",
    "                    if (\n",
    "                        fields[user_field] not in users\n",
    "                        and fields[user_field] in valid_usernames\n",
    "                    ):\n",
    "                        out_file.write(line)\n",
    "\n",
    "        # copy over the new block\n",
    "        with open(blockfile, \"r\") as in_file:\n",
    "            header = False\n",
    "            for line in tqdm(in_file):\n",
    "                if not header:\n",
    "                    if first_run:\n",
    "                        out_file.write(line)\n",
    "                    header = True\n",
    "                    continue\n",
    "                out_file.write(line)\n",
    "    os.replace(outfile, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584609b8-ac89-49e6-b5d8-bcc3eaaab9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lock_name():\n",
    "    # we use a file lock to prevent uploading to aws in the\n",
    "    # middle of a database update\n",
    "    return f\"../../get_user_media_lists.{NAME}.lock\"\n",
    "\n",
    "\n",
    "def acquire_lock(fn):\n",
    "    logger.info(f\"Acquiring lock\")\n",
    "    file = get_lock_name()\n",
    "    while True:\n",
    "        sleep_time = 1\n",
    "        while os.path.exists(file):\n",
    "            time.sleep(sleep_time)\n",
    "            sleep_time = min(sleep_time * 2, 10)\n",
    "        try:\n",
    "            with open(file, \"x\") as _:\n",
    "                logger.info(f\"Acquired lock\")\n",
    "                fn()\n",
    "                return\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def release_lock():\n",
    "    logger.info(f\"Releasing lock\")\n",
    "    file = get_lock_name()\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def run_blocking_function(fn):\n",
    "    acquire_lock(fn)\n",
    "    release_lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f788a-4908-417d-b4d8-c76f1e18d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_blocks():\n",
    "    logger.info(f\"Merging block into the main database\")\n",
    "    users = set(\n",
    "        pd.read_csv(\n",
    "            f\"{STATUS_FILE}.block\",\n",
    "            keep_default_na=False,\n",
    "            usecols=[\"username\"],\n",
    "            dtype={\"username\": str},\n",
    "        )[\"username\"]\n",
    "    )\n",
    "    for medium in [\"anime\", \"manga\"]:\n",
    "        merge_block(media_list_file(medium), -1, users)\n",
    "    merge_block(STATUS_FILE, 0, users)\n",
    "    logger.info(f\"Merged block of {len(users)} users into the main database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee022e6-419a-4a8e-aed7-de796e88107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge any user lists from the last run\n",
    "if os.path.exists(f\"{STATUS_FILE}.block\"):\n",
    "    run_blocking_function(merge_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff37af-acda-491d-9f30-2deec9e4c6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # get the list for each new user and write to disk\n",
    "    while True:\n",
    "        gc.collect()\n",
    "        prioritized_users = prioritize_users(users_per_batch)\n",
    "        logger.info(\n",
    "            f\"Memory usage: {psutil.Process().memory_info().rss / 1e9} {psutil.Process().memory_info().vms / 1e9}\"\n",
    "        )\n",
    "        logger.info(f\"Fetching lists\")\n",
    "        block = set()\n",
    "        for username in tqdm(prioritized_users):\n",
    "            any_ok = False\n",
    "            updated_at = 0\n",
    "            for medium in [\"anime\", \"manga\"]:\n",
    "                user_media_list, ok = get_user_media_list(username, medium)\n",
    "                user_media_list.to_csv(\n",
    "                    f\"{media_list_file(medium)}.block\",\n",
    "                    index=False,\n",
    "                    mode=\"w\" if not block else \"a+\",\n",
    "                    header=not block,\n",
    "                )\n",
    "                any_ok |= ok\n",
    "                if len(user_media_list) > 0:\n",
    "                    updated_at = max(updated_at, user_media_list[\"updated_at\"].max())\n",
    "            user_status_entry = pd.DataFrame.from_dict(\n",
    "                {\n",
    "                    \"username\": [username],\n",
    "                    \"access_timestamp\": [int(datetime.datetime.now().timestamp())],\n",
    "                    \"last_update_timestamp\": [updated_at],\n",
    "                    \"failed_attempts\": [\n",
    "                        0 if any_ok else prioritized_users[username] + 1\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "            user_status_entry.to_csv(\n",
    "                f\"{STATUS_FILE}.block\",\n",
    "                index=False,\n",
    "                mode=\"w\" if not block else \"a+\",\n",
    "                header=not block,\n",
    "            )\n",
    "            block.add(username)\n",
    "            monitor_failures(ok)\n",
    "        run_blocking_function(merge_blocks)\n",
    "        block = None\n",
    "except Exception as e:\n",
    "    logger.info(str(e))\n",
    "    logger.info(f\"ERROR with {username}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
