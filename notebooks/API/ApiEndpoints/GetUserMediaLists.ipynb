{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1915b44a-3a42-431f-bd6c-94edf9e5bc36",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting user lists\n",
    "* Stores lists at `data/{NAME}/user_media_facts/user_{MEDIATYPE}_list.{PARTITION}.csv`\n",
    "* The notebook will only collect data for users whose hash is equal to PARTITION_NUMBER mod NUM_PARTITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a55928-bc2d-4dfb-974f-2480e13c1f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from hashlib import sha256\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from filelock import FileLock\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65822b0-3bec-4869-94e1-d956684c4bba",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f4b5d-cf96-48c8-8bd3-45f69036faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from API import anilist_api, animeplanet_api, api_setup, kitsu_api, mal_api\n",
    "\n",
    "PROXIES = api_setup.load_proxies(PROXY_NUMBER, NUM_PROXIES)\n",
    "if NAME == \"mal\":\n",
    "    mal_api.load_token(TOKEN_NUMBER)\n",
    "    api = mal_api\n",
    "    users_per_batch = 2000\n",
    "elif NAME == \"anilist\":\n",
    "    api = anilist_api\n",
    "    users_per_batch = 2000\n",
    "elif NAME == \"kitsu\":\n",
    "    api = kitsu_api\n",
    "    users_per_batch = 4000\n",
    "elif NAME == \"animeplanet\":\n",
    "    api = animeplanet_api\n",
    "    users_per_batch = 2000\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "SESSION = api.make_session(PROXIES, 1)\n",
    "get_user_media_list = lambda username, medium: api.get_user_media_list(\n",
    "    SESSION, username, medium\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50d2d6-3aa2-4ae2-b6cb-8daa401f6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdir\n",
    "data_path = f\"../../../data/{NAME}/user_media_facts\"\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b89df-ed09-412c-adc7-ef0cb605e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_FILE = f\"user_status.{PARTITION}.csv\"\n",
    "LOG_FILE = f\"get_user_media_lists.{PARTITION}.log\"\n",
    "LOCK_FILE = f\"../../get_user_media_lists.{NAME}.lock\"\n",
    "LOCK = FileLock(LOCK_FILE, timeout=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc7e24-ee38-4735-81b8-9d04f6a95e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def media_list_file(medium):\n",
    "    return f\"user_{medium}_list.{PARTITION}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d710c-4d9c-4f75-b204-1fb2853a4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(logfile):\n",
    "    name = \"get_media\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.handlers.clear()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(name)s:%(levelname)s:%(asctime)s: %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    for stream in [\n",
    "        logging.handlers.RotatingFileHandler(\n",
    "            logfile, \"w+\", maxBytes=1000000, backupCount=1\n",
    "        ),\n",
    "    ]:\n",
    "        stream.setFormatter(formatter)\n",
    "        logger.addHandler(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388e860-0e91-40d6-8d8e-0ac2e73be8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_logging(LOG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2c1b8-99ed-4bb3-8c15-338d0709db5a",
   "metadata": {},
   "source": [
    "## Sort users by recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a66a2a-5b92-4906-9a64-e83c80ba9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portable_hash(username):\n",
    "    return int(sha256(username.encode(\"utf-8\")).hexdigest(), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007b0d7f-32cd-40bd-9fb4-56a602113872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for rebalancing when NUM_PARTITIONS changes\n",
    "def repartition(fn, N, M):\n",
    "    with open(f\"{fn}.unified.csv\", \"w\") as f:\n",
    "        for t in range(N):\n",
    "            header = False\n",
    "            with open(fn + f\".{t}.csv\") as infile:\n",
    "                for line in tqdm(infile):\n",
    "                    if not header:\n",
    "                        header = True\n",
    "                        if t == 0:\n",
    "                            f.write(line)\n",
    "                        continue\n",
    "                    f.write(line)\n",
    "            os.remove(fn + f\".{t}.csv\")\n",
    "\n",
    "    with open(f\"{fn}.unified.csv\") as infile:\n",
    "        files = [open(fn + f\".{t}.csv\", \"w\") for t in range(M)]\n",
    "        header = False\n",
    "        for line in tqdm(infile):\n",
    "            if not header:\n",
    "                header = True\n",
    "                usercol = line.strip().split(\",\").index(\"username\")\n",
    "                for f in files:\n",
    "                    f.write(line)\n",
    "                continue\n",
    "            username = line.strip().split(\",\")[usercol]\n",
    "            files[portable_hash(username) % M].write(line)\n",
    "        for f in files:\n",
    "            f.close()\n",
    "    os.remove(f\"{fn}.unified.csv\")\n",
    "\n",
    "\n",
    "def repartition_all(N, M):\n",
    "    for base in [\"user_status\", \"user_manga_list\", \"user_anime_list\"]:\n",
    "        repartition(f\"../../../data/{NAME}/user_media_facts/{base}\", N, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c00c6d-6ae8-451c-a2f6-24b9facf06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_status_file(fn):\n",
    "    return pd.read_csv(\n",
    "        fn,\n",
    "        keep_default_na=False,\n",
    "        dtype={\"username\": str},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87981bb0-ac1c-4f0c-9c3a-5027db3936ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@LOCK\n",
    "def prioritize_users(K):\n",
    "    usernames = {\n",
    "        x for x in read_usernames() if (portable_hash(x) % NUM_PARTITIONS) == PARTITION\n",
    "    }\n",
    "    if not os.path.exists(STATUS_FILE):\n",
    "        broken_users = []\n",
    "        monthly_users = []\n",
    "        yearly_users = []\n",
    "        refresh_users = []\n",
    "        remaining_users = []\n",
    "        new_users = list(usernames)\n",
    "        failed_attempts = {}\n",
    "    else:\n",
    "        df = read_status_file(STATUS_FILE).sort_values(\n",
    "            by=[\"access_timestamp\", \"last_update_timestamp\"]\n",
    "        )\n",
    "        new_users = list(usernames - set(df.username))\n",
    "        q = \"failed_attempts >= 3\"\n",
    "        broken_users = list(df.query(q).username)\n",
    "        df = df.query(f\"not {q}\")\n",
    "        secs_per_day = 24 * 60 * 60\n",
    "        year = 365 * secs_per_day\n",
    "        month = 30 * secs_per_day\n",
    "        week = 7 * secs_per_day\n",
    "        now = datetime.datetime.now().timestamp()\n",
    "        monthly_users = list(\n",
    "            df.query(\n",
    "                f\"{week} <= access_timestamp - last_update_timestamp  < {month} \"\n",
    "                + f\"and access_timestamp < {now - week}\"\n",
    "            ).username\n",
    "        )\n",
    "        yearly_users = list(\n",
    "            df.query(\n",
    "                f\"{month} <= access_timestamp - last_update_timestamp  < {year} \"\n",
    "                f\"and access_timestamp < {now - month}\"\n",
    "            ).username\n",
    "        )\n",
    "        refresh_users = list(\n",
    "            df.query(f\" access_timestamp < {now - 2 * month}\").username\n",
    "        )\n",
    "        recent_users = set(monthly_users) | set(yearly_users) | set(refresh_users)\n",
    "        remaining_users = [x for x in df.username if x not in recent_users]\n",
    "        failed_attempts = (\n",
    "            read_status_file(STATUS_FILE)\n",
    "            .sort_values(by=[\"access_timestamp\", \"last_update_timestamp\"])\n",
    "            .set_index(\"username\")[\"failed_attempts\"]\n",
    "            .to_dict()\n",
    "        )\n",
    "    logging.info(\n",
    "        f\"Getting the lists of {len(new_users)} new users, \"\n",
    "        + f\"{len(monthly_users)} stale monthly users, \"\n",
    "        + f\"{len(yearly_users)} stale yearly users, \"\n",
    "        + f\"{len(refresh_users)} stale refresh users, \"\n",
    "        + f\"{len(remaining_users)} remaining users, \"\n",
    "        + f\"and skipping {len(broken_users)} broken users!\"\n",
    "    )\n",
    "    # randomize order\n",
    "    random.shuffle(broken_users)\n",
    "    random.shuffle(monthly_users)\n",
    "    random.shuffle(yearly_users)\n",
    "    random.shuffle(refresh_users)\n",
    "    random.shuffle(remaining_users)\n",
    "    random.shuffle(new_users)\n",
    "    # prioritize active users\n",
    "    N_monthly = int(round(K * 0.33))\n",
    "    N_yearly = int(round(K * 0.33))\n",
    "    N_broken = int(round(K * 0.01))\n",
    "    assert N_monthly > 0 and N_yearly > 0 and N_broken > 0\n",
    "    monthly_track = monthly_users[:N_monthly]\n",
    "    yearly_track = yearly_users[: N_yearly + (N_monthly - len(monthly_track))]\n",
    "    broken_track = broken_users[:N_broken]\n",
    "    refresh_track = refresh_users[:K]\n",
    "    remaining_track = remaining_users[:K]\n",
    "    order = (\n",
    "        new_users\n",
    "        + monthly_track\n",
    "        + yearly_track\n",
    "        + broken_track\n",
    "        + refresh_track\n",
    "        + remaining_track\n",
    "    )[:K]\n",
    "    random.shuffle(order)\n",
    "    return {x: failed_attempts.get(x, 0) for x in order}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbe5f8-eb51-482c-9b2b-0d833d42c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_failure_count = 0\n",
    "\n",
    "\n",
    "def monitor_failures(ok):\n",
    "    global consecutive_failure_count\n",
    "    if ok:\n",
    "        consecutive_failure_count = 0\n",
    "    else:\n",
    "        consecutive_failure_count += 1\n",
    "    if consecutive_failure_count >= 20:\n",
    "        timeout = 3600\n",
    "        logging.info(\n",
    "            f\"The most recent {consecutive_failure_count} attempts failed, \"\n",
    "            f\"pausing collection for {timeout} seconds\"\n",
    "        )\n",
    "        time.sleep(timeout)\n",
    "        consecutive_failure_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f61b54-8179-4dda-8e9a-a4d8a0b9234f",
   "metadata": {},
   "source": [
    "## Continuously refresh lists\n",
    "* We take the least recently refreshed users and refresh their lists\n",
    "* These lists are stored in a temporary block\n",
    "* Once the block is big enough, we atomically merge it with the existing lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098f0a2-fb1e-4022-a154-ad82a362d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@LOCK\n",
    "def merge_block(file, users):\n",
    "    outfile = file + \"~\"\n",
    "    blockfile = file + \".block\"\n",
    "    first_run = not os.path.exists(file)\n",
    "    with open(outfile, \"w\") as out_file:\n",
    "        # copy over all the unchaged users\n",
    "        if not first_run:\n",
    "            valid_usernames = {\n",
    "                x\n",
    "                for x in read_usernames()\n",
    "                if (portable_hash(x) % NUM_PARTITIONS) == PARTITION\n",
    "            }\n",
    "            with open(file, \"r\") as in_file:\n",
    "                header = False\n",
    "                for line in tqdm(in_file):\n",
    "                    fields = line.strip().split(\",\")\n",
    "                    if not header:\n",
    "                        header = True\n",
    "                        out_file.write(line)\n",
    "                        user_field = fields.index(\"username\")\n",
    "                        nfields = len(fields)\n",
    "                        continue\n",
    "                    assert len(fields) == nfields, line\n",
    "                    if (\n",
    "                        fields[user_field] not in users\n",
    "                        and fields[user_field] in valid_usernames\n",
    "                    ):\n",
    "                        out_file.write(line)\n",
    "\n",
    "        # copy over the new block\n",
    "        with open(blockfile, \"r\") as in_file:\n",
    "            header = False\n",
    "            for line in tqdm(in_file):\n",
    "                fields = line.strip().split(\",\")\n",
    "                if not header:\n",
    "                    if first_run:\n",
    "                        out_file.write(line)\n",
    "                    header = True\n",
    "                    nfields = len(fields)\n",
    "                    continue\n",
    "                assert len(fields) == nfields, line\n",
    "                out_file.write(line)\n",
    "    os.replace(outfile, file)\n",
    "    os.remove(blockfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f788a-4908-417d-b4d8-c76f1e18d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_blocks():\n",
    "    logging.info(f\"Merging block into the main database\")\n",
    "    users = set(read_status_file(f\"{STATUS_FILE}.block\")[\"username\"])\n",
    "    for medium in [\"anime\", \"manga\"]:\n",
    "        merge_block(media_list_file(medium), users)\n",
    "    merge_block(STATUS_FILE, users)\n",
    "    logging.info(f\"Merged block of {len(users)} users into the main database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee022e6-419a-4a8e-aed7-de796e88107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete any corrupted files from the previous run\n",
    "for file in [\n",
    "    STATUS_FILE + \".block\",\n",
    "    media_list_file(\"anime\") + \".block\",\n",
    "    media_list_file(\"manga\") + \".block\",\n",
    "]:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff37af-acda-491d-9f30-2deec9e4c6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # get the list for each new user and write to disk\n",
    "    prioritized_users = prioritize_users(users_per_batch)\n",
    "    logging.info(f\"Fetching lists\")\n",
    "    block = set()\n",
    "    for username in tqdm(prioritized_users):\n",
    "        any_ok = False\n",
    "        updated_at = 0\n",
    "        for medium in [\"anime\", \"manga\"]:\n",
    "            user_media_list, ok = get_user_media_list(username, medium)\n",
    "            any_ok |= ok\n",
    "            if len(user_media_list) > 0:\n",
    "                append = os.path.exists(media_list_file(medium) + \".block\")\n",
    "                user_media_list.to_csv(\n",
    "                    f\"{media_list_file(medium)}.block\",\n",
    "                    index=False,\n",
    "                    mode=\"a+\" if append else \"w\",\n",
    "                    header=not append,\n",
    "                )\n",
    "                updated_at = max(updated_at, user_media_list[\"updated_at\"].max())\n",
    "        user_status_entry = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"username\": [username],\n",
    "                \"access_timestamp\": [int(datetime.datetime.now().timestamp())],\n",
    "                \"last_update_timestamp\": [updated_at],\n",
    "                \"failed_attempts\": [0 if any_ok else prioritized_users[username] + 1],\n",
    "            }\n",
    "        )\n",
    "        user_status_entry.to_csv(\n",
    "            f\"{STATUS_FILE}.block\",\n",
    "            index=False,\n",
    "            mode=\"w\" if not block else \"a+\",\n",
    "            header=not block,\n",
    "        )\n",
    "        block.add(username)\n",
    "        monitor_failures(ok)\n",
    "    merge_blocks()\n",
    "except Exception as e:\n",
    "    # errors require human supervision\n",
    "    logging.error(traceback.format_exc())\n",
    "    logging.error(str(e))\n",
    "    logging.error(f\"ERROR with {username}\")\n",
    "    while True:\n",
    "        time.sleep(3600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
