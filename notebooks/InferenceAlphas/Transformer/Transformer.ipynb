{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a267fa3-cdb3-4a96-affc-6f03ee3fa040",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "* See the corresponding file in `../../TrainingAlphas` for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838fe4d8-cffa-40fb-a0ab-d34edc6d9fcd",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "username = \"\"\n",
    "task = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f2c65-9add-4db8-ae90-94ec7b96367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "const source_name = \"Transformer\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3bee8-6535-49b7-b0ae-d3fc19a73cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"../../TrainingAlphas/Transformer/Transformer.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b4abf-8ebd-4cd5-a13b-6e0a6549ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_training_data(include_ptw, cls_tokens)\n",
    "    function get_df(content)\n",
    "        df = get_raw_recommendee_split(content)\n",
    "        if content != \"explicit\"\n",
    "            df.rating .= 11\n",
    "        end\n",
    "        df\n",
    "    end\n",
    "\n",
    "    contents = [\"explicit\", \"implicit\"]\n",
    "    if include_ptw\n",
    "        push!(contents, \"ptw\")\n",
    "    end\n",
    "    sentences = Dict{Int32,Vector{word_type}}()\n",
    "    df = reduce(cat, [get_df(content) for content in contents])\n",
    "    order = sortperm(df.timestamp)\n",
    "    for idx = 1:length(order)\n",
    "        i = order[idx]\n",
    "        if df.user[i] âˆ‰ keys(sentences)\n",
    "            sentences[df.user[i]] = [replace_user(cls_tokens, df.user[i])]\n",
    "        end\n",
    "        word = encode_word(\n",
    "            df.item[i],\n",
    "            df.rating[i],\n",
    "            df.timestamp[i],\n",
    "            df.status[i],\n",
    "            df.completion[i],\n",
    "            df.user[i],\n",
    "            length(sentences[df.user[i]]),\n",
    "        )\n",
    "        push!(sentences[df.user[i]], word)\n",
    "    end\n",
    "    sentences[1]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed5d6f-f116-43a4-b354-e53a2bb23fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "function crop_sentence(s, task, max_seq_len, mask_tokens)\n",
    "    rng = Random.GLOBAL_RNG\n",
    "    if task == \"random\"\n",
    "        s = subset_sentence(s, max_seq_len - 1; recent = false, rng = rng)\n",
    "        masked_word = mask_tokens\n",
    "    elseif task in [\"causal\", \"temporal\"]\n",
    "        s = subset_sentence(s, max_seq_len - 1; recent = true, rng = rng)\n",
    "        masked_word = replace_position(mask_tokens, Int32(length(s)))\n",
    "        if task == \"temporal\"\n",
    "            masked_word = replace_timestamp(mask_tokens, 1)\n",
    "        end\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    push!(s, masked_word)\n",
    "    s\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f21dfe-c1e6-428a-af8f-7edf9c9e08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_inputs(sentences, max_seq_len, vocab_sizes, pad_tokens, cls_tokens)\n",
    "    rng = Random.GLOBAL_RNG\n",
    "\n",
    "    # dynamically pad to the largest sequence length\n",
    "    seq_len = min(maximum(length.(sentences)), max_seq_len)\n",
    "\n",
    "    # get tokenized sentences\n",
    "    tokens =\n",
    "        get_token_ids(sentences, seq_len, vocab_sizes[7], pad_tokens, cls_tokens; rng = rng)\n",
    "\n",
    "    # don't attend to padding tokens\n",
    "    attention_mask = reshape(\n",
    "        convert.(Float32, tokens[1] .!= pad_tokens[1]),\n",
    "        (1, seq_len, length(sentences)),\n",
    "    )\n",
    "    attention_mask = attention_mask .* permutedims(attention_mask, (2, 1, 3))\n",
    "\n",
    "    tokens, attention_mask\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c7d84-10a7-47e8-878b-a0649e2b2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_alpha()\n",
    "    checkpoint = readdir(get_data_path(\"alphas/random/Transformer/checkpoints/\"))[end]\n",
    "    params = read_params(\"random/Transformer/checkpoints/$checkpoint\")\n",
    "    if \"pretrain_checkpoint\" in keys(params)\n",
    "        pretrain_checkpoint = params[\"pretrain_checkpoint\"]\n",
    "    else\n",
    "        pretrain_checkpoint = \"all/Transformer/noprior/checkpoints/8\" # TODO remove after the next checkpoint\n",
    "    end\n",
    "    config = read_params(pretrain_checkpoint)[\"training_config\"]\n",
    "    model = params[\"m\"]\n",
    "\n",
    "    s = get_training_data(config[\"include_ptw_impressions\"], config[\"cls_tokens\"])\n",
    "    s = crop_sentence(s, task, config[\"max_sequence_length\"], config[\"mask_tokens\"])\n",
    "    tokens, attention_mask = get_inputs(\n",
    "        [s],\n",
    "        config[\"max_sequence_length\"],\n",
    "        config[\"vocab_sizes\"],\n",
    "        config[\"pad_tokens\"],\n",
    "        config[\"cls_tokens\"],\n",
    "    )\n",
    "\n",
    "    X = model.embed(\n",
    "        item = tokens[1],\n",
    "        rating = tokens[2],\n",
    "        timestamp = tokens[3],\n",
    "        status = tokens[4],\n",
    "        completion = tokens[5],\n",
    "        position = tokens[7],\n",
    "    )\n",
    "    X = model.transformers(X, attention_mask)\n",
    "    X = gather(X, [(size(X)[2], 1)])\n",
    "    item_preds =\n",
    "        transpose(model.embed.embeddings.item.embedding) *\n",
    "        model.classifier.item.transform(X) .+ model.classifier.item.output_bias.b\n",
    "    rating_preds = model.classifier.rating.transform(X)\n",
    "    item_preds = softmax(item_preds[1:num_items()])\n",
    "    rating_preds = rating_preds[1:num_items()]\n",
    "\n",
    "    write_recommendee_alpha(rating_preds, \"$task/Transformer/explicit\")\n",
    "    write_recommendee_alpha(item_preds, \"$task/Transformer/implicit\")\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b760c892-3765-4c0c-89cd-14899a4d8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_alpha()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
