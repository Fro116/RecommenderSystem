{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@functools.wraps(smf.ols)\n",
    "def lm(*args, **kwargs):\n",
    "    return smf.ols(*args, **kwargs).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to get recommendations for a different user\n",
    "recommendee = \"Fro116\"\n",
    "item_neighborhood_size = 64\n",
    "user_neighborhood_size = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache()\n",
    "def get_data():\n",
    "    raw_df = pd.read_csv(\"UserAnimeList.csv\")\n",
    "    filtered_df = raw_df[[\"username\", \"anime_id\", \"my_score\"]].loc[\n",
    "        lambda x: x[\"my_score\"] != 0\n",
    "    ]\n",
    "\n",
    "    # add additional user anime-lists\n",
    "    extraUsers = pickle.load(open(\"user_profiles/ExtraUserAnimeLists.pkl\", \"rb\"))\n",
    "    filtered_df = filtered_df.loc[lambda x: ~x[\"username\"].isin(extraUsers.username)]\n",
    "    filtered_df = pd.concat([filtered_df, extraUsers], ignore_index=True)\n",
    "\n",
    "    average_rating = filtered_df[\"my_score\"].mean()\n",
    "    user_bias = (\n",
    "        pd.DataFrame(filtered_df.groupby(\"username\")[\"my_score\"].mean()).rename(\n",
    "            {\"my_score\": \"user_bias\"}, axis=1\n",
    "        )\n",
    "        - average_rating\n",
    "    )\n",
    "    anime_bias = (\n",
    "        pd.DataFrame(filtered_df.groupby(\"anime_id\")[\"my_score\"].mean()).rename(\n",
    "            {\"my_score\": \"anime_bias\"}, axis=1\n",
    "        )\n",
    "        - average_rating\n",
    "    )\n",
    "\n",
    "    filtered_df = filtered_df.merge(anime_bias, on=[\"anime_id\"]).merge(\n",
    "        user_bias, on=[\"username\"]\n",
    "    )\n",
    "    filtered_df[\"blp\"] = (\n",
    "        filtered_df[\"anime_bias\"] + filtered_df[\"user_bias\"] + average_rating\n",
    "    )\n",
    "    filtered_df[\"normalized_score\"] = filtered_df[\"my_score\"] - filtered_df[\"blp\"]\n",
    "    filtered_df = filtered_df.set_index(\"username\")\n",
    "    filtered_df = filtered_df.dropna()\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta(score):\n",
    "    return score.groupby(\"anime_id\").apply(\n",
    "        lambda x: np.dot(x[\"normalized_score\"], x[\"corr\"]) / x[\"corr\"].abs().sum()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_variance(score):\n",
    "    # The following formulae are used to compute the variance of the delta. Delta\n",
    "    # is a weighted sum of the form δ = Σ(s_i * w_i) / (Σw_i), where s_i is\n",
    "    # a vector scores and w_i is the weight.\n",
    "    #\n",
    "    # By linearity, it suffices to compute (s_i * w_i) / (Σw_i). We assume that\n",
    "    # Var(s_i) is the same as the variance over all items s_i has rated). We treat\n",
    "    # w_i as a random variable with mean w_i and variance corr['corr_var']\n",
    "    #\n",
    "    # The variance for (w_i) / (Σw_i) can be estimated by doing a Taylor Approximation.\n",
    "    # See equation 20 of https://www.stat.cmu.edu/~hseltman/files/ratio.pdf. The\n",
    "    # formula for the ratio of two correlated variables R,S is\n",
    "    # Var(R/S) = E[R]^2/E[S]^2(Var[R]/E[R]^2 - 2Cov(R,S)/(E[R]E[S]) + Var[S]/E[S]^2)\n",
    "    #\n",
    "    # Lastly we take the product distribution of s_i and (w_i) / (Σw_i).\n",
    "    def correction_factor(x):\n",
    "        return (\n",
    "            1\n",
    "            + x[\"corr_var\"] / (x[\"corr\"] ** 2)\n",
    "            - 2 * x[\"corr_var\"] / (x[\"corr\"].abs().sum() * x[\"corr\"].abs())\n",
    "            + x[\"corr_var\"].sum() / (x[\"corr\"].abs().sum() ** 2)\n",
    "        )\n",
    "\n",
    "    delta_var = score.groupby(\"anime_id\").apply(\n",
    "        lambda x: np.sum(\n",
    "            x[\"normalized_score_var\"] * x[\"corr\"] ** 2 * correction_factor(x)\n",
    "        )\n",
    "        / (x[\"corr\"].abs().sum() ** 2)\n",
    "    )\n",
    "\n",
    "    # if the var < 0, then the ratio distribution approximation failed,\n",
    "    # usually because sample size is too small\n",
    "    delta_var.loc[lambda x: x < 0] = np.inf\n",
    "\n",
    "    #     # The above is a biased estimator of the variance. To unbias the estimator,\n",
    "    #     # we need to apply a Bessel-like correction. See the formula in\n",
    "    #     # (https://stats.stackexchange.com/questions/47325/bias-correction-in-weighted-variance)\n",
    "    #     bias_correction = (\n",
    "    #         score.set_index(\"anime_id\")\n",
    "    #         .loc[score.groupby(\"anime_id\").size() > 1]\n",
    "    #         .groupby(\"anime_id\")\n",
    "    #         .apply(\n",
    "    #             lambda x: (x[\"corr\"].abs().sum() ** 2)\n",
    "    #             / (x[\"corr\"].abs().sum() ** 2 - (x[\"corr\"] ** 2).sum())\n",
    "    #         )\n",
    "    #     )\n",
    "    #     delta_var *= bias_correction\n",
    "\n",
    "    # Apply a bessel correction to unbias the variance\n",
    "    effective_sample_size = score.groupby(\"anime_id\")[\"effective_sample_size\"].median()\n",
    "    delta_var.loc[effective_sample_size <= 1] = np.inf\n",
    "    delta_var.loc[effective_sample_size > 1] *= effective_sample_size / (\n",
    "        effective_sample_size - 1\n",
    "    )\n",
    "\n",
    "    return delta_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deltas(is_df, anime_ids, recommendee, neighborhood_size, score_fn):\n",
    "    # get the neighborhood for each item\n",
    "    score = score_fn(is_df, recommendee, neighborhood_size)\n",
    "\n",
    "    # extract model features\n",
    "    pred_df = pd.DataFrame()\n",
    "    pred_df[\"delta\"] = get_delta(score)\n",
    "    pred_df[\"delta_var\"] = get_delta_variance(score)\n",
    "    pred_df = pred_df.loc[lambda x: x.index.isin(anime_ids)]\n",
    "\n",
    "    # fill in missing predictions with nan\n",
    "    for anime_id in set(anime_ids) - set(pred_df.index):\n",
    "        pred_df = pred_df.append(pd.Series(name=anime_id, dtype=float))\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache()\n",
    "def get_item_corrs():\n",
    "    corrs = pickle.load(open(\"item_correlations/correlations.pkl\", \"rb\"))\n",
    "    corrs[\"similarity\"] = corrs[\"corr\"].abs()\n",
    "    corrs = corrs.dropna()\n",
    "    corrs = corrs.loc[\n",
    "        lambda x: x.index.get_level_values(\"anime_id_x\")\n",
    "        != x.index.get_level_values(\"anime_id_y\")\n",
    "    ]\n",
    "    corrs = corrs.sort_values(by=\"similarity\")\n",
    "    return corrs\n",
    "\n",
    "\n",
    "def get_item_scores(df, recommendee, neighborhood_size):\n",
    "    corrs = get_item_corrs()\n",
    "    corrs = corrs.groupby(\"anime_id_x\").tail(neighborhood_size)\n",
    "    score = df.loc[recommendee].merge(\n",
    "        corrs.reset_index(\"anime_id_x\"), left_on=\"anime_id\", right_on=\"anime_id_y\",\n",
    "    )\n",
    "\n",
    "    user_var = (\n",
    "        pd.DataFrame(df.groupby(\"username\")[\"normalized_score\"].var())\n",
    "        .rename({\"normalized_score\": \"user_var\"}, axis=1)\n",
    "        .dropna()\n",
    "    )\n",
    "    score[\"normalized_score_var\"] = user_var.loc[recommendee].squeeze()\n",
    "    score = score.drop(\"anime_id\", axis=1).rename({\"anime_id_x\": \"anime_id\"}, axis=1)\n",
    "\n",
    "    weights = score.groupby(\"anime_id\").apply(lambda x: x[\"corr\"].abs().sum())\n",
    "    average_weight = corrs.groupby(\"anime_id_x\").apply(lambda x: x[\"corr\"].abs().mean())\n",
    "    average_weight.index.rename('anime_id', inplace=True)    \n",
    "    effective_sample_size = pd.DataFrame(weights / average_weight).rename(\n",
    "        {0: \"effective_sample_size\"}, axis=1\n",
    "    )\n",
    "    score = score.merge(effective_sample_size, on=\"anime_id\")\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_corrs(df, recommendee):\n",
    "    user_subset = df.loc[[recommendee]].merge(df.reset_index(), on=\"anime_id\")\n",
    "    corr_numerator = user_subset.groupby(\"username\").apply(\n",
    "        lambda x: np.dot(x[\"normalized_score_x\"], x[\"normalized_score_y\"])\n",
    "    )\n",
    "    corr_denom = df.groupby(\"username\").apply(\n",
    "        lambda x: np.sqrt(np.dot(x[\"normalized_score\"], x[\"normalized_score\"]))\n",
    "    )\n",
    "    corr_denom *= corr_denom.loc[recommendee]\n",
    "    corrs = pd.DataFrame((corr_numerator / corr_denom), columns=[\"corr\"])\n",
    "    corrs[\"similarity\"] = corrs[\"corr\"].abs()\n",
    "    corrs[\"corr_size\"] = user_subset.groupby(\"username\").size()\n",
    "    corrs = corrs.drop(recommendee)\n",
    "    corrs = corrs.dropna()\n",
    "    return corrs\n",
    "\n",
    "\n",
    "def get_user_scores(df, recommendee, neighborhood_size):\n",
    "    corrs = get_user_corrs(df, recommendee)\n",
    "\n",
    "    # We assume variance is the same as the variance for pearson correlation.\n",
    "    # see https://www.jstor.org/stable/2277400?seq=1\n",
    "    corrs = corrs.loc[lambda x: x[\"corr_size\"] > 2]\n",
    "    corrs[\"corr_var\"] = (1 - corrs[\"corr\"] * corrs[\"corr\"]) ** 2 / (\n",
    "        corrs[\"corr_size\"] - 2\n",
    "    )\n",
    "    corrs = corrs.sort_values(by=\"similarity\").dropna()[-neighborhood_size:]\n",
    "\n",
    "    score = (df.merge(pd.DataFrame(corrs), on=\"username\")).dropna()\n",
    "\n",
    "    user_var = (\n",
    "        pd.DataFrame(df.groupby(\"username\")[\"normalized_score\"].var())\n",
    "        .rename({\"normalized_score\": \"normalized_score_var\"}, axis=1)\n",
    "        .dropna()\n",
    "    )\n",
    "    score = score.merge(user_var, on=\"username\")\n",
    "\n",
    "    weights = score.groupby(\"anime_id\").apply(lambda x: x[\"corr\"].abs().sum())\n",
    "    average_weight = corrs[\"corr\"].abs().mean()\n",
    "    effective_sample_size = pd.DataFrame(weights / average_weight).rename(\n",
    "        {0: \"effective_sample_size\"}, axis=1\n",
    "    )\n",
    "    score = score.merge(effective_sample_size, on=\"anime_id\")\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_deltas(recommendee, neighborhood_size, score_fn, delta_name):\n",
    "    df = get_data()\n",
    "\n",
    "    # compute cross-validated deltas\n",
    "    oos_pred_dfs = []\n",
    "    K = len(df.loc[recommendee])\n",
    "    np.random.seed(1)\n",
    "    splits = np.array_split(df.loc[recommendee].sample(frac=1), K)\n",
    "    for split in tqdm(splits):\n",
    "        oos_df = split\n",
    "        is_df = df.loc[\n",
    "            lambda x: ~(\n",
    "                (x.index.get_level_values(\"username\") == recommendee)\n",
    "                & x.anime_id.isin(oos_df.anime_id)\n",
    "            )\n",
    "        ]\n",
    "        oos_pred_df = get_deltas(\n",
    "            is_df=is_df,\n",
    "            anime_ids=list(oos_df.anime_id),\n",
    "            recommendee=recommendee,\n",
    "            neighborhood_size=neighborhood_size,\n",
    "            score_fn=score_fn,\n",
    "        )\n",
    "        oos_pred_dfs.append(oos_pred_df)\n",
    "    oos_pred_df = pd.concat(oos_pred_dfs)\n",
    "\n",
    "    # compute deltas over the full data\n",
    "    is_pred_df = get_deltas(\n",
    "        is_df=df,\n",
    "        anime_ids=list(df.anime_id),\n",
    "        recommendee=recommendee,\n",
    "        neighborhood_size=neighborhood_size,\n",
    "        score_fn=score_fn,\n",
    "    )\n",
    "\n",
    "    # store deltas\n",
    "    outdir = f\"deltas/{recommendee}\"\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    oos_pred_df.to_pickle(os.path.join(outdir, f\"{delta_name}_oos.pkl\"))\n",
    "    is_pred_df.to_pickle(os.path.join(outdir, f\"{delta_name}_is.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_baselines(recommendee):\n",
    "    df = get_data()\n",
    "    df.loc[recommendee][[\"anime_id\", \"normalized_score\"]].to_pickle(\n",
    "        f\"deltas/{recommendee}/recommendee.pkl\"\n",
    "    )\n",
    "\n",
    "    average_rating = df[\"my_score\"].mean()\n",
    "    user_bias = (\n",
    "        pd.DataFrame(df.groupby(\"username\")[\"my_score\"].mean()).rename(\n",
    "            {\"my_score\": \"user_bias\"}, axis=1\n",
    "        )\n",
    "        - average_rating\n",
    "    )\n",
    "    anime_bias = (\n",
    "        pd.DataFrame(df.groupby(\"anime_id\")[\"my_score\"].mean()).rename(\n",
    "            {\"my_score\": \"anime_bias\"}, axis=1\n",
    "        )\n",
    "        - average_rating\n",
    "    )\n",
    "    blp = anime_bias + user_bias.loc[recommendee].squeeze() + average_rating\n",
    "    blp = blp.rename({\"anime_bias\": \"blp\"}, axis=1)\n",
    "    blp.to_pickle(f\"deltas/{recommendee}/blp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [3:51:50<00:00, 37.09s/it]  \n"
     ]
    }
   ],
   "source": [
    "store_deltas(\n",
    "    recommendee=recommendee,\n",
    "    neighborhood_size=item_neighborhood_size,\n",
    "    score_fn=get_item_scores,\n",
    "    delta_name=\"item\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [8:58:38<00:00, 86.18s/it]   \n"
     ]
    }
   ],
   "source": [
    "store_deltas(\n",
    "    recommendee=recommendee,\n",
    "    neighborhood_size=user_neighborhood_size,\n",
    "    score_fn=get_user_scores,\n",
    "    delta_name=\"user\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_baselines(recommendee=recommendee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
