{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "@functools.wraps(smf.ols)\n",
    "def lm(*args, **kwargs):\n",
    "    return smf.ols(*args, **kwargs).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendee = \"Fro116\"\n",
    "neighborhood_size = 8192\n",
    "confidence_interval = 0.99\n",
    "normalize_variance = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = pd.read_csv(\"AnimeList.csv\")\n",
    "anime = anime[[\"anime_id\", \"title\", \"type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"UserAnimeList.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"username\"].unique()), len(df[\"anime_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[[\"username\", \"anime_id\", \"my_score\"]].loc[lambda x: x[\"my_score\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(file, username):\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    xml_data = open(file, \"r\").read()  # Read file\n",
    "    root = ET.XML(xml_data)  # Parse XML\n",
    "\n",
    "    data = []\n",
    "    cols = []\n",
    "    for i, child in enumerate(root):\n",
    "        data.append([subchild.text for subchild in child])\n",
    "        cols.append(child.tag)\n",
    "    new_list = pd.DataFrame(data).T\n",
    "    new_list.columns = cols\n",
    "\n",
    "    df = new_list.loc[[0, 9]].T.dropna().rename({0: \"anime_id\", 9: \"my_score\"}, axis=1)\n",
    "    df[\"username\"] = username\n",
    "    df[\"anime_id\"] = df[\"anime_id\"].astype(int)\n",
    "    df[\"my_score\"] = df[\"my_score\"].astype(int)\n",
    "    df[\"username\"] = df[\"username\"].astype(str)\n",
    "    df = df.loc[lambda x: x[\"my_score\"] != 0]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_user(full_df, xml_file, username):\n",
    "    user_df = read_xml(xml_file, username)\n",
    "    without_user = full_df.loc[lambda x: x[\"username\"] != username]\n",
    "    return pd.concat([without_user, user_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = add_user(filtered_df, \"user_profiles/Fro116.xml\", \"Fro116\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rating = filtered_df[\"my_score\"].mean()\n",
    "user_bias = (\n",
    "    pd.DataFrame(filtered_df.groupby(\"username\")[\"my_score\"].mean()).rename(\n",
    "        {\"my_score\": \"user_bias\"}, axis=1\n",
    "    )\n",
    "    - average_rating\n",
    ")\n",
    "anime_bias = (\n",
    "    pd.DataFrame(filtered_df.groupby(\"anime_id\")[\"my_score\"].mean()).rename(\n",
    "        {\"my_score\": \"anime_bias\"}, axis=1\n",
    "    )\n",
    "    - average_rating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.merge(anime_bias, on=[\"anime_id\"]).merge(\n",
    "    user_bias, on=[\"username\"]\n",
    ")\n",
    "filtered_df[\"normalized_score\"] = (\n",
    "    filtered_df[\"my_score\"]\n",
    "    - filtered_df[\"anime_bias\"]\n",
    "    - filtered_df[\"user_bias\"]\n",
    "    - average_rating\n",
    ")\n",
    "filtered_df = filtered_df.set_index(\"username\")\n",
    "filtered_df = filtered_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize_variance:\n",
    "    user_stds = (\n",
    "        filtered_df.groupby(\"username\")[[\"normalized_score\"]]\n",
    "        .std()\n",
    "        .rename({\"normalized_score\": \"user_std\"}, axis=1)\n",
    "    )\n",
    "    filtered_df = filtered_df.merge(user_stds, on=\"username\")\n",
    "    filtered_df[\"normalized_score\"] /= filtered_df[\"user_std\"]\n",
    "    filtered_df = filtered_df.drop(\"user_std\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_subset = filtered_df.loc[[recommendee]].merge(\n",
    "    filtered_df.reset_index(), on=\"anime_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_cos_corr_numerator = user_subset.groupby(\"username\").apply(\n",
    "    lambda x: np.dot(x[\"normalized_score_x\"], x[\"normalized_score_y\"])\n",
    ")\n",
    "adj_cos_corr_denom = filtered_df.groupby(\"username\").apply(\n",
    "    lambda x: np.sqrt(np.dot(x[\"normalized_score\"], x[\"normalized_score\"]))\n",
    ")\n",
    "adj_cos_corr_denom *= adj_cos_corr_denom.loc[recommendee]\n",
    "adj_cos_corrs = pd.DataFrame(\n",
    "    (adj_cos_corr_numerator / adj_cos_corr_denom), columns=[\"corr\"]\n",
    ")\n",
    "adj_cos_corrs = adj_cos_corrs.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = adj_cos_corrs.copy()\n",
    "corrs[\"similarity\"] = corrs[\"corr\"].abs()\n",
    "corrs[\"size\"] = user_subset.groupby(\"username\").size()\n",
    "corrs = corrs.drop(\n",
    "    recommendee\n",
    ")  # Technically not needed because it's a noop for new series, but its useful for debugging\n",
    "\n",
    "# We assume variance is the same as the variance for pearson correlation.\n",
    "# see https://www.jstor.org/stable/2277400?seq=1\n",
    "corrs = corrs.loc[lambda x: x[\"size\"] > 2]\n",
    "corrs[\"corr_var\"] = (1 - corrs[\"corr\"] * corrs[\"corr\"]) ** 2 / (corrs[\"size\"] - 2)\n",
    "corrs = corrs.sort_values(by=\"similarity\").dropna()[-neighborhood_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = (filtered_df.merge(pd.DataFrame(corrs), on=\"username\")).dropna()\n",
    "\n",
    "user_var = (\n",
    "    pd.DataFrame(filtered_df.groupby(\"username\")[\"normalized_score\"].var())\n",
    "    .rename({\"normalized_score\": \"user_var\"}, axis=1)\n",
    "    .dropna()\n",
    ")\n",
    "score = score.merge(user_var, on=\"username\")\n",
    "\n",
    "anime_var = (\n",
    "    pd.DataFrame(filtered_df.groupby(\"anime_id\")[\"normalized_score\"].var())\n",
    "    .rename({\"normalized_score\": \"anime_var\"}, axis=1)\n",
    "    .dropna()\n",
    ")\n",
    "score = score.merge(anime_var, on=\"anime_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = score.groupby(\"anime_id\").apply(\n",
    "    lambda x: np.dot(x[\"normalized_score\"], x[\"corr\"]) / x[\"corr\"].abs().sum()\n",
    ")\n",
    "weights = score.groupby(\"anime_id\").apply(lambda x: x[\"corr\"].abs().sum())\n",
    "counts = score.groupby(\"anime_id\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following formulae are used to compute the variance of the delta. Delta\n",
    "# is a weighted sum of the form δ = Σ(s_i * w_i) / (Σw_i), where s_i is\n",
    "# a vector scores for user i and w_i is the weight for user_i.\n",
    "#\n",
    "# By linearity, it suffices to compute (s_i * w_i) / (Σw_i). We assume that\n",
    "# Var(s_i) is the same as the variance over the vector s_i (i.e. over\n",
    "# all items s_i has rated). We treat w_i as a random variable with mean w_i\n",
    "# and variance corr['corr_var']\n",
    "#\n",
    "# The variance for (w_i) / (Σw_i) can be estimated by doing a Taylor Approximation.\n",
    "# See equation 20 of https://www.stat.cmu.edu/~hseltman/files/ratio.pdf. The\n",
    "# formula for the ratio of two correlated variables R,S is\n",
    "# Var(R/S) = E[R]^2/E[S]^2(Var[R]/E[R]^2 - 2Cov(R,S)/(E[R]E[S]) + Var[S]/E[S]^2)\n",
    "#\n",
    "# Lastly we take the product distribution of s_i and (w_i) / (Σw_i).\n",
    "def correction_factor(x):\n",
    "    return (\n",
    "        1\n",
    "        + x[\"corr_var\"] / (x[\"corr\"] ** 2)\n",
    "        - 2 * x[\"corr_var\"] / (x[\"corr\"].abs().sum() * x[\"corr\"].abs())\n",
    "        + x[\"corr_var\"].sum() / (x[\"corr\"].abs().sum() ** 2)\n",
    "    )\n",
    "\n",
    "\n",
    "delta_var = score.groupby(\"anime_id\").apply(\n",
    "    lambda x: np.sum(x[\"user_var\"] * x[\"corr\"] ** 2 * correction_factor(x))\n",
    "    / (x[\"corr\"].abs().sum() ** 2)\n",
    ")\n",
    "\n",
    "# if the var < 0, then the ratio distribution approximation failed,\n",
    "# usually because sample size is too small\n",
    "delta_var.loc[lambda x: x < 0] = np.inf\n",
    "\n",
    "# The above is a biased estimator of the variance. To unbias the estimator,\n",
    "# we need to apply a Bessel-like correction. See the formula in\n",
    "# (https://stats.stackexchange.com/questions/47325/bias-correction-in-weighted-variance)\n",
    "bias_correction = (\n",
    "    score.set_index(\"anime_id\")\n",
    "    .loc[counts > 1]\n",
    "    .groupby(\"anime_id\")\n",
    "    .apply(\n",
    "        lambda x: (x[\"corr\"].abs().sum() ** 2)\n",
    "        / (x[\"corr\"].abs().sum() ** 2 - (x[\"corr\"] ** 2).sum())\n",
    "    )\n",
    ")\n",
    "delta_var *= bias_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "pred_df[\"delta\"] = deltas\n",
    "pred_df[\"weight\"] = weights\n",
    "pred_df[\"counts\"] = counts\n",
    "pred_df[\"delta_sem\"] = np.sqrt(delta_var)\n",
    "pred_df[\"blp\"] = anime_bias + user_bias.loc[recommendee].squeeze() + average_rating\n",
    "pred_df = pred_df.dropna()\n",
    "\n",
    "recomendee_seen_shows = filtered_df.loc[recommendee].merge(pred_df, on=[\"anime_id\"])\n",
    "recomendee_seen_shows[\"target\"] = (\n",
    "    recomendee_seen_shows[\"my_score\"] - recomendee_seen_shows[\"blp\"]\n",
    ")\n",
    "model = lm(\"target ~ delta + 0\", recomendee_seen_shows)\n",
    "pred_df[\"score\"] = model.predict(pred_df) + pred_df[\"blp\"]\n",
    "pred_df[\"sem\"] = np.sqrt(\n",
    "    (\n",
    "        (pred_df[\"delta_sem\"] ** 2 + pred_df[\"delta\"] ** 2)\n",
    "        * (model.bse[\"delta\"] ** 2 + model.params[\"delta\"] ** 2)\n",
    "    )\n",
    "    - pred_df[\"delta\"] ** 2 * model.params[\"delta\"] ** 2\n",
    ")\n",
    "\n",
    "zscore = st.norm.ppf(1 - (1 - confidence_interval) / 2)\n",
    "pred_df[\"score_lower_bound\"] = pred_df[\"score\"] - pred_df[\"sem\"] * zscore\n",
    "pred_df[\"score_upper_bound\"] = pred_df[\"score\"] + pred_df[\"sem\"] * zscore\n",
    "\n",
    "pred_df = pred_df.merge(anime, on=\"anime_id\")\n",
    "pred_df = pred_df.set_index(\"anime_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that setting blp = 1 is reasonable\n",
    "print(lm(\"my_score ~ delta + blp + 0\", recomendee_seen_shows).summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the top shows are ones that the user rates highly\n",
    "pred_df.sort_values(by=\"score_lower_bound\", ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_recs = pred_df.drop(filtered_df.loc[recommendee].anime_id, errors=\"ignore\").loc[\n",
    "    lambda x: (x[\"type\"] != \"Movie\")\n",
    "    & (x[\"type\"] != \"Special\")\n",
    "    & (x[\"type\"] != \"OVA\")\n",
    "    & (x[\"type\"] != \"ONA\")\n",
    "    & (x[\"type\"] != \"Music\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_recs.loc[lambda x: (x[\"delta\"] > 0)].sort_values(\n",
    "    by=\"score_lower_bound\", ascending=False\n",
    ")[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(\n",
    "    new_recs.loc[lambda x: (x[\"delta\"] > 0)].sort_values(by=\"score_lower_bound\")[-20:][\n",
    "        \"title\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_pickle(\"deltas/user.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.loc[recommendee].to_pickle(\"deltas/recommendee.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
