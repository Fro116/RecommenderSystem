{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df5351f-2171-4365-b750-8984d42c5fe5",
   "metadata": {},
   "source": [
    "# NonlinearModel\n",
    "* Uses LightGBM to estimate a given (user, item) prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b5a19a-24f5-4730-afe2-f451d27d18e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: lib_lightgbm not found in system dirs, trying fallback\n",
      "└ @ LightGBM /home/kundan/.julia/packages/LightGBM/A7zVd/src/LightGBM.jl:25\n"
     ]
    }
   ],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"TreeModelBase.ipynb\")\n",
    "@nbinclude(\"SuppressImplicit.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db4a783-c061-40b7-9a1d-f5bf8aa30aae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:21:07 getting features for implicit, validation\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.48 μs/it)\u001b[39m/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:22:02 getting features for negative, validation\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 (11.69 ns/it)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4, test_1's binary_logloss: 0.5400266031809134, \n",
      "Iteration: 4, test_1's auc: 0.9209139309835374\n",
      "Iteration: 5, test_1's binary_logloss: 0.5194604445774033, \n",
      "Iteration: 5, test_1's auc: 0.9176674063803004\n",
      "Iteration: 6, test_1's binary_logloss: 0.48420849955690654, \n",
      "Iteration: 6, test_1's auc: 0.9343893493559577\n",
      "Iteration: 7, test_1's binary_logloss: 0.46918453450400616, \n",
      "Iteration: 7, test_1's auc: 0.9320411668315971\n",
      "Iteration: 8, test_1's binary_logloss: 0.440330843520026, \n",
      "Iteration: 8, test_1's auc: 0.9412532541026536\n",
      "Iteration: 9, test_1's binary_logloss: 0.4179579910679605, \n",
      "Iteration: 9, test_1's auc: 0.9457847684825069\n",
      "Iteration: 10, test_1's binary_logloss: 0.39869369861872256, \n",
      "Iteration: 10, test_1's auc: 0.9487929442725842\n",
      "Iteration: 11, test_1's binary_logloss: 0.3904944353085416, \n",
      "Iteration: 11, test_1's auc: 0.9478460601844865\n",
      "Iteration: 12, test_1's binary_logloss: 0.3839235963620761, \n",
      "Iteration: 12, test_1's auc: 0.9469800188207529\n",
      "Iteration: 13, test_1's binary_logloss: 0.3771941305386837, \n",
      "Iteration: 13, test_1's auc: 0.9460941340783865\n",
      "Iteration: 14, test_1's binary_logloss: 0.3592769377265748, \n",
      "Iteration: 14, test_1's auc: 0.950444196748317\n",
      "Iteration: 15, test_1's binary_logloss: 0.34596198121455374, \n",
      "Iteration: 15, test_1's auc: 0.9528170363672005\n",
      "Iteration: 16, test_1's binary_logloss: 0.3414491236468641, \n",
      "Iteration: 16, test_1's auc: 0.9524698149662323\n",
      "Iteration: 17, test_1's binary_logloss: 0.33801452910568186, \n",
      "Iteration: 17, test_1's auc: 0.9519946919695683\n",
      "Iteration: 18, test_1's binary_logloss: 0.3283052323928508, \n",
      "Iteration: 18, test_1's auc: 0.9535595999167351\n",
      "Iteration: 19, test_1's binary_logloss: 0.3147938036905041, \n",
      "Iteration: 19, test_1's auc: 0.9568234041558146\n",
      "Iteration: 20, test_1's binary_logloss: 0.3029291175761306, \n",
      "Iteration: 20, test_1's auc: 0.9594243780683539\n",
      "Iteration: 21, test_1's binary_logloss: 0.3002883625121669, \n",
      "Iteration: 21, test_1's auc: 0.9591592247727175\n",
      "Iteration: 22, test_1's binary_logloss: 0.2914445126270894, \n",
      "Iteration: 22, test_1's auc: 0.9608561231105576\n",
      "Iteration: 23, test_1's binary_logloss: 0.28929533894909, \n",
      "Iteration: 23, test_1's auc: 0.9606943670108669\n",
      "Iteration: 24, test_1's binary_logloss: 0.27967139667843993, \n",
      "Iteration: 24, test_1's auc: 0.9628071372955267\n",
      "Iteration: 25, test_1's binary_logloss: 0.27817530461021556, \n",
      "Iteration: 25, test_1's auc: 0.9627647996177174\n",
      "Iteration: 26, test_1's binary_logloss: 0.2765453155286286, \n",
      "Iteration: 26, test_1's auc: 0.9626683190834068\n",
      "Iteration: 27, test_1's binary_logloss: 0.2681437967648568, \n",
      "Iteration: 27, test_1's auc: 0.9645486590622104\n",
      "Iteration: 28, test_1's binary_logloss: 0.26052618685166556, \n",
      "Iteration: 28, test_1's auc: 0.9661641743264276\n",
      "Iteration: 29, test_1's binary_logloss: 0.25370369181862223, \n",
      "Iteration: 29, test_1's auc: 0.9675409481153503\n",
      "Iteration: 30, test_1's binary_logloss: 0.2526398086525505, \n",
      "Iteration: 30, test_1's auc: 0.9674908648570724\n",
      "Iteration: 31, test_1's binary_logloss: 0.25173628465526154, \n",
      "Iteration: 31, test_1's auc: 0.9675168912506166\n",
      "Iteration: 32, test_1's binary_logloss: 0.25102456971692977, \n",
      "Iteration: 32, test_1's auc: 0.9676003570011382\n",
      "Iteration: 33, test_1's binary_logloss: 0.24752863836628342, \n",
      "Iteration: 33, test_1's auc: 0.9681155606214058\n",
      "Iteration: 34, test_1's binary_logloss: 0.2426744067458302, \n",
      "Iteration: 34, test_1's auc: 0.96902074325736\n",
      "Iteration: 35, test_1's binary_logloss: 0.23699152734724876, \n",
      "Iteration: 35, test_1's auc: 0.9702438892677607\n",
      "Iteration: 36, test_1's binary_logloss: 0.2321734610557362, \n",
      "Iteration: 36, test_1's auc: 0.9711892713500518\n",
      "Iteration: 37, test_1's binary_logloss: 0.22737286631882497, \n",
      "Iteration: 37, test_1's auc: 0.972207327461802\n",
      "Iteration: 38, test_1's binary_logloss: 0.22337673543776174, \n",
      "Iteration: 38, test_1's auc: 0.9729632510082667\n",
      "Iteration: 39, test_1's binary_logloss: 0.22046570154576237, \n",
      "Iteration: 39, test_1's auc: 0.9734387134885603\n",
      "Iteration: 40, test_1's binary_logloss: 0.2169701962688437, \n",
      "Iteration: 40, test_1's auc: 0.9741178846872308\n",
      "Iteration: 41, test_1's binary_logloss: 0.2152674434825817, \n",
      "Iteration: 41, test_1's auc: 0.9743428389215657\n",
      "Iteration: 42, test_1's binary_logloss: 0.21275710120066868, \n",
      "Iteration: 42, test_1's auc: 0.9747559526194077\n",
      "Iteration: 43, test_1's binary_logloss: 0.2124847857257893, \n",
      "Iteration: 43, test_1's auc: 0.9747710995773217\n",
      "Iteration: 44, test_1's binary_logloss: 0.21186635553189107, \n",
      "Iteration: 44, test_1's auc: 0.9748801317154836\n",
      "Iteration: 45, test_1's binary_logloss: 0.20888730058286914, \n",
      "Iteration: 45, test_1's auc: 0.9754475527949139\n",
      "Iteration: 46, test_1's binary_logloss: 0.20831528933688226, \n",
      "Iteration: 46, test_1's auc: 0.9755532879618417\n",
      "Iteration: 47, test_1's binary_logloss: 0.20781548386329307, \n",
      "Iteration: 47, test_1's auc: 0.9756591542763248\n",
      "Iteration: 48, test_1's binary_logloss: 0.20529013914226468, \n",
      "Iteration: 48, test_1's auc: 0.9762133432639372\n",
      "Iteration: 49, test_1's binary_logloss: 0.20513425810963667, \n",
      "Iteration: 49, test_1's auc: 0.9762259181142998\n",
      "Iteration: 50, test_1's binary_logloss: 0.20473455981111421, \n",
      "Iteration: 50, test_1's auc: 0.9762951105871164\n",
      "Iteration: 51, test_1's binary_logloss: 0.2029192194472116, \n",
      "Iteration: 51, test_1's auc: 0.9766011740497803\n",
      "Iteration: 52, test_1's binary_logloss: 0.20228680454784448, \n",
      "Iteration: 52, test_1's auc: 0.9767507149641922\n",
      "Iteration: 53, test_1's binary_logloss: 0.20150398587222032, \n",
      "Iteration: 53, test_1's auc: 0.9768893783298436\n",
      "Iteration: 54, test_1's binary_logloss: 0.20020973159642744, \n",
      "Iteration: 54, test_1's auc: 0.9771764337251417\n",
      "Iteration: 55, test_1's binary_logloss: 0.19842601762143472, \n",
      "Iteration: 55, test_1's auc: 0.9776135574506757\n",
      "Iteration: 56, test_1's binary_logloss: 0.19797397717631948, \n",
      "Iteration: 56, test_1's auc: 0.9777034823519766\n",
      "Iteration: 57, test_1's binary_logloss: 0.19693181582942765, \n",
      "Iteration: 57, test_1's auc: 0.9778435245951772\n",
      "Iteration: 58, test_1's binary_logloss: 0.19543979472865794, \n",
      "Iteration: 58, test_1's auc: 0.9781268754747593\n",
      "Iteration: 59, test_1's binary_logloss: 0.19536419472260239, \n",
      "Iteration: 59, test_1's auc: 0.9781361694105972\n",
      "Iteration: 60, test_1's binary_logloss: 0.1930116390544589, \n",
      "Iteration: 60, test_1's auc: 0.9785644236845166\n",
      "Iteration: 61, test_1's binary_logloss: 0.1909767305310334, \n",
      "Iteration: 61, test_1's auc: 0.9789524426896354\n",
      "Iteration: 62, test_1's binary_logloss: 0.19057154347743294, \n",
      "Iteration: 62, test_1's auc: 0.9790462234850787\n",
      "Iteration: 63, test_1's binary_logloss: 0.18917038347344015, \n",
      "Iteration: 63, test_1's auc: 0.9793619185220469\n",
      "Iteration: 64, test_1's binary_logloss: 0.18889038393338903, \n",
      "Iteration: 64, test_1's auc: 0.9794179896615509\n",
      "Iteration: 65, test_1's binary_logloss: 0.1874044350233123, \n",
      "Iteration: 65, test_1's auc: 0.9796639228471418\n",
      "Iteration: 66, test_1's binary_logloss: 0.18714487110934788, \n",
      "Iteration: 66, test_1's auc: 0.9796913734997962\n",
      "Iteration: 67, test_1's binary_logloss: 0.18512031324032527, \n",
      "Iteration: 67, test_1's auc: 0.9800798007528387\n",
      "Iteration: 68, test_1's binary_logloss: 0.18507568783027592, \n",
      "Iteration: 68, test_1's auc: 0.9800849102041317\n",
      "Iteration: 69, test_1's binary_logloss: 0.18466846059848083, \n",
      "Iteration: 69, test_1's auc: 0.9801765788385155\n",
      "Iteration: 70, test_1's binary_logloss: 0.18353627690013555, \n",
      "Iteration: 70, test_1's auc: 0.9803852916477035\n",
      "Iteration: 71, test_1's binary_logloss: 0.18247875871322863, \n",
      "Iteration: 71, test_1's auc: 0.9805682199844111\n",
      "Iteration: 72, test_1's binary_logloss: 0.18204870835778633, \n",
      "Iteration: 72, test_1's auc: 0.9806631604405404\n",
      "Iteration: 73, test_1's binary_logloss: 0.18060045974401168, \n",
      "Iteration: 73, test_1's auc: 0.9809456343575168\n",
      "Iteration: 74, test_1's binary_logloss: 0.18040311580334265, \n",
      "Iteration: 74, test_1's auc: 0.9809867005399531\n",
      "Iteration: 75, test_1's binary_logloss: 0.17892208843689747, \n",
      "Iteration: 75, test_1's auc: 0.9812784130789023\n",
      "Iteration: 76, test_1's binary_logloss: 0.17861653688045737, \n",
      "Iteration: 76, test_1's auc: 0.9813468319674342\n",
      "Iteration: 77, test_1's binary_logloss: 0.17692462847967666, \n",
      "Iteration: 77, test_1's auc: 0.9816405355611533\n",
      "Iteration: 78, test_1's binary_logloss: 0.17656905163734007, \n",
      "Iteration: 78, test_1's auc: 0.9817201339517316\n",
      "Iteration: 79, test_1's binary_logloss: 0.17631150578527455, \n",
      "Iteration: 79, test_1's auc: 0.9817757569286472\n",
      "Iteration: 80, test_1's binary_logloss: 0.17607075556778343, \n",
      "Iteration: 80, test_1's auc: 0.9818361752814526\n",
      "Iteration: 81, test_1's binary_logloss: 0.17495568834064812, \n",
      "Iteration: 81, test_1's auc: 0.9820359952925765\n",
      "Iteration: 82, test_1's binary_logloss: 0.17457717513310356, \n",
      "Iteration: 82, test_1's auc: 0.9820802541979448\n",
      "Iteration: 83, test_1's binary_logloss: 0.1745856125380391, \n",
      "Iteration: 83, test_1's auc: 0.9820792084585891\n",
      "Iteration: 84, test_1's binary_logloss: 0.1742425902148538, \n",
      "Iteration: 84, test_1's auc: 0.9821276695912077\n",
      "Iteration: 85, test_1's binary_logloss: 0.17397334197760542, \n",
      "Iteration: 85, test_1's auc: 0.9821623166261257\n",
      "Iteration: 86, test_1's binary_logloss: 0.17380783002908065, \n",
      "Iteration: 86, test_1's auc: 0.9821808805356828\n",
      "Iteration: 87, test_1's binary_logloss: 0.17238870848776686, \n",
      "Iteration: 87, test_1's auc: 0.9824288992198376\n",
      "Iteration: 88, test_1's binary_logloss: 0.1722239208135097, \n",
      "Iteration: 88, test_1's auc: 0.9824472752313842\n",
      "Iteration: 89, test_1's binary_logloss: 0.17206650823442618, \n",
      "Iteration: 89, test_1's auc: 0.9824718730557992\n",
      "Iteration: 90, test_1's binary_logloss: 0.17060888225252402, \n",
      "Iteration: 90, test_1's auc: 0.9827213534999213\n",
      "Iteration: 91, test_1's binary_logloss: 0.17041104646504557, \n",
      "Iteration: 91, test_1's auc: 0.9827642320371931\n",
      "Iteration: 92, test_1's binary_logloss: 0.1694040303640054, \n",
      "Iteration: 92, test_1's auc: 0.9829451098122668\n",
      "Iteration: 93, test_1's binary_logloss: 0.16876163588863496, \n",
      "Iteration: 93, test_1's auc: 0.9830540411293731\n",
      "Iteration: 94, test_1's binary_logloss: 0.1682826401312085, \n",
      "Iteration: 94, test_1's auc: 0.9831321691366286\n",
      "Iteration: 95, test_1's binary_logloss: 0.16817850057949732, \n",
      "Iteration: 95, test_1's auc: 0.9831472528050622\n",
      "Iteration: 96, test_1's binary_logloss: 0.1672909192196007, \n",
      "Iteration: 96, test_1's auc: 0.9833099277820759\n",
      "Iteration: 97, test_1's binary_logloss: 0.166647371506289, \n",
      "Iteration: 97, test_1's auc: 0.9834069749596124\n",
      "Iteration: 98, test_1's binary_logloss: 0.16581801363496684, \n",
      "Iteration: 98, test_1's auc: 0.9835520762509427\n",
      "Iteration: 99, test_1's binary_logloss: 0.16565156959760824, \n",
      "Iteration: 99, test_1's auc: 0.9835872418277295\n",
      "Iteration: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:55:15 Saving model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100, test_1's binary_logloss: 0.16512776229492418, \n",
      "Iteration: 100, test_1's auc: 0.9836975626205693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:55:17 not logging split losses\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:55:17 predicting training explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:55:30 predicting training implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:55:36 predicting training negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:55:38 predicting training ptw\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:55:45 predicting validation explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:55:46 saving chunk 1 out of 1 (15118941 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:14\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:56:16 predicting validation implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:56:16 saving chunk 1 out of 1 (9581039 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:10\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:56:40 predicting validation negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:56:42 saving chunk 1 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:38\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 10:58:46 saving chunk 2 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:38\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:00:51 saving chunk 3 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:38\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:02:55 saving chunk 4 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:38\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:04:58 saving chunk 5 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:20\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:05:42 predicting validation ptw\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:05:43 saving chunk 1 out of 1 (8205676 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:06:04 predicting test explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:06:05 saving chunk 1 out of 1 (15121362 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:14\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:06:35 predicting test implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:06:36 saving chunk 1 out of 1 (9573324 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:10\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:06:59 predicting test negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:07:01 saving chunk 1 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:38\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:09:06 saving chunk 2 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:38\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:11:10 saving chunk 3 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:38\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:13:14 saving chunk 4 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:38\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:15:18 saving chunk 5 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:20\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:16:02 predicting test ptw\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:16:03 saving chunk 1 out of 1 (8208159 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    [\n",
    "        explicit_raw_alphas\n",
    "        implicit_raw_alphas\n",
    "        nondirectional_raw_alphas\n",
    "        [\"LinearExplicit\", \"LinearImplicit\"]\n",
    "    ],\n",
    "    String[],\n",
    "    [\"implicit\", \"negative\"],\n",
    "    true,\n",
    "    \"validation\",\n",
    "    false,\n",
    "    \"NonlinearImplicit\";\n",
    "    λ = nothing,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e4e38f-c51c-49b1-9c18-029b74d2752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:16:31 getting features for ptw, validation\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:16:57 getting features for negative, validation\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 8.62 ns/it)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7384619, number of negative: 379124361\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.385214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3317\n",
      "[LightGBM] [Info] Number of data points in the train set: 386508980, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.287811 -> initscore=-0.906040\n",
      "[LightGBM] [Info] Start training from score -0.906040\n",
      "Iteration: 1, test_1's binary_logloss: 0.5612709307135096, \n",
      "Iteration: 1, test_1's auc: 0.8827183290457644\n",
      "Iteration: 2, test_1's binary_logloss: 0.5301659557040053, \n",
      "Iteration: 2, test_1's auc: 0.8991946055213895\n",
      "Iteration: 3, test_1's binary_logloss: 0.5008654276709125, \n",
      "Iteration: 3, test_1's auc: 0.9081753800342739\n",
      "Iteration: 4, test_1's binary_logloss: 0.4755941618929039, \n",
      "Iteration: 4, test_1's auc: 0.9134063837472771\n",
      "Iteration: 5, test_1's binary_logloss: 0.4565010919373126, \n",
      "Iteration: 5, test_1's auc: 0.9153313194580319\n",
      "Iteration: 6, test_1's binary_logloss: 0.439120611142607, \n",
      "Iteration: 6, test_1's auc: 0.9160543002322918\n",
      "Iteration: 7, test_1's binary_logloss: 0.4227568460076424, \n",
      "Iteration: 7, test_1's auc: 0.9182799966198005\n",
      "Iteration: 8, test_1's binary_logloss: 0.4112750533464036, \n",
      "Iteration: 8, test_1's auc: 0.9186158765995109\n",
      "Iteration: 9, test_1's binary_logloss: 0.3978670373573614, \n",
      "Iteration: 9, test_1's auc: 0.9221050445214396\n",
      "Iteration: 10, test_1's binary_logloss: 0.38918268139819645, \n",
      "Iteration: 10, test_1's auc: 0.9220184839404408\n",
      "Iteration: 11, test_1's binary_logloss: 0.3803225747330118, \n",
      "Iteration: 11, test_1's auc: 0.9226147961215855\n",
      "Iteration: 12, test_1's binary_logloss: 0.3722687853669868, \n",
      "Iteration: 12, test_1's auc: 0.9229164690340134\n",
      "Iteration: 13, test_1's binary_logloss: 0.36527492100175957, \n",
      "Iteration: 13, test_1's auc: 0.9238217908918838\n",
      "Iteration: 14, test_1's binary_logloss: 0.35879266092357914, \n",
      "Iteration: 14, test_1's auc: 0.9241878556262713\n",
      "Iteration: 15, test_1's binary_logloss: 0.3494752395328623, \n",
      "Iteration: 15, test_1's auc: 0.9277988995477556\n",
      "Iteration: 16, test_1's binary_logloss: 0.3438723144405249, \n",
      "Iteration: 16, test_1's auc: 0.9283479156778297\n",
      "Iteration: 17, test_1's binary_logloss: 0.33903422557723756, \n",
      "Iteration: 17, test_1's auc: 0.9287570885395676\n",
      "Iteration: 18, test_1's binary_logloss: 0.3346556188306117, \n",
      "Iteration: 18, test_1's auc: 0.9295405056484289\n",
      "Iteration: 19, test_1's binary_logloss: 0.33049076719598436, \n",
      "Iteration: 19, test_1's auc: 0.9300009534328112\n",
      "Iteration: 20, test_1's binary_logloss: 0.3276920988025903, \n",
      "Iteration: 20, test_1's auc: 0.9300967490931471\n",
      "Iteration: 21, test_1's binary_logloss: 0.32443958136875617, \n",
      "Iteration: 21, test_1's auc: 0.9303581662725396\n",
      "Iteration: 22, test_1's binary_logloss: 0.32112144786456714, \n",
      "Iteration: 22, test_1's auc: 0.931248969839046\n",
      "Iteration: 23, test_1's binary_logloss: 0.3183935940212312, \n",
      "Iteration: 23, test_1's auc: 0.9317346759544479\n",
      "Iteration: 24, test_1's binary_logloss: 0.3159820698158439, \n",
      "Iteration: 24, test_1's auc: 0.9321284682444116\n",
      "Iteration: 25, test_1's binary_logloss: 0.3090452233411841, \n",
      "Iteration: 25, test_1's auc: 0.9352464475813508\n",
      "Iteration: 26, test_1's binary_logloss: 0.3008514711866578, \n",
      "Iteration: 26, test_1's auc: 0.939292167972203\n",
      "Iteration: 27, test_1's binary_logloss: 0.297364194835565, \n",
      "Iteration: 27, test_1's auc: 0.9403535666507331\n",
      "Iteration: 28, test_1's binary_logloss: 0.29603773590829746, \n",
      "Iteration: 28, test_1's auc: 0.9404263583953032\n",
      "Iteration: 29, test_1's binary_logloss: 0.2946391984333974, \n",
      "Iteration: 29, test_1's auc: 0.9405748019477312\n",
      "Iteration: 30, test_1's binary_logloss: 0.29105732355721264, \n",
      "Iteration: 30, test_1's auc: 0.9419264003472428\n",
      "Iteration: 31, test_1's binary_logloss: 0.28694399836508405, \n",
      "Iteration: 31, test_1's auc: 0.9434469955143319\n",
      "Iteration: 32, test_1's binary_logloss: 0.28264384231311207, \n",
      "Iteration: 32, test_1's auc: 0.9450870075961294\n",
      "Iteration: 33, test_1's binary_logloss: 0.27891757176806037, \n",
      "Iteration: 33, test_1's auc: 0.9464828669283047\n",
      "Iteration: 34, test_1's binary_logloss: 0.2714570472397023, \n",
      "Iteration: 34, test_1's auc: 0.9498243864893714\n",
      "Iteration: 35, test_1's binary_logloss: 0.26783086413644563, \n",
      "Iteration: 35, test_1's auc: 0.9511060169108142\n",
      "Iteration: 36, test_1's binary_logloss: 0.2648118274063421, \n",
      "Iteration: 36, test_1's auc: 0.952103826850124\n",
      "Iteration: 37, test_1's binary_logloss: 0.26386003888510823, \n",
      "Iteration: 37, test_1's auc: 0.9522416991789172\n",
      "Iteration: 38, test_1's binary_logloss: 0.2628540149286817, \n",
      "Iteration: 38, test_1's auc: 0.952491374654323\n",
      "Iteration: 39, test_1's binary_logloss: 0.26025636542303765, \n",
      "Iteration: 39, test_1's auc: 0.9533441206311017\n",
      "Iteration: 40, test_1's binary_logloss: 0.25805040394357787, \n",
      "Iteration: 40, test_1's auc: 0.9541816815084836\n",
      "Iteration: 41, test_1's binary_logloss: 0.25568400969886956, \n",
      "Iteration: 41, test_1's auc: 0.955054689935536\n",
      "Iteration: 42, test_1's binary_logloss: 0.25196426301742253, \n",
      "Iteration: 42, test_1's auc: 0.9564285085183636\n",
      "Iteration: 43, test_1's binary_logloss: 0.24932199380908843, \n",
      "Iteration: 43, test_1's auc: 0.9573662394656168\n",
      "Iteration: 44, test_1's binary_logloss: 0.2469775726458525, \n",
      "Iteration: 44, test_1's auc: 0.9581017872197084\n",
      "Iteration: 45, test_1's binary_logloss: 0.24493444656980615, \n",
      "Iteration: 45, test_1's auc: 0.9588290522116802\n",
      "Iteration: 46, test_1's binary_logloss: 0.2404045106698309, \n",
      "Iteration: 46, test_1's auc: 0.9604405079298166\n",
      "Iteration: 47, test_1's binary_logloss: 0.2388315616446059, \n",
      "Iteration: 47, test_1's auc: 0.960966819664126\n",
      "Iteration: 48, test_1's binary_logloss: 0.23639780386528153, \n",
      "Iteration: 48, test_1's auc: 0.9619021776046434\n",
      "Iteration: 49, test_1's binary_logloss: 0.23337692552124048, \n",
      "Iteration: 49, test_1's auc: 0.9629561353325159\n",
      "Iteration: 50, test_1's binary_logloss: 0.23170062377404824, \n",
      "Iteration: 50, test_1's auc: 0.9635717225670715\n",
      "Iteration: 51, test_1's binary_logloss: 0.22787054655988911, \n",
      "Iteration: 51, test_1's auc: 0.9648206489634266\n",
      "Iteration: 52, test_1's binary_logloss: 0.2260383213485165, \n",
      "Iteration: 52, test_1's auc: 0.965370287786332\n",
      "Iteration: 53, test_1's binary_logloss: 0.22397756298158092, \n",
      "Iteration: 53, test_1's auc: 0.9660810605443764\n",
      "Iteration: 54, test_1's binary_logloss: 0.2220878149945186, \n",
      "Iteration: 54, test_1's auc: 0.9667399541678344\n",
      "Iteration: 55, test_1's binary_logloss: 0.22048926206450722, \n",
      "Iteration: 55, test_1's auc: 0.9672872930377093\n",
      "Iteration: 56, test_1's binary_logloss: 0.21850262446490384, \n",
      "Iteration: 56, test_1's auc: 0.9679308745692853\n",
      "Iteration: 57, test_1's binary_logloss: 0.21678584806555548, \n",
      "Iteration: 57, test_1's auc: 0.9685212797304602\n",
      "Iteration: 58, test_1's binary_logloss: 0.2151944044336708, \n",
      "Iteration: 58, test_1's auc: 0.9689488937100664\n",
      "Iteration: 59, test_1's binary_logloss: 0.21335740439721643, \n",
      "Iteration: 59, test_1's auc: 0.9694422498206936\n",
      "Iteration: 60, test_1's binary_logloss: 0.21305183933411137, \n",
      "Iteration: 60, test_1's auc: 0.9695186605539527\n",
      "Iteration: 61, test_1's binary_logloss: 0.21182265120019703, \n",
      "Iteration: 61, test_1's auc: 0.9699400248186444\n",
      "Iteration: 62, test_1's binary_logloss: 0.21035287647476206, \n",
      "Iteration: 62, test_1's auc: 0.9703290706170872\n",
      "Iteration: 63, test_1's binary_logloss: 0.20863904644307113, \n",
      "Iteration: 63, test_1's auc: 0.9707782433128717\n",
      "Iteration: 64, test_1's binary_logloss: 0.2082150034331638, \n",
      "Iteration: 64, test_1's auc: 0.9709288253875349\n",
      "Iteration: 65, test_1's binary_logloss: 0.20762105051732868, \n",
      "Iteration: 65, test_1's auc: 0.9711129662069697\n",
      "Iteration: 66, test_1's binary_logloss: 0.20571485754138505, \n",
      "Iteration: 66, test_1's auc: 0.9717468960348821\n",
      "Iteration: 67, test_1's binary_logloss: 0.20407459987737656, \n",
      "Iteration: 67, test_1's auc: 0.9722711828822571\n",
      "Iteration: 68, test_1's binary_logloss: 0.20312184426677343, \n",
      "Iteration: 68, test_1's auc: 0.9725918425811584\n",
      "Iteration: 69, test_1's binary_logloss: 0.20245955231859883, \n",
      "Iteration: 69, test_1's auc: 0.9727809703403778\n",
      "Iteration: 70, test_1's binary_logloss: 0.2012734002210012, \n",
      "Iteration: 70, test_1's auc: 0.973145195019442\n",
      "Iteration: 71, test_1's binary_logloss: 0.1998839551023514, \n",
      "Iteration: 71, test_1's auc: 0.9735263392492247\n",
      "Iteration: 72, test_1's binary_logloss: 0.19926509288522973, \n",
      "Iteration: 72, test_1's auc: 0.973685527066431\n",
      "Iteration: 73, test_1's binary_logloss: 0.19722261358133725, \n",
      "Iteration: 73, test_1's auc: 0.9743120023122726\n",
      "Iteration: 74, test_1's binary_logloss: 0.1960400895267346, \n",
      "Iteration: 74, test_1's auc: 0.9746894225740993\n",
      "Iteration: 75, test_1's binary_logloss: 0.19481591784303556, \n",
      "Iteration: 75, test_1's auc: 0.9750565665254215\n",
      "Iteration: 76, test_1's binary_logloss: 0.1940830527393114, \n",
      "Iteration: 76, test_1's auc: 0.9752719483606103\n",
      "Iteration: 77, test_1's binary_logloss: 0.19335223099983292, \n",
      "Iteration: 77, test_1's auc: 0.9755051981557917\n",
      "Iteration: 78, test_1's binary_logloss: 0.19180028335281585, \n",
      "Iteration: 78, test_1's auc: 0.9759799444033285\n",
      "Iteration: 79, test_1's binary_logloss: 0.1902767732459367, \n",
      "Iteration: 79, test_1's auc: 0.9763736464162307\n",
      "Iteration: 80, test_1's binary_logloss: 0.18975788576409916, \n",
      "Iteration: 80, test_1's auc: 0.9765339829146168\n",
      "Iteration: 81, test_1's binary_logloss: 0.18819699894242903, \n",
      "Iteration: 81, test_1's auc: 0.9769734111072544\n",
      "Iteration: 82, test_1's binary_logloss: 0.1876097006043123, \n",
      "Iteration: 82, test_1's auc: 0.9771243857792918\n",
      "Iteration: 83, test_1's binary_logloss: 0.18686356252133896, \n",
      "Iteration: 83, test_1's auc: 0.9773532004514468\n",
      "Iteration: 84, test_1's binary_logloss: 0.18657379972921495, \n",
      "Iteration: 84, test_1's auc: 0.977439187773313\n",
      "Iteration: 85, test_1's binary_logloss: 0.18582667667681885, \n",
      "Iteration: 85, test_1's auc: 0.9776754328751441\n",
      "Iteration: 86, test_1's binary_logloss: 0.18521105515498268, \n",
      "Iteration: 86, test_1's auc: 0.977818294881087\n",
      "Iteration: 87, test_1's binary_logloss: 0.18402073349370923, \n",
      "Iteration: 87, test_1's auc: 0.9780576519807977\n",
      "Iteration: 88, test_1's binary_logloss: 0.18377386626165731, \n",
      "Iteration: 88, test_1's auc: 0.9781376965571829\n",
      "Iteration: 89, test_1's binary_logloss: 0.18251104847752647, \n",
      "Iteration: 89, test_1's auc: 0.9784544691108248\n",
      "Iteration: 90, test_1's binary_logloss: 0.18148076573482042, \n",
      "Iteration: 90, test_1's auc: 0.9786948161087692\n",
      "Iteration: 91, test_1's binary_logloss: 0.1807931571012399, \n",
      "Iteration: 91, test_1's auc: 0.9788289798601973\n",
      "Iteration: 92, test_1's binary_logloss: 0.17913998888964963, \n",
      "Iteration: 92, test_1's auc: 0.9791837730928454\n",
      "Iteration: 93, test_1's binary_logloss: 0.17836525732199046, \n",
      "Iteration: 93, test_1's auc: 0.9793767969277867\n",
      "Iteration: 94, test_1's binary_logloss: 0.17819895493535245, \n",
      "Iteration: 94, test_1's auc: 0.9794285248982908\n",
      "Iteration: 95, test_1's binary_logloss: 0.17793291192618163, \n",
      "Iteration: 95, test_1's auc: 0.9795022414967647\n",
      "Iteration: 96, test_1's binary_logloss: 0.17693718528129967, \n",
      "Iteration: 96, test_1's auc: 0.9797353256597234\n",
      "Iteration: 97, test_1's binary_logloss: 0.17589710813987824, \n",
      "Iteration: 97, test_1's auc: 0.9799667120910277\n",
      "Iteration: 98, test_1's binary_logloss: 0.17479089000078837, \n",
      "Iteration: 98, test_1's auc: 0.9802037142056419\n",
      "Iteration: 99, test_1's binary_logloss: 0.1740719416431368, \n",
      "Iteration: 99, test_1's auc: 0.9803478500955366\n",
      "Iteration: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:49:52 Saving model...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:49:52 not logging split losses\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:49:52 predicting training explicit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100, test_1's binary_logloss: 0.17346397704779234, \n",
      "Iteration: 100, test_1's auc: 0.9805045716587715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:50:05 predicting training implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:50:11 predicting training negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:50:14 predicting training ptw\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:50:20 predicting validation explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:50:21 saving chunk 1 out of 1 (15118941 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:15\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:50:53 predicting validation implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:50:53 saving chunk 1 out of 1 (9581039 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:10\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:51:17 predicting validation negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:51:18 saving chunk 1 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:40\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:53:28 saving chunk 2 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:40\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:55:38 saving chunk 3 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:40\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:57:47 saving chunk 4 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:40\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 11:59:56 saving chunk 5 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:21\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:00:42 predicting validation ptw\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:00:42 saving chunk 1 out of 1 (8205676 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:01:04 predicting test explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:01:05 saving chunk 1 out of 1 (15121362 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:15\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:01:38 predicting test implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:01:38 saving chunk 1 out of 1 (9573324 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:10\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:02:02 predicting test negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:02:04 saving chunk 1 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:40\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:04:14 saving chunk 2 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:40\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:06:27 saving chunk 3 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:40\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:08:37 saving chunk 4 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:40\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:10:46 saving chunk 5 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:21\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:11:34 predicting test ptw\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 12:11:34 saving chunk 1 out of 1 (8208159 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    [\n",
    "        explicit_raw_alphas\n",
    "        implicit_raw_alphas\n",
    "        ptw_raw_alphas\n",
    "        nondirectional_raw_alphas\n",
    "        [\"LinearExplicit\", \"LinearImplicit\", \"LinearPtw\"]\n",
    "    ],\n",
    "    String[],\n",
    "    [\"ptw\", \"negative\"],\n",
    "    true,\n",
    "    \"validation\",\n",
    "    false,\n",
    "    \"NonlinearPtw\";\n",
    "    λ = nothing,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517ce91a-bf09-446a-8be9-a7c5bbe0c0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:34:46 getting features for explicit, validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3317\n",
      "[LightGBM] [Info] Number of data points in the train set: 13606482, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 0.003684\n",
      "Iteration: 1, test_1's l2: 1.7329531295941416\n",
      "Iteration: 2, test_1's l2: 1.72839896251443\n",
      "Iteration: 3, test_1's l2: 1.7245785304367558\n",
      "Iteration: 4, test_1's l2: 1.7211510440139735\n",
      "Iteration: 5, test_1's l2: 1.7183366429457956\n",
      "Iteration: 6, test_1's l2: 1.7160076358285823\n",
      "Iteration: 7, test_1's l2: 1.7139523694164618\n",
      "Iteration: 8, test_1's l2: 1.7121758867455315\n",
      "Iteration: 9, test_1's l2: 1.7103177418511866\n",
      "Iteration: 10, test_1's l2: 1.7091809339674573\n",
      "Iteration: 11, test_1's l2: 1.708164585742004\n",
      "Iteration: 12, test_1's l2: 1.7070999306429062\n",
      "Iteration: 13, test_1's l2: 1.70613353024792\n",
      "Iteration: 14, test_1's l2: 1.7055812562348662\n",
      "Iteration: 15, test_1's l2: 1.7045914617827056\n",
      "Iteration: 16, test_1's l2: 1.704152043721966\n",
      "Iteration: 17, test_1's l2: 1.7036593384629688\n",
      "Iteration: 18, test_1's l2: 1.7031329565966407\n",
      "Iteration: 19, test_1's l2: 1.7027047684773133\n",
      "Iteration: 20, test_1's l2: 1.7022640989635107\n",
      "Iteration: 21, test_1's l2: 1.7018541940041771\n",
      "Iteration: 22, test_1's l2: 1.7013608573392767\n",
      "Iteration: 23, test_1's l2: 1.701054988293679\n",
      "Iteration: 24, test_1's l2: 1.7008400620395845\n",
      "Iteration: 25, test_1's l2: 1.7004966315785408\n",
      "Iteration: 26, test_1's l2: 1.700064820627124\n",
      "Iteration: 27, test_1's l2: 1.699791786966818\n",
      "Iteration: 28, test_1's l2: 1.6996257818919962\n",
      "Iteration: 29, test_1's l2: 1.699517367395502\n",
      "Iteration: 30, test_1's l2: 1.6993441868649692\n",
      "Iteration: 31, test_1's l2: 1.6991806917739476\n",
      "Iteration: 32, test_1's l2: 1.6990870845894102\n",
      "Iteration: 33, test_1's l2: 1.69901555643693\n",
      "Iteration: 34, test_1's l2: 1.6989224560082756\n",
      "Iteration: 35, test_1's l2: 1.6987408415539764\n",
      "Iteration: 36, test_1's l2: 1.6986114333838127\n",
      "Iteration: 37, test_1's l2: 1.6986564630791139\n",
      "Iteration: 38, test_1's l2: 1.6986710342532694\n",
      "Iteration: 39, test_1's l2: 1.6986970531816181\n",
      "Iteration: 40, test_1's l2: 1.6986445618437815\n",
      "Iteration: 41, test_1's l2: 1.6987143117646457\n",
      "Iteration: 42, test_1's l2: 1.6986678478110724\n",
      "Iteration: 43, test_1's l2: 1.6985708619215139\n",
      "Iteration: 44, test_1's l2: 1.6986828474559015\n",
      "Iteration: 45, test_1's l2: 1.6986618168718026\n",
      "Iteration: 46, test_1's l2: 1.6986472353986364\n",
      "Iteration: 47, test_1's l2: 1.6986896808471503\n",
      "Iteration: 48, test_1's l2: 1.6985359618067082\n",
      "Iteration: 49, test_1's l2: 1.6985351125640817\n",
      "Iteration: 50, test_1's l2: 1.6985472666815862\n",
      "Iteration: 51, test_1's l2: 1.698548590041371\n",
      "Iteration: 52, test_1's l2: 1.6985447552472441\n",
      "Iteration: 53, test_1's l2: 1.6985636372208013\n",
      "Iteration: 54, test_1's l2: 1.698560339256287\n",
      "Iteration: 55, test_1's l2: 1.698620749433691\n",
      "Iteration: 56, test_1's l2: 1.6985999833032326\n",
      "Iteration: 57, test_1's l2: 1.698513896888246\n",
      "Iteration: 58, test_1's l2: 1.6985538708490295\n",
      "Iteration: 59, test_1's l2: 1.698604020973446\n",
      "Iteration: 60, test_1's l2: 1.698700616283314\n",
      "Iteration: 61, test_1's l2: 1.6987225128028258\n",
      "Iteration: 62, test_1's l2: 1.6988129304958548\n",
      "Iteration: 63, test_1's l2: 1.6988188715294399\n",
      "Iteration: 64, test_1's l2: 1.6988256733731295\n",
      "Iteration: 65, test_1's l2: 1.6988489142387366\n",
      "Iteration: 66, test_1's l2: 1.6988818286882421\n",
      "Early stopping at iteration 67, the best iteration round is 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:35:53 Saving model...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:35:55 not logging split losses\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:35:55 predicting training explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:36:04 predicting training implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:36:08 predicting training negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:36:11 predicting training ptw\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:36:16 predicting validation explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:36:17 saving chunk 1 out of 1 (15118941 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:36:37 predicting validation implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:36:37 saving chunk 1 out of 1 (9581039 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:05\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:36:55 predicting validation negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:36:57 saving chunk 1 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:49\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:38:16 saving chunk 2 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:56\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:39:40 saving chunk 3 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:49\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:41:01 saving chunk 4 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:49\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:42:16 saving chunk 5 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:10\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:42:51 predicting validation ptw\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:42:51 saving chunk 1 out of 1 (8205676 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:04\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:43:09 predicting test explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:43:10 saving chunk 1 out of 1 (15121362 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:06\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:43:33 predicting test implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:43:33 saving chunk 1 out of 1 (9573324 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:05\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:43:52 predicting test negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:43:54 saving chunk 1 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:49\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:45:12 saving chunk 2 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:49\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:46:33 saving chunk 3 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:49\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:47:52 saving chunk 4 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:49\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:49:08 saving chunk 5 out of 5 (420927700 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:10\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:49:43 predicting test ptw\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:49:43 saving chunk 1 out of 1 (8208159 / 100000000)\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:04\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    [\n",
    "        explicit_raw_alphas\n",
    "        implicit_raw_alphas\n",
    "        ptw_raw_alphas\n",
    "        nondirectional_raw_alphas\n",
    "        [\"LinearExplicit\", \"LinearImplicit\", \"LinearPtw\"]\n",
    "    ],\n",
    "    [\"LinearExplicit\"],\n",
    "    [\"explicit\"],\n",
    "    false,\n",
    "    \"validation\",\n",
    "    false,\n",
    "    \"NonlinearExplicit\";\n",
    "    λ = nothing,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4928ac14-f5c8-4119-9215-7e9f61e97cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_final_explicit_model()\n",
    "    function model(split::String, content::String; raw_splits = true)\n",
    "        @info \"saving $split $content\"\n",
    "        split_fn = raw_splits ? get_raw_split : get_split\n",
    "        alpha_fn = raw_splits ? read_raw_alpha : read_alpha\n",
    "        if split == \"training\"\n",
    "            return zeros(Float32, length(split_fn(split, content).user))\n",
    "        end\n",
    "        linear_split = alpha_fn(\"LinearExplicit\", split, content).rating\n",
    "        nonlinear_split = alpha_fn(\"NonlinearExplicit\", split, content).rating\n",
    "        linear_split + nonlinear_split\n",
    "    end\n",
    "    write_params(Dict(\"alphas\" => [\"LinearExplicit\", \"NonlinearExplicit\"]), \"Explicit\")\n",
    "    write_alpha(\n",
    "        model,\n",
    "        \"Explicit\";\n",
    "        by_split = true,\n",
    "        log = true,\n",
    "        log_content = \"explicit\",\n",
    "        log_splits = [\"validation\", \"test\"],\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed398d5-ecd5-4707-829b-0a6e09c40ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:56:46 saving validation explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:56:52 validation loss: 1.6501325, β: Float32[0.9999582]\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:56:52 saving test explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:56:53 test loss: 1.7052939, β: Float32[0.9999582]\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:56:53 saving training explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:57:02 saving training implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:57:06 saving training negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:57:09 saving training ptw\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:57:14 saving validation explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:57:14 saving validation implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:57:14 saving validation negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:57:17 saving validation ptw\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:57:17 saving test explicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:57:18 saving test implicit\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:57:19 saving test negative\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220824 20:57:21 saving test ptw\n"
     ]
    }
   ],
   "source": [
    "save_final_explicit_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
