{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df5351f-2171-4365-b750-8984d42c5fe5",
   "metadata": {},
   "source": [
    "# TreeModelBase\n",
    "* Uses LightGBM to fit a tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5a19a-24f5-4730-afe2-f451d27d18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LightGBM\n",
    "\n",
    "import Flux: sigmoid\n",
    "import SparseArrays: sparse\n",
    "import Statistics: mean\n",
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\");\n",
    "@nbinclude(\"EnsembleInputs.ipynb\")\n",
    "@nbinclude(\"ItemMetadata.ipynb\")\n",
    "@nbinclude(\"SuppressImplicit.ipynb\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8b6e4-e53d-4f2b-9183-df18f637b182",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabf561-dd38-46fd-93ee-a465b51d9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function augment_dataset(ds, y, w)\n",
    "    LightGBM.LGBM_DatasetSetField(ds, \"label\", y)\n",
    "    LightGBM.LGBM_DatasetSetField(ds, \"weight\", w)\n",
    "    ds\n",
    "end\n",
    "\n",
    "function create_train_dataset(X, y, w, estimator)\n",
    "    augment_dataset(\n",
    "        LightGBM.LGBM_DatasetCreateFromMat(X, LightGBM.stringifyparams(estimator), false),\n",
    "        y,\n",
    "        w,\n",
    "    )\n",
    "end\n",
    "\n",
    "function create_test_dataset(X, y, w, estimator, train_ds)\n",
    "    augment_dataset(\n",
    "        LightGBM.LGBM_DatasetCreateFromMat(\n",
    "            X,\n",
    "            LightGBM.stringifyparams(estimator),\n",
    "            train_ds,\n",
    "            false,\n",
    "        ),\n",
    "        y,\n",
    "        w,\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e3b27-07c6-4943-a3b7-037dada703b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_features(alphas, split, implicit)\n",
    "    @info \"getting features for $split\"\n",
    "    base_features = reduce(hcat, [read_alpha(x, split, implicit).rating for x in alphas])\n",
    "    metadata_features = get_item_metadata_features(split, implicit)\n",
    "    hcat(base_features, metadata_features)\n",
    "end\n",
    "\n",
    "function get_data(split::String, feature_alphas, target_alphas, implicit, error_model)\n",
    "    X = get_features(feature_alphas, split, implicit)\n",
    "    if implicit\n",
    "        if error_model\n",
    "            # cross entropy loss between the distribution and the true rating\n",
    "            y =\n",
    "                get_split(split, implicit).rating .*\n",
    "                get_weights(split, implicit, \"inverse\") .* 0.5 -\n",
    "                read_alpha(target_alphas, split, implicit).rating\n",
    "            w = get_weights(split, implicit, \"inverse\")\n",
    "        else\n",
    "            @assert false\n",
    "        end\n",
    "    else\n",
    "        y =\n",
    "            get_split(split, implicit).rating -\n",
    "            read_alpha(target_alphas, split, implicit).rating\n",
    "        w = get_weights(split, implicit, \"inverse\")\n",
    "    end\n",
    "    if error_model\n",
    "        y = abs.(y)\n",
    "    end\n",
    "    training_mask = get_split(split, implicit).user .<= num_users() * 0.9\n",
    "    X_train, X_test = X[training_mask, :], X[.!training_mask, :]\n",
    "    y_train, y_test = y[training_mask], y[.!training_mask]\n",
    "    w_train, w_test = w[training_mask], w[.!training_mask]\n",
    "    X_train, X_test, y_train, y_test, w_train, w_test\n",
    "end\n",
    "\n",
    "function get_data(\n",
    "    splits::Vector{String},\n",
    "    feature_alphas,\n",
    "    target_alphas,\n",
    "    implicit,\n",
    "    error_model,\n",
    "    estimator,\n",
    ")\n",
    "    data = []\n",
    "    for split in splits\n",
    "        push!(data, get_data(split, feature_alphas, target_alphas, implicit, error_model))\n",
    "    end\n",
    "    X_train = reduce(vcat, data[n][1] for n = 1:length(data))\n",
    "    X_test = reduce(vcat, data[n][2] for n = 1:length(data))\n",
    "    y_train = reduce(vcat, data[n][3] for n = 1:length(data))\n",
    "    y_test = reduce(vcat, data[n][4] for n = 1:length(data))\n",
    "    w_train = reduce(vcat, data[n][5] for n = 1:length(data))\n",
    "    w_test = reduce(vcat, data[n][6] for n = 1:length(data))\n",
    "    train_ds = create_train_dataset(X_train, y_train, w_train, estimator)\n",
    "    test_ds = create_test_dataset(X_test, y_test, w_test, estimator, train_ds)\n",
    "    train_ds, test_ds\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535156df-73c9-47a2-9463-08650269c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "function memory_efficient_predict!(estimator, features, preds)\n",
    "    batch_size = 1024\n",
    "    n = size(features)[1]\n",
    "    @showprogress for iter = 1:Int(ceil(n / batch_size))\n",
    "        range = (iter-1)*batch_size+1:min(iter * batch_size, n)\n",
    "        batch = features[range, :]\n",
    "        preds[range] .= convert.(Float32, predict(estimator, batch))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c251aa5-45a0-44b3-99ea-110d6d111845",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_estimator(λ)\n",
    "    LGBMRegression(\n",
    "        num_iterations = 1000,\n",
    "        early_stopping_round = 10,\n",
    "        learning_rate = 0.1 * exp(λ[1]),\n",
    "        feature_fraction = sigmoid(λ[2]),\n",
    "        bagging_fraction = sigmoid(λ[3]),\n",
    "        bagging_freq = 1,\n",
    "        num_leaves = Int(round(1000 * exp(λ[4]))),\n",
    "        min_data_in_leaf = Int(round(100 * exp(λ[5]))),\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8dffd6-135b-49a0-98ca-92ae62ae06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sparse_predict(estimator, split, implicit, alphas)\n",
    "    splits = get_split(split, implicit)\n",
    "    split_features = get_features(alphas, split, implicit)\n",
    "    memory_efficient_predict!(estimator, split_features, splits.rating)\n",
    "    sparse_preds = sparse(splits.user, splits.item, splits.rating, num_users(), num_items())    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b8c10-b11a-43de-8e22-955a07e29c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_model(\n",
    "    feature_alphas,\n",
    "    target_alphas,\n",
    "    implicit,\n",
    "    training_splits::Vector{String},\n",
    "    outdir,\n",
    "    error_model;\n",
    "    λ = nothing\n",
    ")\n",
    "    set_logging_outdir(outdir)\n",
    "\n",
    "    # create lightgbm tree model\n",
    "    if isnothing(λ)\n",
    "        estimator = LGBMRegression(\n",
    "        num_iterations = 1000,\n",
    "        learning_rate = 0.1,\n",
    "        early_stopping_round = 10,\n",
    "        feature_fraction = 0.8,\n",
    "        bagging_fraction = 0.9,\n",
    "        bagging_freq = 1,\n",
    "        num_leaves = 1000,\n",
    "        min_data_in_leaf = 100,\n",
    "    )\n",
    "    else\n",
    "        estimator = create_estimator(λ)\n",
    "    end\n",
    "\n",
    "\n",
    "    # get training data\n",
    "    train_ds, test_ds = get_data(\n",
    "        training_splits,\n",
    "        feature_alphas,\n",
    "        target_alphas,\n",
    "        implicit,\n",
    "        error_model,\n",
    "        estimator,\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    fit!(estimator, train_ds, test_ds)\n",
    "\n",
    "    # save model\n",
    "    @info \"Saving model...\"\n",
    "    write_params(Dict(\"model\" => estimator, \"alphas\" => feature_alphas), outdir)\n",
    "    sparse_preds = [sparse_predict(estimator, split, implicit, feature_alphas) for split in all_nontraining_raw_splits]\n",
    "    @info \"concatenating sparse preds\"\n",
    "    sparse_preds = reduce(+, sparse_preds)\n",
    "    @info \"Average model value: $(mean(sparse_preds.nzval))\"\n",
    "    @info \"Average model absolute value: $(mean(abs.(sparse_preds.nzval)))\"\n",
    "    write_alpha(sparse_preds, target_alphas, implicit, outdir; log_splits = false)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aac257-9782-4234-887a-399770b5b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need to tune once every now and then\n",
    "# function nlopt_loss(λ)\n",
    "#     implicit = false\n",
    "#     linear_alphas = [\"LinearExplicit\", \"LinearImplicit\"]\n",
    "#     all_features = [\n",
    "#         explicit_raw_alphas\n",
    "#         implicit_raw_alphas\n",
    "#         nondirectional_raw_alphas\n",
    "#         linear_alphas\n",
    "#     ]\n",
    "#     estimator = create_estimator(λ)\n",
    "#     train_ds, test_ds = get_data(\n",
    "#         [\"validation\"],\n",
    "#         all_features,\n",
    "#         [\"LinearExplicit\"],\n",
    "#         implicit,\n",
    "#         false,\n",
    "#         estimator,\n",
    "#     )\n",
    "#     results = fit!(estimator, train_ds, test_ds, verbosity = -1)\n",
    "#     loss = results[\"metrics\"][\"test_1\"][\"l2\"][end]\n",
    "#     @info loss, λ\n",
    "#     loss\n",
    "# end\n",
    "\n",
    "# function nlopt_loss_no_lr(λ, grad)\n",
    "#     # don't the learning rate\n",
    "#     nlopt_loss([[0.0]; λ])\n",
    "# end\n",
    "\n",
    "# import NLopt\n",
    "# opt = NLopt.Opt(:LN_NELDERMEAD, 4)\n",
    "# opt.initial_step = 1\n",
    "# opt.maxeval = 25\n",
    "# opt.min_objective = nlopt_loss_no_lr\n",
    "# minf, λ, ret = NLopt.optimize(opt, zeros(4))\n",
    "# numevals = opt.numevals;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
