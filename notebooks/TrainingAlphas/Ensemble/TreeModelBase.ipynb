{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df5351f-2171-4365-b750-8984d42c5fe5",
   "metadata": {},
   "source": [
    "# TreeModelBase\n",
    "* Uses LightGBM to fit a tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5a19a-24f5-4730-afe2-f451d27d18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LightGBM\n",
    "\n",
    "import Flux: sigmoid\n",
    "import SparseArrays: sparse\n",
    "import Statistics: mean\n",
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"EnsembleInputs.ipynb\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121cdf27-ec22-4573-823f-a9c8662e2a19",
   "metadata": {},
   "source": [
    "## LightGBM interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabf561-dd38-46fd-93ee-a465b51d9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function augment_dataset(ds, y, w)\n",
    "    LightGBM.LGBM_DatasetSetField(ds, \"label\", y)\n",
    "    LightGBM.LGBM_DatasetSetField(ds, \"weight\", w)\n",
    "    ds\n",
    "end\n",
    "\n",
    "function create_train_dataset(X, y, w, estimator)\n",
    "    augment_dataset(\n",
    "        LightGBM.LGBM_DatasetCreateFromMat(X, LightGBM.stringifyparams(estimator), false),\n",
    "        y,\n",
    "        w,\n",
    "    )\n",
    "end\n",
    "\n",
    "function create_test_dataset(X, y, w, estimator, train_ds)\n",
    "    augment_dataset(\n",
    "        LightGBM.LGBM_DatasetCreateFromMat(\n",
    "            X,\n",
    "            LightGBM.stringifyparams(estimator),\n",
    "            train_ds,\n",
    "            false,\n",
    "        ),\n",
    "        y,\n",
    "        w,\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c251aa5-45a0-44b3-99ea-110d6d111845",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_estimator(λ::Vector{Float64}, implicit::Bool, error::Bool)\n",
    "    n_trees = 100\n",
    "    if error || !implicit\n",
    "        return LGBMRegression(\n",
    "            num_iterations = n_trees,\n",
    "            early_stopping_round = 10,\n",
    "            learning_rate = 0.1 * exp(λ[1]),\n",
    "            feature_fraction = sigmoid(λ[2]),\n",
    "            bagging_fraction = sigmoid(λ[3]),\n",
    "            bagging_freq = 1,\n",
    "            num_leaves = Int(round(1000 * exp(λ[4]))),\n",
    "            min_data_in_leaf = Int(round(100 * exp(λ[5]))),\n",
    "        )\n",
    "    else\n",
    "        estimator = LGBMClassification(\n",
    "            objective = \"binary\",\n",
    "            num_iterations = n_trees,\n",
    "            early_stopping_round = 10,\n",
    "            learning_rate = 0.1 * exp(λ[1]),\n",
    "            feature_fraction = sigmoid(λ[2]),\n",
    "            bagging_fraction = sigmoid(λ[3]),\n",
    "            bagging_freq = 1,\n",
    "            num_leaves = Int(round(1000 * exp(λ[4]))),\n",
    "            min_data_in_leaf = Int(round(100 * exp(λ[5]))),\n",
    "            num_class = 1,\n",
    "            metric = [\"binary_logloss\", \"auc\"],\n",
    "        )\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e8106-6982-42a6-b000-5306b9ca8e6d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e3b27-07c6-4943-a3b7-037dada703b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_features(alphas::Vector{String}, split::String, task::String, content::String)\n",
    "    @info \"getting features for $split, $task, $content\"\n",
    "    base_features =\n",
    "        reduce(hcat, [read_alpha(x, split, task, content).rating for x in alphas])\n",
    "    base_features\n",
    "end\n",
    "\n",
    "function get_data(\n",
    "    split::String,\n",
    "    feature_alphas::Vector{String},\n",
    "    target_alphas::Vector{String},\n",
    "    task::String,\n",
    "    content::String,\n",
    "    error_model::Bool,\n",
    ")\n",
    "    X = get_features(feature_alphas, split, task, content)\n",
    "    if content ∈ [\"implicit\", \"ptw\", \"negative\"]\n",
    "        if error_model\n",
    "            y =\n",
    "                get_split(split, task, content; fields = [:rating]).rating -\n",
    "                read_alpha(target_alphas, split, task, content).rating\n",
    "            w = get_weights(split, content, \"constant\")\n",
    "        else\n",
    "            @assert length(target_alphas) == 0\n",
    "            y = get_split(split, task, content; fields = [:rating]).rating\n",
    "            w = get_weights(split, task, content, \"constant\")\n",
    "        end\n",
    "    elseif content == \"explicit\"\n",
    "        y =\n",
    "            get_split(split, task, content; fields = [:rating]).rating -\n",
    "            read_alpha(target_alphas, split, task, content, false).rating\n",
    "        w = get_weights(split, task, content, \"inverse\")\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    if error_model\n",
    "        y = abs.(y)\n",
    "    end\n",
    "    training_mask = get_split(split, task, content).user .<= num_users() * 0.9\n",
    "    X_train, X_test = X[training_mask, :], X[.!training_mask, :]\n",
    "    y_train, y_test = y[training_mask], y[.!training_mask]\n",
    "    w_train, w_test = w[training_mask], w[.!training_mask]\n",
    "    X_train, X_test, y_train, y_test, w_train, w_test\n",
    "end\n",
    "\n",
    "function get_data(\n",
    "    split::String,\n",
    "    feature_alphas::Vector{String},\n",
    "    target_alphas::Vector{String},\n",
    "    task::String,\n",
    "    contents::Vector{String},\n",
    "    error_model::Bool,\n",
    "    estimator,\n",
    ")\n",
    "    data = []\n",
    "    for content in contents\n",
    "        push!(\n",
    "            data,\n",
    "            get_data(split, feature_alphas, target_alphas, task, content, error_model),\n",
    "        )\n",
    "    end\n",
    "    X_train = reduce(vcat, data[n][1] for n = 1:length(data))\n",
    "    X_test = reduce(vcat, data[n][2] for n = 1:length(data))\n",
    "    y_train = reduce(vcat, data[n][3] for n = 1:length(data))\n",
    "    y_test = reduce(vcat, data[n][4] for n = 1:length(data))\n",
    "    w_train = reduce(vcat, data[n][5] for n = 1:length(data))\n",
    "    w_test = reduce(vcat, data[n][6] for n = 1:length(data))\n",
    "    train_ds = create_train_dataset(X_train, y_train, w_train, estimator)\n",
    "    test_ds = create_test_dataset(X_test, y_test, w_test, estimator, train_ds)\n",
    "    train_ds, test_ds\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e517a-209e-4cef-aaf5-01cf1e64ff66",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535156df-73c9-47a2-9463-08650269c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "function memory_efficient_predict(estimator, features::Matrix{Float32})\n",
    "    batch_size = 1024\n",
    "    n = size(features)[1]\n",
    "    preds = fill(NaN32, n)\n",
    "    @showprogress for iter = 1:Int(ceil(n / batch_size))\n",
    "        range = (iter-1)*batch_size+1:min(iter * batch_size, n)\n",
    "        batch = features[range, :]\n",
    "        preds[range] .= convert.(Float32, predict(estimator, batch))\n",
    "    end\n",
    "    preds\n",
    "end\n",
    "\n",
    "function memory_efficient_predict(\n",
    "    estimator,\n",
    "    alphas::Vector{String},\n",
    "    split::String,\n",
    "    task::String,\n",
    "    content::String,\n",
    "    splits_to_skip::Vector{String},\n",
    "    raw_splits::Bool,\n",
    ")\n",
    "    @assert \"training\" in splits_to_skip\n",
    "    @info \"predicting $split $task $content\"\n",
    "    if raw_splits\n",
    "        split_fn = get_raw_split\n",
    "        alpha_fn = read_raw_alpha\n",
    "    else\n",
    "        split_fn = get_split\n",
    "        alpha_fn = read_alpha\n",
    "    end\n",
    "    df = split_fn(split, task, content; fields = [:user, :item])\n",
    "    if split in splits_to_skip\n",
    "        return zeros(Float32, length(df.user))\n",
    "    end\n",
    "\n",
    "    N = length(df.user)\n",
    "    preds = zeros(Float32, N)\n",
    "    chunk_size = Int(5e7)\n",
    "    n_chunks = Int(ceil(N / chunk_size))\n",
    "    for i = 1:n_chunks\n",
    "        @info \"saving chunk $i out of $n_chunks ($N / $chunk_size)\"\n",
    "        range = (i-1)*chunk_size+1:min(i * chunk_size - 1, N)\n",
    "        features =\n",
    "            reduce(hcat, [alpha_fn(x, split, task, content).rating[range] for x in alphas])\n",
    "        preds[range] .= memory_efficient_predict(estimator, features)\n",
    "    end\n",
    "    preds\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc317a8-3a69-465b-bd50-65983dcd5f5b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b8c10-b11a-43de-8e22-955a07e29c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_model(\n",
    "    feature_alphas::Vector{String},\n",
    "    target_alphas::Vector{String},\n",
    "    task::String,\n",
    "    contents::Vector{String},\n",
    "    implicit::Bool,\n",
    "    training_split::String,\n",
    "    error_model::Bool,\n",
    "    outdir::String;\n",
    "    λ::Union{Vector{Float32},Nothing} = nothing,\n",
    ")\n",
    "    set_logging_outdir(outdir)\n",
    "\n",
    "    # create a lightgbm tree model\n",
    "    if isnothing(λ)\n",
    "        inverse_sigmoid(y) = -log(1 / y - 1)\n",
    "        λ = [0, inverse_sigmoid(0.8), inverse_sigmoid(0.9), 0, 0]\n",
    "    end\n",
    "    estimator = create_estimator(λ, implicit, error_model)\n",
    "\n",
    "    # get training data\n",
    "    train_ds, test_ds = get_data(\n",
    "        training_split,\n",
    "        feature_alphas,\n",
    "        target_alphas,\n",
    "        task,\n",
    "        contents,\n",
    "        error_model,\n",
    "        estimator,\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    fit!(estimator, train_ds, test_ds)\n",
    "\n",
    "    # save model\n",
    "    @info \"Saving model...\"\n",
    "    write_params(Dict(\"model\" => estimator, \"alphas\" => feature_alphas), outdir)\n",
    "    if training_split == \"test\"\n",
    "        splits_to_skip = [\"training\", \"validation\"]\n",
    "    elseif training_split == \"validation\"\n",
    "        splits_to_skip = [\"training\"]\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    function model(split, task, content; raw_splits)\n",
    "        memory_efficient_predict(\n",
    "            estimator,\n",
    "            feature_alphas,\n",
    "            split,\n",
    "            task,\n",
    "            content,\n",
    "            splits_to_skip,\n",
    "            raw_splits,\n",
    "        )\n",
    "    end\n",
    "    write_alpha(model, outdir; task = task, by_split = true, log = false)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aac257-9782-4234-887a-399770b5b5ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we only need to tune once every now and then\n",
    "# function nlopt_loss(λ)\n",
    "#     implicit = false\n",
    "#     linear_alphas = [\"LinearExplicit\", \"LinearImplicit\"]\n",
    "#     all_features = [\n",
    "#         explicit_raw_alphas\n",
    "#         implicit_raw_alphas\n",
    "#         nondirectional_raw_alphas\n",
    "#         linear_alphas\n",
    "#     ]\n",
    "#     estimator = create_estimator(λ)\n",
    "#     train_ds, test_ds = get_data(\n",
    "#         [\"validation\"],\n",
    "#         all_features,\n",
    "#         [\"LinearExplicit\"],\n",
    "#         implicit,\n",
    "#         false,\n",
    "#         estimator,\n",
    "#     )\n",
    "#     results = fit!(estimator, train_ds, test_ds, verbosity = -1)\n",
    "#     loss = results[\"metrics\"][\"test_1\"][\"l2\"][end]\n",
    "#     @info loss, λ\n",
    "#     loss\n",
    "# end\n",
    "\n",
    "# function nlopt_loss_no_lr(λ, grad)\n",
    "#     # don't the learning rate\n",
    "#     nlopt_loss([[0.0]; λ])\n",
    "# end\n",
    "\n",
    "# import NLopt\n",
    "# opt = NLopt.Opt(:LN_NELDERMEAD, 4)\n",
    "# opt.initial_step = 1\n",
    "# opt.maxeval = 25\n",
    "# opt.min_objective = nlopt_loss_no_lr\n",
    "# minf, λ, ret = NLopt.optimize(opt, zeros(4))\n",
    "# numevals = opt.numevals;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
