{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df5351f-2171-4365-b750-8984d42c5fe5",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "* Runs all the ensemble alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b5a19a-24f5-4730-afe2-f451d27d18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf2b7dc-6ec0-44f9-9f88-1df84ba11362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:18:50 alphas: [\"ExplicitUserItemBiases\", \"ExplicitItemCF\", \"NeuralExplicitMatrixFactorization\", \"NeuralExplicitItemCFUntuned\", \"NeuralExplicitAutoencoderUntuned\"]\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:18:50 coefficients: Float32[1.0176827, 0.16860016, -0.029639833, 0.46684384, 0.5104295]\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:20\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.48 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:19:34 validation loss: 1.3584444563770786, β: [0.99999759584032]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.39 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 (10.95 ns/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:19:51 training loss: 1.0864266849786999, β: [0.99999759584032]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.50 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:19:53 test loss: 1.3681797487033311, β: [0.99999759584032]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.43 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.43 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.42 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.45 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.45 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.03 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.03 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:21:49 alphas: [\"NeuralImplicitUserItemBiases\", \"NeuralImplicitMatrixFactorization\", \"NeuralImplicitItemCFUntuned\"]\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:21:49 coefficients: Float32[0.019830352, 0.6352052, 0.34011975, 0.0048447805]\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:15\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:02 ( 1.42 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:22:59 validation loss: 6.266239148137566, β: Float32[6.525346f-7, 0.9999994]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:18 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 8.59 ns/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:23:24 training loss: 5.474239612923833, β: Float32[6.525346f-7, 0.9999994]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:02 ( 1.49 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:23:28 test loss: 6.259880538564864, β: Float32[6.525346f-7, 0.9999994]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.47 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.39 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.43 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.03 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.04 μs/it)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"LinearModel.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517ce91a-bf09-446a-8be9-a7c5bbe0c0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:24:10 lib_lightgbm not found in system dirs, trying fallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 22508916, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.002064\n",
      "Iteration: 1, test_1's l2: 1.3538376757503725\n",
      "Iteration: 2, test_1's l2: 1.3501616023121645\n",
      "Iteration: 3, test_1's l2: 1.3471443143826738\n",
      "Iteration: 4, test_1's l2: 1.3446453419935644\n",
      "Iteration: 5, test_1's l2: 1.342294067792423\n",
      "Iteration: 6, test_1's l2: 1.340473513782381\n",
      "Iteration: 7, test_1's l2: 1.3389751619179497\n",
      "Iteration: 8, test_1's l2: 1.337715281465482\n",
      "Iteration: 9, test_1's l2: 1.3364703552430666\n",
      "Iteration: 10, test_1's l2: 1.3354742270624322\n",
      "Iteration: 11, test_1's l2: 1.3345422455327554\n",
      "Iteration: 12, test_1's l2: 1.3336527888957206\n",
      "Iteration: 13, test_1's l2: 1.332984893758936\n",
      "Iteration: 14, test_1's l2: 1.3323876861491777\n",
      "Iteration: 15, test_1's l2: 1.331784218494369\n",
      "Iteration: 16, test_1's l2: 1.3314430727381283\n",
      "Iteration: 17, test_1's l2: 1.3310142231300002\n",
      "Iteration: 18, test_1's l2: 1.3305507083797163\n",
      "Iteration: 19, test_1's l2: 1.3301315921552095\n",
      "Iteration: 20, test_1's l2: 1.3298807128012076\n",
      "Iteration: 21, test_1's l2: 1.3295584549164183\n",
      "Iteration: 22, test_1's l2: 1.329301565485338\n",
      "Iteration: 23, test_1's l2: 1.3290950785300772\n",
      "Iteration: 24, test_1's l2: 1.3289066279350474\n",
      "Iteration: 25, test_1's l2: 1.328793945516043\n",
      "Iteration: 26, test_1's l2: 1.3285543057014335\n",
      "Iteration: 27, test_1's l2: 1.3284285463251044\n",
      "Iteration: 28, test_1's l2: 1.3283216552879527\n",
      "Iteration: 29, test_1's l2: 1.3281649562835696\n",
      "Iteration: 30, test_1's l2: 1.3280857917677158\n",
      "Iteration: 31, test_1's l2: 1.328012169546483\n",
      "Iteration: 32, test_1's l2: 1.3279881802485232\n",
      "Iteration: 33, test_1's l2: 1.32784487284821\n",
      "Iteration: 34, test_1's l2: 1.3278399459483312\n",
      "Iteration: 35, test_1's l2: 1.327772978296002\n",
      "Iteration: 36, test_1's l2: 1.327715058372089\n",
      "Iteration: 37, test_1's l2: 1.3275579057243092\n",
      "Iteration: 38, test_1's l2: 1.3275033499628577\n",
      "Iteration: 39, test_1's l2: 1.3274421469506126\n",
      "Iteration: 40, test_1's l2: 1.327296550071059\n",
      "Iteration: 41, test_1's l2: 1.3272327094784209\n",
      "Iteration: 42, test_1's l2: 1.3271870358905495\n",
      "Iteration: 43, test_1's l2: 1.3271856441859466\n",
      "Iteration: 44, test_1's l2: 1.327144562233934\n",
      "Iteration: 45, test_1's l2: 1.3269581619220412\n",
      "Iteration: 46, test_1's l2: 1.3268979122242648\n",
      "Iteration: 47, test_1's l2: 1.3269129504252195\n",
      "Iteration: 48, test_1's l2: 1.3268660061040616\n",
      "Iteration: 49, test_1's l2: 1.3268385748241966\n",
      "Iteration: 50, test_1's l2: 1.326772089344327\n",
      "Iteration: 51, test_1's l2: 1.3267879778122025\n",
      "Iteration: 52, test_1's l2: 1.326688398177422\n",
      "Iteration: 53, test_1's l2: 1.32669325090045\n",
      "Iteration: 54, test_1's l2: 1.3266701902445597\n",
      "Iteration: 55, test_1's l2: 1.326566658026416\n",
      "Iteration: 56, test_1's l2: 1.326554985146155\n",
      "Iteration: 57, test_1's l2: 1.3264366428929164\n",
      "Iteration: 58, test_1's l2: 1.3264264274603978\n",
      "Iteration: 59, test_1's l2: 1.3264246373711066\n",
      "Iteration: 60, test_1's l2: 1.3264830371334873\n",
      "Iteration: 61, test_1's l2: 1.3265023838927763\n",
      "Iteration: 62, test_1's l2: 1.3265200861187556\n",
      "Iteration: 63, test_1's l2: 1.3265035170970272\n",
      "Iteration: 64, test_1's l2: 1.3265343159765588\n",
      "Iteration: 65, test_1's l2: 1.3265983183840027\n",
      "Iteration: 66, test_1's l2: 1.326650335502643\n",
      "Iteration: 67, test_1's l2: 1.326654197648068\n",
      "Iteration: 68, test_1's l2: 1.3266553696999643\n",
      "Early stopping at iteration 69, the best iteration round is 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:25:03 Saving model...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:04:02\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:30:22 Average model value: 0.056173906\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:30:23 Average model absolute value: 0.17983936\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.42 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.42 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.47 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.47 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.45 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.04 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.05 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.45 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:31:40 validation loss: 1.2837550266973863, β: [1.0002833521015482]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 (11.08 ns/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:31:57 training loss: 1.038561693206658, β: [1.0002833521015482]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:31:59 test loss: 1.3357724780327678, β: [1.0002833521015482]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.39 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.44 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.44 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.48 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.45 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.03 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.04 μs/it)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"NonlinearModel.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765ef7e7-abf6-432e-b55d-2eea75753f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.11 μs/it)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3315\n",
      "[LightGBM] [Info] Number of data points in the train set: 66963121, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.104162\n",
      "Iteration: 1, test_1's l2: 0.04210724560285368\n",
      "Iteration: 2, test_1's l2: 0.037187299359665776\n",
      "Iteration: 3, test_1's l2: 0.03305286348117026\n",
      "Iteration: 4, test_1's l2: 0.029686661684525037\n",
      "Iteration: 5, test_1's l2: 0.027916939851296636\n",
      "Iteration: 6, test_1's l2: 0.02553470313784\n",
      "Iteration: 7, test_1's l2: 0.023601511989526158\n",
      "Iteration: 8, test_1's l2: 0.02202771543841103\n",
      "Iteration: 9, test_1's l2: 0.02075652424078879\n",
      "Iteration: 10, test_1's l2: 0.019719831638364306\n",
      "Iteration: 11, test_1's l2: 0.01890648740365084\n",
      "Iteration: 12, test_1's l2: 0.01821426744685492\n",
      "Iteration: 13, test_1's l2: 0.017656293301793143\n",
      "Iteration: 14, test_1's l2: 0.01720491625657941\n",
      "Iteration: 15, test_1's l2: 0.016835148933406385\n",
      "Iteration: 16, test_1's l2: 0.01653370933674349\n",
      "Iteration: 17, test_1's l2: 0.01628900044229013\n",
      "Iteration: 18, test_1's l2: 0.016174774852916542\n",
      "Iteration: 19, test_1's l2: 0.01599924050067444\n",
      "Iteration: 20, test_1's l2: 0.01585619219219645\n",
      "Iteration: 21, test_1's l2: 0.01574001356604478\n",
      "Iteration: 22, test_1's l2: 0.015689536271317008\n",
      "Iteration: 23, test_1's l2: 0.015605524093915118\n",
      "Iteration: 24, test_1's l2: 0.015537644567285314\n",
      "Iteration: 25, test_1's l2: 0.015482084000742649\n",
      "Iteration: 26, test_1's l2: 0.015461322280782541\n",
      "Iteration: 27, test_1's l2: 0.01541850295232133\n",
      "Iteration: 28, test_1's l2: 0.015404249702527116\n",
      "Iteration: 29, test_1's l2: 0.01538981385115605\n",
      "Iteration: 30, test_1's l2: 0.015359256878610617\n",
      "Iteration: 31, test_1's l2: 0.01535162902473734\n",
      "Iteration: 32, test_1's l2: 0.015328411893991\n",
      "Iteration: 33, test_1's l2: 0.015323790727855814\n",
      "Iteration: 34, test_1's l2: 0.015305147930167637\n",
      "Iteration: 35, test_1's l2: 0.015301697953183785\n",
      "Iteration: 36, test_1's l2: 0.01528672435823291\n",
      "Iteration: 37, test_1's l2: 0.015275676890642952\n",
      "Iteration: 38, test_1's l2: 0.015275351949971153\n",
      "Iteration: 39, test_1's l2: 0.015265657067767599\n",
      "Iteration: 40, test_1's l2: 0.015259822269543014\n",
      "Iteration: 41, test_1's l2: 0.015259780594931698\n",
      "Iteration: 42, test_1's l2: 0.01525886371783941\n",
      "Iteration: 43, test_1's l2: 0.01525350890816692\n",
      "Iteration: 44, test_1's l2: 0.015248708432474752\n",
      "Iteration: 45, test_1's l2: 0.01524435444717186\n",
      "Iteration: 46, test_1's l2: 0.015242700348918007\n",
      "Iteration: 47, test_1's l2: 0.015241768061334555\n",
      "Iteration: 48, test_1's l2: 0.015239976783693666\n",
      "Iteration: 49, test_1's l2: 0.015239902224763898\n",
      "Iteration: 50, test_1's l2: 0.015238244221092812\n",
      "Iteration: 51, test_1's l2: 0.01523934830770076\n",
      "Iteration: 52, test_1's l2: 0.015238268533723957\n",
      "Iteration: 53, test_1's l2: 0.0152388921553023\n",
      "Iteration: 54, test_1's l2: 0.01523767355297712\n",
      "Iteration: 55, test_1's l2: 0.015237160013260677\n",
      "Iteration: 56, test_1's l2: 0.015237700141112505\n",
      "Iteration: 57, test_1's l2: 0.015238032988650748\n",
      "Iteration: 58, test_1's l2: 0.015238223101587955\n",
      "Iteration: 59, test_1's l2: 0.015238587378185987\n",
      "Iteration: 60, test_1's l2: 0.01523842275496199\n",
      "Iteration: 61, test_1's l2: 0.015239029358337737\n",
      "Iteration: 62, test_1's l2: 0.01524064465647062\n",
      "Iteration: 63, test_1's l2: 0.015241903790036003\n",
      "Iteration: 64, test_1's l2: 0.015243009976405307\n",
      "Early stopping at iteration 65, the best iteration round is 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:34:33 Saving model...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:53\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:37:56 Average model value: 0.0647673\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:37:58 Average model absolute value: 0.064773984\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.42 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.42 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.46 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.43 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.06 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.03 μs/it)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3315\n",
      "[LightGBM] [Info] Number of data points in the train set: 22508916, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.814650\n",
      "Iteration: 1, test_1's l2: 0.6352539645496822\n",
      "Iteration: 2, test_1's l2: 0.6268879818995818\n",
      "Iteration: 3, test_1's l2: 0.6202649577945503\n",
      "Iteration: 4, test_1's l2: 0.6150349693391807\n",
      "Iteration: 5, test_1's l2: 0.6105235781655323\n",
      "Iteration: 6, test_1's l2: 0.6064483988118592\n",
      "Iteration: 7, test_1's l2: 0.6030762389242282\n",
      "Iteration: 8, test_1's l2: 0.5999594271962286\n",
      "Iteration: 9, test_1's l2: 0.5976698253425086\n",
      "Iteration: 10, test_1's l2: 0.5957145474085933\n",
      "Iteration: 11, test_1's l2: 0.59405465500096\n",
      "Iteration: 12, test_1's l2: 0.592483946837338\n",
      "Iteration: 13, test_1's l2: 0.5912606418888865\n",
      "Iteration: 14, test_1's l2: 0.5901953703567887\n",
      "Iteration: 15, test_1's l2: 0.5893388184935471\n",
      "Iteration: 16, test_1's l2: 0.5884974627896393\n",
      "Iteration: 17, test_1's l2: 0.587793152644472\n",
      "Iteration: 18, test_1's l2: 0.5871659750880253\n",
      "Iteration: 19, test_1's l2: 0.5866016056793814\n",
      "Iteration: 20, test_1's l2: 0.5861162299061577\n",
      "Iteration: 21, test_1's l2: 0.5856685756298693\n",
      "Iteration: 22, test_1's l2: 0.5851441835248741\n",
      "Iteration: 23, test_1's l2: 0.5847866812434972\n",
      "Iteration: 24, test_1's l2: 0.5844901609626496\n",
      "Iteration: 25, test_1's l2: 0.5842651105560003\n",
      "Iteration: 26, test_1's l2: 0.5836935214697594\n",
      "Iteration: 27, test_1's l2: 0.583339364393517\n",
      "Iteration: 28, test_1's l2: 0.5830661772155254\n",
      "Iteration: 29, test_1's l2: 0.5827552097850738\n",
      "Iteration: 30, test_1's l2: 0.5825637412541823\n",
      "Iteration: 31, test_1's l2: 0.5823506391097761\n",
      "Iteration: 32, test_1's l2: 0.5822024132431651\n",
      "Iteration: 33, test_1's l2: 0.5819024538137042\n",
      "Iteration: 34, test_1's l2: 0.581607971828411\n",
      "Iteration: 35, test_1's l2: 0.5813348468764279\n",
      "Iteration: 36, test_1's l2: 0.5812112953140707\n",
      "Iteration: 37, test_1's l2: 0.5809822624206764\n",
      "Iteration: 38, test_1's l2: 0.5808336163826111\n",
      "Iteration: 39, test_1's l2: 0.5806761337200834\n",
      "Iteration: 40, test_1's l2: 0.580512188727936\n",
      "Iteration: 41, test_1's l2: 0.5803831555471703\n",
      "Iteration: 42, test_1's l2: 0.5802021260659042\n",
      "Iteration: 43, test_1's l2: 0.5800993701304423\n",
      "Iteration: 44, test_1's l2: 0.5800432810690442\n",
      "Iteration: 45, test_1's l2: 0.5798489735942564\n",
      "Iteration: 46, test_1's l2: 0.5797996021696474\n",
      "Iteration: 47, test_1's l2: 0.5796081027996431\n",
      "Iteration: 48, test_1's l2: 0.5794956453539701\n",
      "Iteration: 49, test_1's l2: 0.5794528647482405\n",
      "Iteration: 50, test_1's l2: 0.5793769116458274\n",
      "Iteration: 51, test_1's l2: 0.5793707158227617\n",
      "Iteration: 52, test_1's l2: 0.5791835481986851\n",
      "Iteration: 53, test_1's l2: 0.5790084706464376\n",
      "Iteration: 54, test_1's l2: 0.578921772439891\n",
      "Iteration: 55, test_1's l2: 0.5788349326012644\n",
      "Iteration: 56, test_1's l2: 0.5786517300904309\n",
      "Iteration: 57, test_1's l2: 0.5785479349291051\n",
      "Iteration: 58, test_1's l2: 0.578358605212123\n",
      "Iteration: 59, test_1's l2: 0.5782167634723532\n",
      "Iteration: 60, test_1's l2: 0.5781364663243558\n",
      "Iteration: 61, test_1's l2: 0.5780556622337784\n",
      "Iteration: 62, test_1's l2: 0.5779376426178761\n",
      "Iteration: 63, test_1's l2: 0.5779007847324487\n",
      "Iteration: 64, test_1's l2: 0.5777343198555484\n",
      "Iteration: 65, test_1's l2: 0.5777260908578806\n",
      "Iteration: 66, test_1's l2: 0.5777374530664621\n",
      "Iteration: 67, test_1's l2: 0.5776705429762151\n",
      "Iteration: 68, test_1's l2: 0.5775688208478399\n",
      "Iteration: 69, test_1's l2: 0.5775762326704846\n",
      "Iteration: 70, test_1's l2: 0.5775726718047729\n",
      "Iteration: 71, test_1's l2: 0.5775629461576495\n",
      "Iteration: 72, test_1's l2: 0.5775269574270899\n",
      "Iteration: 73, test_1's l2: 0.5774925226301666\n",
      "Iteration: 74, test_1's l2: 0.5774734456858363\n",
      "Iteration: 75, test_1's l2: 0.577466989829059\n",
      "Iteration: 76, test_1's l2: 0.577444669825534\n",
      "Iteration: 77, test_1's l2: 0.5772609743199834\n",
      "Iteration: 78, test_1's l2: 0.5772853151203364\n",
      "Iteration: 79, test_1's l2: 0.5772818403291617\n",
      "Iteration: 80, test_1's l2: 0.5772943920120635\n",
      "Iteration: 81, test_1's l2: 0.5772951399546564\n",
      "Iteration: 82, test_1's l2: 0.5773643471840549\n",
      "Iteration: 83, test_1's l2: 0.5773683619536999\n",
      "Iteration: 84, test_1's l2: 0.577377493934138\n",
      "Iteration: 85, test_1's l2: 0.5773809621293461\n",
      "Iteration: 86, test_1's l2: 0.5774264309934087\n",
      "Early stopping at iteration 87, the best iteration round is 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:39:43 Saving model...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:05:51\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:47:02 Average model value: 0.92416865\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:47:03 Average model absolute value: 0.9241687\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.43 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.42 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.46 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.44 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.05 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.05 μs/it)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"ErrorModel.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f9ca73-b22b-4565-b492-a0bdc4a193c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:47:44 Optimizing hyperparameters...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:02\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:25\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:11\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:02\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:01\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:04\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:44\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:27\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:02:32\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:56:05 loss 0.18181119664171336\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:56:38 loss 0.17967817743784165\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:57:04 loss 0.18178368837304584\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:57:04 Float32[0.0, 0.0] 0.17967817743784165\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:57:38 loss 0.18196715128979382\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:58:11 loss 0.18117378522813748\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:58:46 loss 0.18035041577116312\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:59:12 loss 0.18025348681474532\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:59:46 loss 0.18083748668263608\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 23:59:46 Float32[1.0, 0.0] 0.18025348681474532\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:00:21 loss 0.1849213486930102\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:00:55 loss 0.191530598515117\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:00:55 Float32[0.0, 1.0] 0.1849213486930102\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:01:21 loss 0.18264283248834995\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:01:56 loss 0.18072645807999208\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:02:30 loss 0.18252254251892366\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:02:30 Float32[1.0, -1.0] 0.18072645807999208\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:03:05 loss 0.1854236047083996\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:03:31 loss 0.1799242546235962\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:04:05 loss 0.18111199935914346\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:04:05 Float32[0.75, -0.5] 0.1799242546235962\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:04:40 loss 0.18341996252356904\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:05:14 loss 0.18032441661183404\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:05:41 loss 0.1807724864740994\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:05:41 Float32[-0.25, -0.5] 0.18032441661183404\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:06:16 loss 0.18166625193945196\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:06:51 loss 0.1806827523783889\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:07:25 loss 0.18040878444021938\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:07:52 loss 0.17919621354384227\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:08:27 loss 0.18543692548087928\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:08:27 Float32[0.6875, -0.125] 0.17919621354384227\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:09:02 loss 0.18164809795201206\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:09:36 loss 0.18143691114100452\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:10:03 loss 0.18312750100774705\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:10:03 Float32[-0.0625, 0.375] 0.18143691114100452\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:10:38 loss 0.1843386506391264\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:11:13 loss 0.18197729714813435\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:11:48 loss 0.17982659554400565\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:12:15 loss 0.17940505357091718\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:12:49 loss 0.18043497962485794\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:12:49 Float32[0.546875, -0.28125] 0.17940505357091718\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:13:24 loss 0.18283438420057388\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:13:59 loss 0.18091304359661772\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:14:35 loss 0.18266738504902097\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:14:35 Float32[1.234375, -0.40625] 0.18091304359661772\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:14:35 found minimum 0.17919621354384227 at point [0.6875, -0.125] after 10 function calls (ended because MAXEVAL_REACHED) and saved model at\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:14:35 Training model...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:14:39 loss 0.19432475986974457\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:14:43 loss 0.18737783935869562\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:14:47 loss 0.1845668392028956\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:14:51 loss 0.1815240993190828\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:14:54 loss 0.18226342017315145\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:14:58 loss 0.18494911493557167\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:03 loss 0.18097667348290425\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:06 loss 0.18167807143820947\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:10 loss 0.18068486459972002\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:14 loss 0.1821964116916707\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:18 loss 0.18174801911893138\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:22 loss 0.18230944060236046\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:35 loss 0.17967599274549156\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:39 loss 0.18028806106678183\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:43 loss 0.1838761184157987\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:47 loss 0.1833372090344877\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:51 loss 0.1814611626881017\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:55 loss 0.18041545098702982\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:15:59 loss 0.18026135759010084\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:16:03 loss 0.1808382692718362\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:16:07 loss 0.1803846103836572\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:16:11 loss 0.1801005854095346\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:16:15 loss 0.18098445919629358\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:16:19 loss 0.18023611726580094\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:16:19 Trained model loss: 0.17967599274549156\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:16:19 Writing alpha...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220703 00:16:20 Wrote alpha!\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"BPR.ipynb\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
