{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df5351f-2171-4365-b750-8984d42c5fe5",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "* Runs all the ensemble alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b5a19a-24f5-4730-afe2-f451d27d18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf2b7dc-6ec0-44f9-9f88-1df84ba11362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:16:51 alphas: [\"ExplicitUserItemBiases\", \"NeuralExplicitMatrixFactorization\"]\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:16:51 coefficients: Float32[0.99515796, 0.509932]\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:12\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.44 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:17:22 validation loss: 1.6258118731651534, β: [0.9999977640713184]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 (10.92 ns/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:17:39 training loss: 0.8700420963826132, β: [0.9999977640713184]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.53 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:17:42 test loss: 1.6348366182808112, β: [0.9999977640713184]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.39 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.39 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.44 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.95 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.38 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.45 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.42 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.96 μs/it)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"LinearModel.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517ce91a-bf09-446a-8be9-a7c5bbe0c0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:21:33 lib_lightgbm not found in system dirs, trying fallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 22508916, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 0.001410\n",
      "Iteration: 1, test_1's l2: 1.621419582538166\n",
      "Iteration: 2, test_1's l2: 1.6162875647006705\n",
      "Iteration: 3, test_1's l2: 1.6113765689509665\n",
      "Iteration: 4, test_1's l2: 1.606572218924535\n",
      "Iteration: 5, test_1's l2: 1.6037578243579262\n",
      "Iteration: 6, test_1's l2: 1.6002862110762182\n",
      "Iteration: 7, test_1's l2: 1.5974499365145882\n",
      "Iteration: 8, test_1's l2: 1.5953835367057212\n",
      "Iteration: 9, test_1's l2: 1.5933357247305615\n",
      "Iteration: 10, test_1's l2: 1.5915529663092882\n",
      "Iteration: 11, test_1's l2: 1.5900228779224828\n",
      "Iteration: 12, test_1's l2: 1.588946312145382\n",
      "Iteration: 13, test_1's l2: 1.5877911121270096\n",
      "Iteration: 14, test_1's l2: 1.5867487840334638\n",
      "Iteration: 15, test_1's l2: 1.5859653498652282\n",
      "Iteration: 16, test_1's l2: 1.5852869558815175\n",
      "Iteration: 17, test_1's l2: 1.5850242269581005\n",
      "Iteration: 18, test_1's l2: 1.5843673457457073\n",
      "Iteration: 19, test_1's l2: 1.584151950715137\n",
      "Iteration: 20, test_1's l2: 1.5836644284200492\n",
      "Iteration: 21, test_1's l2: 1.5833524848138572\n",
      "Iteration: 22, test_1's l2: 1.5832467416826914\n",
      "Iteration: 23, test_1's l2: 1.5827909722964413\n",
      "Iteration: 24, test_1's l2: 1.5824049604289898\n",
      "Iteration: 25, test_1's l2: 1.5820564532195909\n",
      "Iteration: 26, test_1's l2: 1.581999178485521\n",
      "Iteration: 27, test_1's l2: 1.5816848220967255\n",
      "Iteration: 28, test_1's l2: 1.5814360125792357\n",
      "Iteration: 29, test_1's l2: 1.5811002081189611\n",
      "Iteration: 30, test_1's l2: 1.5808437629013794\n",
      "Iteration: 31, test_1's l2: 1.580653159381274\n",
      "Iteration: 32, test_1's l2: 1.58047172951734\n",
      "Iteration: 33, test_1's l2: 1.5804721062154292\n",
      "Iteration: 34, test_1's l2: 1.5803039713988458\n",
      "Iteration: 35, test_1's l2: 1.5800952614217407\n",
      "Iteration: 36, test_1's l2: 1.5800073813548932\n",
      "Iteration: 37, test_1's l2: 1.5799326443617974\n",
      "Iteration: 38, test_1's l2: 1.5798772037928026\n",
      "Iteration: 39, test_1's l2: 1.5797866876721427\n",
      "Iteration: 40, test_1's l2: 1.5796854302759424\n",
      "Iteration: 41, test_1's l2: 1.579592193394398\n",
      "Iteration: 42, test_1's l2: 1.5795362125490506\n",
      "Iteration: 43, test_1's l2: 1.5794429854459746\n",
      "Iteration: 44, test_1's l2: 1.5794415975906915\n",
      "Iteration: 45, test_1's l2: 1.579424091216826\n",
      "Iteration: 46, test_1's l2: 1.579423121920048\n",
      "Iteration: 47, test_1's l2: 1.5793720053224904\n",
      "Iteration: 48, test_1's l2: 1.5792560642307036\n",
      "Iteration: 49, test_1's l2: 1.5792803338813999\n",
      "Iteration: 50, test_1's l2: 1.5793082572412582\n",
      "Iteration: 51, test_1's l2: 1.579251041925754\n",
      "Iteration: 52, test_1's l2: 1.5792199323938\n",
      "Iteration: 53, test_1's l2: 1.5791703242859367\n",
      "Iteration: 54, test_1's l2: 1.579225108340393\n",
      "Iteration: 55, test_1's l2: 1.57923678321752\n",
      "Iteration: 56, test_1's l2: 1.5791740052239291\n",
      "Iteration: 57, test_1's l2: 1.5792194067385459\n",
      "Iteration: 58, test_1's l2: 1.5792016820307537\n",
      "Iteration: 59, test_1's l2: 1.5791834715901725\n",
      "Iteration: 60, test_1's l2: 1.5792130714081256\n",
      "Iteration: 61, test_1's l2: 1.5791968190585082\n",
      "Iteration: 62, test_1's l2: 1.5792248298359652\n",
      "Early stopping at iteration 63, the best iteration round is 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:22:13 Saving model... (this may take a while)\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:12 ( 1.35 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.36 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.39 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.34 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.37 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.37 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.90 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.52 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:25:55 validation loss: 1.5521670341714102, β: [1.0001874381155869]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.39 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 (10.98 ns/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:26:12 training loss: 0.8083495402555968, β: [1.0001874381155869]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.49 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:26:14 test loss: 1.5858682830605102, β: [1.0001874381155869]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.39 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.44 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.96 μs/it)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"NonlinearModel.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765ef7e7-abf6-432e-b55d-2eea75753f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052786 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 25001360, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 0.926024\n",
      "Iteration: 1, test_1's l2: 0.7178269303115166\n",
      "Iteration: 2, test_1's l2: 0.711579566335258\n",
      "Iteration: 3, test_1's l2: 0.7063374157978112\n",
      "Iteration: 4, test_1's l2: 0.7019408604713154\n",
      "Iteration: 5, test_1's l2: 0.698363678682952\n",
      "Iteration: 6, test_1's l2: 0.6955438990719512\n",
      "Iteration: 7, test_1's l2: 0.6930834002639391\n",
      "Iteration: 8, test_1's l2: 0.6912698343432959\n",
      "Iteration: 9, test_1's l2: 0.6895863531512014\n",
      "Iteration: 10, test_1's l2: 0.6882573375530971\n",
      "Iteration: 11, test_1's l2: 0.687091786277182\n",
      "Iteration: 12, test_1's l2: 0.6861599650686281\n",
      "Iteration: 13, test_1's l2: 0.6854197026276467\n",
      "Iteration: 14, test_1's l2: 0.684763371658274\n",
      "Iteration: 15, test_1's l2: 0.6842360748459325\n",
      "Iteration: 16, test_1's l2: 0.6837682803160401\n",
      "Iteration: 17, test_1's l2: 0.683403170182184\n",
      "Iteration: 18, test_1's l2: 0.6830420044680667\n",
      "Iteration: 19, test_1's l2: 0.682734564839281\n",
      "Iteration: 20, test_1's l2: 0.6824924535950826\n",
      "Iteration: 21, test_1's l2: 0.6823782214766478\n",
      "Iteration: 22, test_1's l2: 0.6821994524400976\n",
      "Iteration: 23, test_1's l2: 0.6820422184771113\n",
      "Iteration: 24, test_1's l2: 0.6818031437240657\n",
      "Iteration: 25, test_1's l2: 0.6817199978538386\n",
      "Iteration: 26, test_1's l2: 0.6815083856802446\n",
      "Iteration: 27, test_1's l2: 0.6814450343852427\n",
      "Iteration: 28, test_1's l2: 0.6813051996358246\n",
      "Iteration: 29, test_1's l2: 0.681259063724313\n",
      "Iteration: 30, test_1's l2: 0.6812182569657853\n",
      "Iteration: 31, test_1's l2: 0.6810847238241903\n",
      "Iteration: 32, test_1's l2: 0.6809712007045602\n",
      "Iteration: 33, test_1's l2: 0.6808742000754899\n",
      "Iteration: 34, test_1's l2: 0.6808331811179993\n",
      "Iteration: 35, test_1's l2: 0.6808243264226211\n",
      "Iteration: 36, test_1's l2: 0.6807504833286311\n",
      "Iteration: 37, test_1's l2: 0.6807727790453496\n",
      "Iteration: 38, test_1's l2: 0.680777178961801\n",
      "Iteration: 39, test_1's l2: 0.6807686106441787\n",
      "Iteration: 40, test_1's l2: 0.6807531315291779\n",
      "Iteration: 41, test_1's l2: 0.6807686924635722\n",
      "Iteration: 42, test_1's l2: 0.6806659522803737\n",
      "Iteration: 43, test_1's l2: 0.6806702007572982\n",
      "Iteration: 44, test_1's l2: 0.6805489621572443\n",
      "Iteration: 45, test_1's l2: 0.6805390164993783\n",
      "Iteration: 46, test_1's l2: 0.680491476936876\n",
      "Iteration: 47, test_1's l2: 0.6803656180484114\n",
      "Iteration: 48, test_1's l2: 0.680300741545854\n",
      "Iteration: 49, test_1's l2: 0.6803159301705873\n",
      "Iteration: 50, test_1's l2: 0.680327807409923\n",
      "Iteration: 51, test_1's l2: 0.6803355920716727\n",
      "Iteration: 52, test_1's l2: 0.680296331739069\n",
      "Iteration: 53, test_1's l2: 0.6802652987109976\n",
      "Iteration: 54, test_1's l2: 0.6803016272798262\n",
      "Iteration: 55, test_1's l2: 0.6803248126219963\n",
      "Iteration: 56, test_1's l2: 0.6802672361891456\n",
      "Iteration: 57, test_1's l2: 0.6802281931385643\n",
      "Iteration: 58, test_1's l2: 0.6802298182447721\n",
      "Iteration: 59, test_1's l2: 0.6802417718202991\n",
      "Iteration: 60, test_1's l2: 0.6802434567435981\n",
      "Iteration: 61, test_1's l2: 0.6802191794317975\n",
      "Iteration: 62, test_1's l2: 0.6802289545019278\n",
      "Iteration: 63, test_1's l2: 0.6802716376054291\n",
      "Iteration: 64, test_1's l2: 0.6802731110656497\n",
      "Iteration: 65, test_1's l2: 0.6802803985773833\n",
      "Iteration: 66, test_1's l2: 0.6802722617097985\n",
      "Iteration: 67, test_1's l2: 0.6802479413225232\n",
      "Iteration: 68, test_1's l2: 0.6802707085298275\n",
      "Iteration: 69, test_1's l2: 0.6802845303602311\n",
      "Iteration: 70, test_1's l2: 0.6803191055022981\n",
      "Early stopping at iteration 71, the best iteration round is 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:27:40 Saving model... (this may take a while)\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:12 ( 1.34 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.35 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.35 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.33 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.36 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.36 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.90 μs/it)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 35243889, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 0.059330\n",
      "Iteration: 1, test_1's l2: 0.008794236296369195\n",
      "Iteration: 2, test_1's l2: 0.00853479936335471\n",
      "Iteration: 3, test_1's l2: 0.008321295467445602\n",
      "Iteration: 4, test_1's l2: 0.008145218744113353\n",
      "Iteration: 5, test_1's l2: 0.0080016921572548\n",
      "Iteration: 6, test_1's l2: 0.007886752781797712\n",
      "Iteration: 7, test_1's l2: 0.007790943462327465\n",
      "Iteration: 8, test_1's l2: 0.00771441833688922\n",
      "Iteration: 9, test_1's l2: 0.007652220024275177\n",
      "Iteration: 10, test_1's l2: 0.007599759029565946\n",
      "Iteration: 11, test_1's l2: 0.007556819344030666\n",
      "Iteration: 12, test_1's l2: 0.007522586645355317\n",
      "Iteration: 13, test_1's l2: 0.0074933177798574575\n",
      "Iteration: 14, test_1's l2: 0.007469517249955276\n",
      "Iteration: 15, test_1's l2: 0.007449835920618947\n",
      "Iteration: 16, test_1's l2: 0.007433842052175457\n",
      "Iteration: 17, test_1's l2: 0.00742005172032449\n",
      "Iteration: 18, test_1's l2: 0.007408880324981703\n",
      "Iteration: 19, test_1's l2: 0.007399643123145027\n",
      "Iteration: 20, test_1's l2: 0.007391989009660615\n",
      "Iteration: 21, test_1's l2: 0.0073860560763037665\n",
      "Iteration: 22, test_1's l2: 0.00738019504542442\n",
      "Iteration: 23, test_1's l2: 0.007375203376780023\n",
      "Iteration: 24, test_1's l2: 0.007370859427758923\n",
      "Iteration: 25, test_1's l2: 0.007367079925454459\n",
      "Iteration: 26, test_1's l2: 0.007363843096079717\n",
      "Iteration: 27, test_1's l2: 0.0073620354181685365\n",
      "Iteration: 28, test_1's l2: 0.007359248574933154\n",
      "Iteration: 29, test_1's l2: 0.00735710956982255\n",
      "Iteration: 30, test_1's l2: 0.007355646956507043\n",
      "Iteration: 31, test_1's l2: 0.0073530654506710575\n",
      "Iteration: 32, test_1's l2: 0.007351354665996191\n",
      "Iteration: 33, test_1's l2: 0.0073494883991491\n",
      "Iteration: 34, test_1's l2: 0.007348348007296149\n",
      "Iteration: 35, test_1's l2: 0.00734781660626939\n",
      "Iteration: 36, test_1's l2: 0.007346591614972695\n",
      "Iteration: 37, test_1's l2: 0.007346074697841459\n",
      "Iteration: 38, test_1's l2: 0.007345711480074152\n",
      "Iteration: 39, test_1's l2: 0.007345354925657782\n",
      "Iteration: 40, test_1's l2: 0.00734403175611442\n",
      "Iteration: 41, test_1's l2: 0.007343822805745625\n",
      "Iteration: 42, test_1's l2: 0.007342574437893336\n",
      "Iteration: 43, test_1's l2: 0.007342274588531372\n",
      "Iteration: 44, test_1's l2: 0.007341216398412548\n",
      "Iteration: 45, test_1's l2: 0.007341039894948273\n",
      "Iteration: 46, test_1's l2: 0.0073401820954504685\n",
      "Iteration: 47, test_1's l2: 0.00733902858635697\n",
      "Iteration: 48, test_1's l2: 0.007338134811887857\n",
      "Iteration: 49, test_1's l2: 0.007338005780287442\n",
      "Iteration: 50, test_1's l2: 0.007337156143891737\n",
      "Iteration: 51, test_1's l2: 0.00733667008106861\n",
      "Iteration: 52, test_1's l2: 0.00733601279887261\n",
      "Iteration: 53, test_1's l2: 0.007335033632621467\n",
      "Iteration: 54, test_1's l2: 0.007334926184063877\n",
      "Iteration: 55, test_1's l2: 0.007334352334303356\n",
      "Iteration: 56, test_1's l2: 0.007334150222106705\n",
      "Iteration: 57, test_1's l2: 0.007333691166070552\n",
      "Iteration: 58, test_1's l2: 0.0073334103272302735\n",
      "Iteration: 59, test_1's l2: 0.007333477919589525\n",
      "Iteration: 60, test_1's l2: 0.007333590050210206\n",
      "Iteration: 61, test_1's l2: 0.007333304306535267\n",
      "Iteration: 62, test_1's l2: 0.0073332480771889285\n",
      "Iteration: 63, test_1's l2: 0.007332589177699218\n",
      "Iteration: 64, test_1's l2: 0.007331915814616492\n",
      "Iteration: 65, test_1's l2: 0.007331955662470512\n",
      "Iteration: 66, test_1's l2: 0.007331623005213071\n",
      "Iteration: 67, test_1's l2: 0.007331408944108194\n",
      "Iteration: 68, test_1's l2: 0.007331129392551483\n",
      "Iteration: 69, test_1's l2: 0.0073309031943899795\n",
      "Iteration: 70, test_1's l2: 0.007330753410031713\n",
      "Iteration: 71, test_1's l2: 0.007330753191113639\n",
      "Iteration: 72, test_1's l2: 0.007330826690487192\n",
      "Iteration: 73, test_1's l2: 0.007330808818224596\n",
      "Iteration: 74, test_1's l2: 0.0073306712207459705\n",
      "Iteration: 75, test_1's l2: 0.007330698299275074\n",
      "Iteration: 76, test_1's l2: 0.007330696674421855\n",
      "Iteration: 77, test_1's l2: 0.007330266860539394\n",
      "Iteration: 78, test_1's l2: 0.0073302158959998075\n",
      "Iteration: 79, test_1's l2: 0.007330137167311188\n",
      "Iteration: 80, test_1's l2: 0.0073301380670768084\n",
      "Iteration: 81, test_1's l2: 0.007329989307688694\n",
      "Iteration: 82, test_1's l2: 0.007330025940699161\n",
      "Iteration: 83, test_1's l2: 0.007329794434321275\n",
      "Iteration: 84, test_1's l2: 0.007329853385978181\n",
      "Iteration: 85, test_1's l2: 0.007329669973350412\n",
      "Iteration: 86, test_1's l2: 0.007329457440126786\n",
      "Iteration: 87, test_1's l2: 0.007329613401355077\n",
      "Iteration: 88, test_1's l2: 0.007329593488675119\n",
      "Iteration: 89, test_1's l2: 0.00732945798989675\n",
      "Iteration: 90, test_1's l2: 0.007329614206884053\n",
      "Iteration: 91, test_1's l2: 0.007329737153443925\n",
      "Iteration: 92, test_1's l2: 0.007329946335046035\n",
      "Iteration: 93, test_1's l2: 0.007329969661656099\n",
      "Iteration: 94, test_1's l2: 0.007330051506088266\n",
      "Iteration: 95, test_1's l2: 0.007329690711832178\n",
      "Early stopping at iteration 96, the best iteration round is 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:33:33 Saving model... (this may take a while)\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:12 ( 1.34 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.44 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.42 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.33 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.38 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.91 μs/it)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"ErrorModel.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f9ca73-b22b-4565-b492-a0bdc4a193c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:38:17 Optimizing hyperparameters...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:02\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:24\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:11\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:02\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:06\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:44\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:27\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:02:33\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:49:27 Float32[0.0, 0.0] 0.22757058047384912\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:52:46 Float32[1.0, 0.0] 0.2152841241348019\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 22:54:30 Float32[1.0, 1.0] 0.23383675012013813\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:00:44 Float32[0.0, -1.0] 0.2152007012401424\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:09:22 Float32[-0.5, -2.0] 0.2220336339611775\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:11:43 Float32[1.0, -1.0] 0.21698197237908368\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:14:05 Float32[0.75, -0.75] 0.21679516980914249\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:17:04 Float32[0.25, -0.25] 0.22188625604712436\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:19:55 Float32[0.625, -0.625] 0.2156934121483814\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:23:31 Float32[0.375, -0.375] 0.21912643163328305\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:23:31 found minimum 0.2152007012401424 at point [0.0, -1.0] after 10 function calls (ended because MAXEVAL_REACHED) and saved model at\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:23:31 Training model...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:35:18 Trained model loss: 0.21375928719688567\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:35:18 Writing alpha...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220623 23:35:18 Wrote alpha!\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"BPR.ipynb\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
