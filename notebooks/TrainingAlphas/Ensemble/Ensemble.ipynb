{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df5351f-2171-4365-b750-8984d42c5fe5",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "* Runs all the ensemble alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b5a19a-24f5-4730-afe2-f451d27d18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf2b7dc-6ec0-44f9-9f88-1df84ba11362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:15:45 alphas: [\"ExplicitUserItemBiases\", \"ExplicitItemCF\", \"NeuralExplicitMatrixFactorization\", \"NeuralExplicitItemCFUntuned\", \"NeuralExplicitAutoencoderUntuned\"]\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:15:45 coefficients: Float32[1.0176827, 0.16860016, -0.029639833, 0.46684384, 0.5104295]\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:19\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.49 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:16:27 validation loss: 1.3584444563770786, β: [0.99999759584032]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.46 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 (10.92 ns/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:16:45 training loss: 1.0864266849786999, β: [0.99999759584032]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.58 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:16:47 test loss: 1.3681797487033311, β: [0.99999759584032]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.45 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.52 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.46 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.43 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.47 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.48 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.09 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.05 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:18:49 alphas: [\"NeuralImplicitUserItemBiases\", \"NeuralImplicitMatrixFactorization\", \"NeuralImplicitItemCFUntuned\", \"NeuralImplicitAutoencoderUntuned\"]\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:18:49 coefficients: Float32[7.9231773f-7, 0.01368896, 0.17084353, 0.8154653, 1.4856255f-6]\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:18\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:02 ( 1.45 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:20:03 validation loss: 6.039655723082712, β: Float32[2.226194f-6, 0.99999774]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:19 ( 1.44 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 9.11 ns/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:20:29 training loss: 5.991564846333298, β: Float32[2.226194f-6, 0.99999774]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:02 ( 1.54 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:20:33 test loss: 6.029067303328806, β: Float32[2.226194f-6, 0.99999774]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.44 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.49 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.45 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.43 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.47 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.48 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.05 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.06 μs/it)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"LinearModel.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517ce91a-bf09-446a-8be9-a7c5bbe0c0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:21:16 lib_lightgbm not found in system dirs, trying fallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 22508916, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.002064\n",
      "Iteration: 1, test_1's l2: 1.3544719658339723\n",
      "Iteration: 2, test_1's l2: 1.351140840822586\n",
      "Iteration: 3, test_1's l2: 1.3480371717354107\n",
      "Iteration: 4, test_1's l2: 1.3453704865678855\n",
      "Iteration: 5, test_1's l2: 1.3429125680916856\n",
      "Iteration: 6, test_1's l2: 1.341241822270122\n",
      "Iteration: 7, test_1's l2: 1.3399438370087424\n",
      "Iteration: 8, test_1's l2: 1.3384853823401723\n",
      "Iteration: 9, test_1's l2: 1.337122766856803\n",
      "Iteration: 10, test_1's l2: 1.336221890475985\n",
      "Iteration: 11, test_1's l2: 1.3351028773937468\n",
      "Iteration: 12, test_1's l2: 1.3342292319309301\n",
      "Iteration: 13, test_1's l2: 1.3333644221352408\n",
      "Iteration: 14, test_1's l2: 1.33288847734367\n",
      "Iteration: 15, test_1's l2: 1.332484131084191\n",
      "Iteration: 16, test_1's l2: 1.3318388016206069\n",
      "Iteration: 17, test_1's l2: 1.331332373870051\n",
      "Iteration: 18, test_1's l2: 1.3308525339718613\n",
      "Iteration: 19, test_1's l2: 1.330494941976565\n",
      "Iteration: 20, test_1's l2: 1.3301159193385357\n",
      "Iteration: 21, test_1's l2: 1.3297576778613203\n",
      "Iteration: 22, test_1's l2: 1.3295787799894196\n",
      "Iteration: 23, test_1's l2: 1.3295122982284753\n",
      "Iteration: 24, test_1's l2: 1.3292243904867926\n",
      "Iteration: 25, test_1's l2: 1.3289861167111252\n",
      "Iteration: 26, test_1's l2: 1.3287183593348344\n",
      "Iteration: 27, test_1's l2: 1.3284995602896272\n",
      "Iteration: 28, test_1's l2: 1.3284381561232204\n",
      "Iteration: 29, test_1's l2: 1.3281636563811985\n",
      "Iteration: 30, test_1's l2: 1.3280331897308877\n",
      "Iteration: 31, test_1's l2: 1.327790254979758\n",
      "Iteration: 32, test_1's l2: 1.3276647832272466\n",
      "Iteration: 33, test_1's l2: 1.3275269305379733\n",
      "Iteration: 34, test_1's l2: 1.3273869053384797\n",
      "Iteration: 35, test_1's l2: 1.3273554107577117\n",
      "Iteration: 36, test_1's l2: 1.3273023316168415\n",
      "Iteration: 37, test_1's l2: 1.327202568779752\n",
      "Iteration: 38, test_1's l2: 1.3271516181781757\n",
      "Iteration: 39, test_1's l2: 1.327074843334834\n",
      "Iteration: 40, test_1's l2: 1.3268890330041576\n",
      "Iteration: 41, test_1's l2: 1.3268029156274266\n",
      "Iteration: 42, test_1's l2: 1.3266979546514246\n",
      "Iteration: 43, test_1's l2: 1.3267005485288892\n",
      "Iteration: 44, test_1's l2: 1.3266554064019693\n",
      "Iteration: 45, test_1's l2: 1.3265536644413576\n",
      "Iteration: 46, test_1's l2: 1.326598903795966\n",
      "Iteration: 47, test_1's l2: 1.3264657526318666\n",
      "Iteration: 48, test_1's l2: 1.3264794749432112\n",
      "Iteration: 49, test_1's l2: 1.3264988915952445\n",
      "Iteration: 50, test_1's l2: 1.326484092513497\n",
      "Iteration: 51, test_1's l2: 1.3264084223725012\n",
      "Iteration: 52, test_1's l2: 1.3264409199801492\n",
      "Iteration: 53, test_1's l2: 1.326330456967661\n",
      "Iteration: 54, test_1's l2: 1.3262777935194285\n",
      "Iteration: 55, test_1's l2: 1.3263238082722484\n",
      "Iteration: 56, test_1's l2: 1.3263449139986823\n",
      "Iteration: 57, test_1's l2: 1.3263808320101333\n",
      "Iteration: 58, test_1's l2: 1.326388928856914\n",
      "Iteration: 59, test_1's l2: 1.3263012682254849\n",
      "Iteration: 60, test_1's l2: 1.3263865921886633\n",
      "Iteration: 61, test_1's l2: 1.3263860117992914\n",
      "Iteration: 62, test_1's l2: 1.3263620659816033\n",
      "Iteration: 63, test_1's l2: 1.3264100704372694\n",
      "Early stopping at iteration 64, the best iteration round is 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:22:10 Saving model...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:13\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:23:49 Average model value: 0.10452417\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:23:50 Average model absolute value: 0.22986889\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:08 ( 0.92 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.03 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.04 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:03 ( 0.92 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.09 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.06 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.94 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.92 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.45 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:24:55 validation loss: 1.2847330776975414, β: [1.0002854569883974]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.42 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 (10.98 ns/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:25:12 training loss: 1.0862933115022007, β: [1.0002854569883974]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.51 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:25:15 test loss: 1.3352324975532748, β: [1.0002854569883974]\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:13 ( 1.40 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.43 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.43 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:05 ( 1.41 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.47 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.45 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.04 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.06 μs/it)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"NonlinearModel.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765ef7e7-abf6-432e-b55d-2eea75753f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.11 μs/it)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.185414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 66963121, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 0.103163\n",
      "Iteration: 1, test_1's l2: 0.04135552856865351\n",
      "Iteration: 2, test_1's l2: 0.036330024227071545\n",
      "Iteration: 3, test_1's l2: 0.03361963876289821\n",
      "Iteration: 4, test_1's l2: 0.03006699012055358\n",
      "Iteration: 5, test_1's l2: 0.02718533525787156\n",
      "Iteration: 6, test_1's l2: 0.024852110167661208\n",
      "Iteration: 7, test_1's l2: 0.022964826774115002\n",
      "Iteration: 8, test_1's l2: 0.02142726490066883\n",
      "Iteration: 9, test_1's l2: 0.020185098013316683\n",
      "Iteration: 10, test_1's l2: 0.019548780778714466\n",
      "Iteration: 11, test_1's l2: 0.01865798418573119\n",
      "Iteration: 12, test_1's l2: 0.017937524538377665\n",
      "Iteration: 13, test_1's l2: 0.01735307233223437\n",
      "Iteration: 14, test_1's l2: 0.01687400925070912\n",
      "Iteration: 15, test_1's l2: 0.016486740748467052\n",
      "Iteration: 16, test_1's l2: 0.01631189418662833\n",
      "Iteration: 17, test_1's l2: 0.016169704578770566\n",
      "Iteration: 18, test_1's l2: 0.015916693622750392\n",
      "Iteration: 19, test_1's l2: 0.0157096171489072\n",
      "Iteration: 20, test_1's l2: 0.015542793332364559\n",
      "Iteration: 21, test_1's l2: 0.015407429388847628\n",
      "Iteration: 22, test_1's l2: 0.015296959018122745\n",
      "Iteration: 23, test_1's l2: 0.01520515835039405\n",
      "Iteration: 24, test_1's l2: 0.015129810049444923\n",
      "Iteration: 25, test_1's l2: 0.015069607434192035\n",
      "Iteration: 26, test_1's l2: 0.015020793549264172\n",
      "Iteration: 27, test_1's l2: 0.014979981683138138\n",
      "Iteration: 28, test_1's l2: 0.014948622181959569\n",
      "Iteration: 29, test_1's l2: 0.014920397200067137\n",
      "Iteration: 30, test_1's l2: 0.01491291049036157\n",
      "Iteration: 31, test_1's l2: 0.014891992872923518\n",
      "Iteration: 32, test_1's l2: 0.01487518614374118\n",
      "Iteration: 33, test_1's l2: 0.014869795373153959\n",
      "Iteration: 34, test_1's l2: 0.014857267528286958\n",
      "Iteration: 35, test_1's l2: 0.014847029231486102\n",
      "Iteration: 36, test_1's l2: 0.014838723216988222\n",
      "Iteration: 37, test_1's l2: 0.014830522943604695\n",
      "Iteration: 38, test_1's l2: 0.014829123814244193\n",
      "Iteration: 39, test_1's l2: 0.014828334565387472\n",
      "Iteration: 40, test_1's l2: 0.01482140120228088\n",
      "Iteration: 41, test_1's l2: 0.014817924850537583\n",
      "Iteration: 42, test_1's l2: 0.014816906818702616\n",
      "Iteration: 43, test_1's l2: 0.014816250069677935\n",
      "Iteration: 44, test_1's l2: 0.014812886083601264\n",
      "Iteration: 45, test_1's l2: 0.01481396822300124\n",
      "Iteration: 46, test_1's l2: 0.014812676581171956\n",
      "Iteration: 47, test_1's l2: 0.014810656888520457\n",
      "Iteration: 48, test_1's l2: 0.014810942374220728\n",
      "Iteration: 49, test_1's l2: 0.014811495811910201\n",
      "Iteration: 50, test_1's l2: 0.014808486417540987\n",
      "Iteration: 51, test_1's l2: 0.014808722877694653\n",
      "Iteration: 52, test_1's l2: 0.014808008556764427\n",
      "Iteration: 53, test_1's l2: 0.014806971229098945\n",
      "Iteration: 54, test_1's l2: 0.014806880688689206\n",
      "Iteration: 55, test_1's l2: 0.01480664237680107\n",
      "Iteration: 56, test_1's l2: 0.014806133408564261\n",
      "Iteration: 57, test_1's l2: 0.014806552479776977\n",
      "Iteration: 58, test_1's l2: 0.01480638187383505\n",
      "Iteration: 59, test_1's l2: 0.014805977472212859\n",
      "Iteration: 60, test_1's l2: 0.014806310578369146\n",
      "Iteration: 61, test_1's l2: 0.014808313590896932\n",
      "Iteration: 62, test_1's l2: 0.01480875698392807\n",
      "Iteration: 63, test_1's l2: 0.014810414301252878\n",
      "Iteration: 64, test_1's l2: 0.01481144864615755\n",
      "Iteration: 65, test_1's l2: 0.014811181546488236\n",
      "Iteration: 66, test_1's l2: 0.014812477264507016\n",
      "Iteration: 67, test_1's l2: 0.014813043475388686\n",
      "Iteration: 68, test_1's l2: 0.014813109305562377\n",
      "Early stopping at iteration 69, the best iteration round is 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:27:55 Saving model...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:35\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:29:00 Average model value: 0.041791685\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:29:00 Average model absolute value: 0.041816685\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:07 ( 0.80 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.94 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.96 μs/it)\u001b[39m\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:03 ( 0.81 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.00 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.95 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.86 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.85 μs/it)\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 22508916, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 0.815052\n",
      "Iteration: 1, test_1's l2: 0.6346735477570526\n",
      "Iteration: 2, test_1's l2: 0.6260494329334274\n",
      "Iteration: 3, test_1's l2: 0.619655673100819\n",
      "Iteration: 4, test_1's l2: 0.6137124562651883\n",
      "Iteration: 5, test_1's l2: 0.6087927379247003\n",
      "Iteration: 6, test_1's l2: 0.6046753282400935\n",
      "Iteration: 7, test_1's l2: 0.6013110113924363\n",
      "Iteration: 8, test_1's l2: 0.5984722124693864\n",
      "Iteration: 9, test_1's l2: 0.5961243546413566\n",
      "Iteration: 10, test_1's l2: 0.5941575613878124\n",
      "Iteration: 11, test_1's l2: 0.5924650600525428\n",
      "Iteration: 12, test_1's l2: 0.5910404116042104\n",
      "Iteration: 13, test_1's l2: 0.5898187225531476\n",
      "Iteration: 14, test_1's l2: 0.5885786745531324\n",
      "Iteration: 15, test_1's l2: 0.5877039929326952\n",
      "Iteration: 16, test_1's l2: 0.5868635982699895\n",
      "Iteration: 17, test_1's l2: 0.586071769206491\n",
      "Iteration: 18, test_1's l2: 0.5852583953558367\n",
      "Iteration: 19, test_1's l2: 0.584672363890006\n",
      "Iteration: 20, test_1's l2: 0.5841210999329892\n",
      "Iteration: 21, test_1's l2: 0.5836116144231732\n",
      "Iteration: 22, test_1's l2: 0.5831768617254053\n",
      "Iteration: 23, test_1's l2: 0.5828240513169188\n",
      "Iteration: 24, test_1's l2: 0.5824977800551605\n",
      "Iteration: 25, test_1's l2: 0.5820874348226199\n",
      "Iteration: 26, test_1's l2: 0.5818535383682942\n",
      "Iteration: 27, test_1's l2: 0.5816481110614723\n",
      "Iteration: 28, test_1's l2: 0.5814731781368678\n",
      "Iteration: 29, test_1's l2: 0.5812384830844285\n",
      "Iteration: 30, test_1's l2: 0.5808563600525881\n",
      "Iteration: 31, test_1's l2: 0.580612445379717\n",
      "Iteration: 32, test_1's l2: 0.5805124605295147\n",
      "Iteration: 33, test_1's l2: 0.5802282943168879\n",
      "Iteration: 34, test_1's l2: 0.5798948643351239\n",
      "Iteration: 35, test_1's l2: 0.5797807014874542\n",
      "Iteration: 36, test_1's l2: 0.5794591252902617\n",
      "Iteration: 37, test_1's l2: 0.5791523927739787\n",
      "Iteration: 38, test_1's l2: 0.5789982982116579\n",
      "Iteration: 39, test_1's l2: 0.5787604656008728\n",
      "Iteration: 40, test_1's l2: 0.578615670375864\n",
      "Iteration: 41, test_1's l2: 0.5785104182851429\n",
      "Iteration: 42, test_1's l2: 0.5782521553349523\n",
      "Iteration: 43, test_1's l2: 0.5781234197518549\n",
      "Iteration: 44, test_1's l2: 0.5779053198013827\n",
      "Iteration: 45, test_1's l2: 0.5778088902604643\n",
      "Iteration: 46, test_1's l2: 0.5777364306809027\n",
      "Iteration: 47, test_1's l2: 0.5776754798152295\n",
      "Iteration: 48, test_1's l2: 0.577582036486386\n",
      "Iteration: 49, test_1's l2: 0.5774698724298115\n",
      "Iteration: 50, test_1's l2: 0.5772988185322036\n",
      "Iteration: 51, test_1's l2: 0.5771821639574031\n",
      "Iteration: 52, test_1's l2: 0.5771650445797701\n",
      "Iteration: 53, test_1's l2: 0.5769839731176776\n",
      "Iteration: 54, test_1's l2: 0.5768427522635297\n",
      "Iteration: 55, test_1's l2: 0.5768249905398627\n",
      "Iteration: 56, test_1's l2: 0.5767739770516397\n",
      "Iteration: 57, test_1's l2: 0.5767784948679833\n",
      "Iteration: 58, test_1's l2: 0.5765687562929852\n",
      "Iteration: 59, test_1's l2: 0.5765723581023252\n",
      "Iteration: 60, test_1's l2: 0.5763730385077356\n",
      "Iteration: 61, test_1's l2: 0.5762761444098572\n",
      "Iteration: 62, test_1's l2: 0.576182193261131\n",
      "Iteration: 63, test_1's l2: 0.5761846753158525\n",
      "Iteration: 64, test_1's l2: 0.5761664685299469\n",
      "Iteration: 65, test_1's l2: 0.5761198201367168\n",
      "Iteration: 66, test_1's l2: 0.5760524314154645\n",
      "Iteration: 67, test_1's l2: 0.5759321076872559\n",
      "Iteration: 68, test_1's l2: 0.5758754912522612\n",
      "Iteration: 69, test_1's l2: 0.5758558306529994\n",
      "Iteration: 70, test_1's l2: 0.5758295354812436\n",
      "Iteration: 71, test_1's l2: 0.575838352588242\n",
      "Iteration: 72, test_1's l2: 0.5758661843677715\n",
      "Iteration: 73, test_1's l2: 0.5758656459829063\n",
      "Iteration: 74, test_1's l2: 0.5758886636822618\n",
      "Iteration: 75, test_1's l2: 0.575877103627999\n",
      "Iteration: 76, test_1's l2: 0.5758928683372873\n",
      "Iteration: 77, test_1's l2: 0.5758859645526822\n",
      "Iteration: 78, test_1's l2: 0.5759083583415241\n",
      "Iteration: 79, test_1's l2: 0.5759121233904952\n",
      "Early stopping at iteration 80, the best iteration round is 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:31:00 Saving model...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:32\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:33:01 Average model value: 1.1114173\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:33:02 Average model absolute value: 1.1114173\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:08 ( 0.93 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.01 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 1.04 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:03 ( 0.93 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.05 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.07 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.93 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.92 μs/it)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"ErrorModel.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f9ca73-b22b-4565-b492-a0bdc4a193c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:33:34 Optimizing hyperparameters...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:02\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:24\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:11\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:02\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:01\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:04\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:44\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:27\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:02:33\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:41:56 loss 0.180360925131274\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:42:30 loss 0.1786584643733497\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:42:55 loss 0.17994100909914804\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:42:55 Float32[0.0, 0.0] 0.1786584643733497\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:43:30 loss 0.17940429824898713\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:44:04 loss 0.18077551678345816\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:44:04 Float32[1.0, 0.0] 0.17940429824898713\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:44:39 loss 0.1833109984484226\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:45:04 loss 0.18470780270385634\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:45:04 Float32[0.0, 1.0] 0.1833109984484226\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:45:39 loss 0.179813195785816\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:46:13 loss 0.18015128158254923\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:46:13 Float32[1.0, -1.0] 0.179813195785816\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:46:48 loss 0.18112541112932157\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:47:14 loss 0.17963119824172774\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:47:48 loss 0.18121826216460443\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:47:48 Float32[0.75, -0.5] 0.17963119824172774\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:48:22 loss 0.18033842544585343\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:48:57 loss 0.1806169845661277\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:48:57 Float32[0.25, 0.5] 0.18033842544585343\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:49:23 loss 0.1802762830956663\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:49:57 loss 0.1787676386835889\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:50:32 loss 0.17844452981857117\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:51:06 loss 0.17857793579457706\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:51:06 Float32[0.625, -0.25] 0.17844452981857117\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:51:32 loss 0.18345491621279347\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:52:07 loss 0.1799282298891646\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:52:42 loss 0.17886841389895475\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:53:16 loss 0.17811595251406423\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:53:43 loss 0.17863841028831356\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:53:43 Float32[-0.375, -0.25] 0.17811595251406423\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:54:17 loss 0.2020850623162096\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:54:52 loss 0.19012233397390474\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:55:26 loss 0.18526581109626838\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:55:53 loss 0.1820205970390119\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:56:27 loss 0.18129743954924896\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:57:02 loss 0.18039927855049695\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:57:37 loss 0.18013693384969517\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:58:03 loss 0.18029569522788597\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:58:03 Float32[-1.0625, -0.375] 0.18013693384969517\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:58:38 loss 0.1797299356572541\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:59:13 loss 0.17881799732928275\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:59:48 loss 0.17981716110790869\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:59:48 Float32[0.25, -0.5] 0.17881799732928275\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:59:48 found minimum 0.17811595251406423 at point [-0.375, -0.25] after 10 function calls (ended because MAXEVAL_REACHED) and saved model at\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:59:48 Training model...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:59:52 loss 0.2190718918431709\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 10:59:56 loss 0.20677619916746995\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:00 loss 0.19953964139685815\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:04 loss 0.19442595318887243\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:08 loss 0.19117134576224237\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:12 loss 0.18807232956061584\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:16 loss 0.18625800989453004\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:20 loss 0.1857749039229246\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:24 loss 0.18523646207046437\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:36 loss 0.18251641617995845\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:40 loss 0.183262907252452\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:44 loss 0.18107522433319437\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:48 loss 0.1811421005478988\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:52 loss 0.18173926485254827\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:00:56 loss 0.1808140582349875\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:00 loss 0.18047547532629848\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:04 loss 0.18022627385999967\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:08 loss 0.17998648528466357\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:12 loss 0.18034892494118202\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:16 loss 0.18095088006752205\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:20 loss 0.18140161590547393\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:23 loss 0.1798824266256903\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:36 loss 0.17887989626947956\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:40 loss 0.1797339037561003\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:44 loss 0.17921949072025353\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:48 loss 0.1789325207797863\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:52 loss 0.17841022073525567\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:56 loss 0.17969356445221127\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:01:59 loss 0.17929084555736716\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:03 loss 0.1790088437966966\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:07 loss 0.18028222630942195\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:11 loss 0.17925042508121528\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:15 loss 0.17877324471290226\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:19 loss 0.17861202376624497\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:23 loss 0.17934803071673167\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:40 loss 0.17833468339345177\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:44 loss 0.1789297597031016\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:48 loss 0.17930579868994403\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:52 loss 0.17855224148231558\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:56 loss 0.1785704749332469\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:02:59 loss 0.17851984418819014\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:03:03 loss 0.17826677385544787\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:03:07 loss 0.17829006332489555\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:03:11 loss 0.17811865312047653\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:03:15 loss 0.17819948496120644\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:03:19 loss 0.17935617023573783\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:03:23 loss 0.17927779948113326\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:03:27 loss 0.1781008371381389\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:03:44 loss 0.178150056376182\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:03:48 loss 0.17789265621904607\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:03:52 loss 0.1783200554895383\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:03:56 loss 0.17798182013596017\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:00 loss 0.17805786409476082\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:04 loss 0.18030583141255677\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:08 loss 0.17814628182942352\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:12 loss 0.1794108275151262\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:16 loss 0.17857873304323715\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:20 loss 0.1786530126168655\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:24 loss 0.17773537061790934\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:28 loss 0.17766298166494107\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:32 loss 0.17880271479167026\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:36 loss 0.17851539083761073\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:48 loss 0.17760744903367315\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:52 loss 0.17804460697963703\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:04:56 loss 0.17843578128556584\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:00 loss 0.1780334578363997\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:04 loss 0.17812972267656135\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:08 loss 0.17798562139351384\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:12 loss 0.17809879377917134\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:16 loss 0.17773464493529387\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:20 loss 0.17837911101724913\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:24 loss 0.17781179444109077\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:28 loss 0.17801489285571043\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:32 loss 0.17793205287131847\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:32 Trained model loss: 0.17760744903367315\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:32 Writing alpha...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220704 11:05:32 Wrote alpha!\n"
     ]
    }
   ],
   "source": [
    "@nbinclude(\"BPR.ipynb\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
