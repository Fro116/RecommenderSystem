{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df5351f-2171-4365-b750-8984d42c5fe5",
   "metadata": {},
   "source": [
    "# Bayesian Personalized Ranking\n",
    "* Creates a model for pairwise classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b5a19a-24f5-4730-afe2-f451d27d18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "\n",
    "import CUDA\n",
    "import SparseArrays: sparse\n",
    "import NBInclude: @nbinclude\n",
    "import NLopt\n",
    "import Random\n",
    "import Setfield: @set\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"EnsembleInputs.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b7f140-5c65-48cd-bbbb-2bc37cfd0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# support both gpu and cpu training\n",
    "\n",
    "function device(x)\n",
    "    gpu(x)\n",
    "end\n",
    "\n",
    "if !CUDA.functional()\n",
    "    LinearAlgebra.BLAS.set_num_threads(Threads.nthreads())\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c844803-8039-40f4-9e6d-d6c32de910ec",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ced8d21-51e8-4193-b66c-2aad73ee9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_kw struct Hyperparams\n",
    "    batch_size::Int\n",
    "    features::Vector{String}\n",
    "    l2penalty::Float32\n",
    "    learning_rate::Float32\n",
    "    seed::UInt64\n",
    "end\n",
    "\n",
    "function to_dict(x::Hyperparams)\n",
    "    Dict(string(key) => getfield(x, key) for key âˆˆ fieldnames(Hyperparams))\n",
    "end\n",
    "\n",
    "function Base.string(x::Hyperparams)\n",
    "    fields = [x for x in fieldnames(Hyperparams)]\n",
    "    max_field_size = maximum(length(string(k)) for k in fields)\n",
    "    ret = \"Hyperparameters:\\n\"\n",
    "    for f in fields\n",
    "        ret *= \"$(rpad(string(f), max_field_size)) => $(getfield(x, f))\\n\"\n",
    "    end\n",
    "    ret\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bfb8c5-fc16-4d8e-b236-94228435451e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2f9f4d-180a-4f2e-b21e-b2e2e15d00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_model(features)\n",
    "    num_inputs = length(features) * 2\n",
    "    Chain(Dense(num_inputs => 8, relu), Dense(8 => 1))\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7086c3c-af84-49c2-a579-45d092183d2a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8ccb15-51c2-4a75-afcc-ef3930dacd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_priorities!(user_priorities, split, priority)\n",
    "    a = get_raw_split(split)\n",
    "    @showprogress for i = 1:length(a.rating)\n",
    "        user_priorities[a.user[i]][a.item[i]] = priority(a.rating[i])\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_user_priorities()\n",
    "    user_priorities = Dict{Int32,Dict{Int32,Tuple{Int32,Float32}}}()\n",
    "    @showprogress for i = 1:num_users()\n",
    "        user_priorities[i] = Dict{Int32,Tuple{Int32,Float32}}()\n",
    "    end\n",
    "    add_priorities!(user_priorities, \"explicit_test\", r -> (1, r))\n",
    "    add_priorities!(user_priorities, \"implicit_test\", r -> (1, NaN32))\n",
    "    add_priorities!(user_priorities, \"negative_test\", r -> (0, NaN32))\n",
    "    user_priorities\n",
    "end\n",
    "\n",
    "function training_test_split(user_priorities; p = 0.9)\n",
    "    training = Dict{Int32,Dict{Int32,Tuple{Int32,Float32}}}()\n",
    "    test = Dict{Int32,Dict{Int32,Tuple{Int32,Float32}}}()\n",
    "    cutoff = num_users() * p\n",
    "    @showprogress for k in keys(user_priorities)\n",
    "        if k < cutoff\n",
    "            training[k] = user_priorities[k]\n",
    "        else\n",
    "            test[k] = user_priorities[k]\n",
    "        end\n",
    "    end\n",
    "    training, test\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8edac26-7aa7-47d4-8794-b6ace6fad0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_features!(user_features, alphas, split)\n",
    "    as = [read_alpha(a, split, false).rating for a in alphas]\n",
    "    df = get_raw_split(split)\n",
    "    @showprogress for i = 1:length(df.rating)\n",
    "        user_features[df.user[i]][df.item[i]] = [a[i] for a in as]\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_user_features(features)\n",
    "    user_features = Dict{Int32,Dict{Int32,Vector{Float32}}}()\n",
    "    @showprogress for i = 1:num_users()\n",
    "        user_features[i] = Dict{Int32,Vector{Float32}}()\n",
    "    end\n",
    "    add_features!(user_features, features, \"explicit_test\")\n",
    "    add_features!(user_features, features, \"implicit_test\")\n",
    "    add_features!(user_features, features, \"negative_test\")\n",
    "    user_features\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06f21d-870c-4834-9863-82f38243662b",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ae753b-940d-4d5b-8542-1d963e0bceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "function compare(x, y)\n",
    "    if isnan(x) || isnan(y)\n",
    "        return NaN\n",
    "    elseif x == y\n",
    "        return 0\n",
    "    elseif x > y\n",
    "        return 1\n",
    "    else\n",
    "        return -1\n",
    "    end\n",
    "end\n",
    "\n",
    "function compare(x::Tuple, y::Tuple)\n",
    "    results = compare.(x, y)\n",
    "    for r in results\n",
    "        if r == 0\n",
    "            continue\n",
    "        else\n",
    "            return r\n",
    "        end\n",
    "    end\n",
    "    0\n",
    "end\n",
    "\n",
    "function sample(user_priorities, user_features)\n",
    "    while true\n",
    "        u, items = rand(user_priorities, 1)[1]\n",
    "        if length(items) > 1\n",
    "            i, j = rand(items, 2)\n",
    "            sign = compare(i[2], j[2])\n",
    "            if !isnan(sign)\n",
    "                features = user_features[u]\n",
    "                x = vcat(features[i[1]], features[j[1]])\n",
    "                return x, [Float32((sign + 1) / 2)]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_batch(user_priorities, user_features, batch_size)\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for i = 1:batch_size\n",
    "        x, y = sample(user_priorities, user_features)\n",
    "        push!(Xs, x)\n",
    "        push!(ys, y)\n",
    "    end\n",
    "    [(Flux.batch(Xs) |> device, Flux.batch(ys) |> device)]\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2afe598-21a3-47b7-8613-b574e25d711b",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ef580ec-b209-42a2-b28c-4d4066a01c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "function model_loss(m, x, y)\n",
    "    Flux.logitbinarycrossentropy(m(x), y)\n",
    "end\n",
    "\n",
    "function split_loss(m, iters, batches::Channel)\n",
    "    losses = 0.0\n",
    "    for _ = 1:iters\n",
    "        losses += model_loss(m, take!(batches)[1]...)\n",
    "    end\n",
    "    losses / iters\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb55068-feaf-4f29-b6e6-e2ea18451efa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cded958-06da-4212-89a4-cccb199e86b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize function get_data(features)\n",
    "    training, test = training_test_split(get_user_priorities())\n",
    "    user_features = get_user_features(features)\n",
    "    training, test, user_features\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad3244b-38e1-4bae-927d-428018c9d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_batches(user_priorities, user_features, batch_size, c::Channel)\n",
    "    while true\n",
    "        try\n",
    "            put!(c, get_batch(user_priorities, user_features, batch_size))\n",
    "        catch e\n",
    "            if isa(e, InvalidStateException)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "000bad20-6037-4d47-8d18-18d72cd35d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains a model with the given hyperparams and returns its validation loss\n",
    "function train_model(\n",
    "    hyp;\n",
    "    max_checkpoints = 100,\n",
    "    epochs_per_checkpoint = 10,\n",
    "    patience = 0,\n",
    "    verbose = true,\n",
    ")\n",
    "    opt = ADAMW(hyp.learning_rate, (0.9, 0.999), hyp.l2penalty)\n",
    "    Random.seed!(hyp.seed)\n",
    "    m = build_model(hyp.features) |> device\n",
    "    best_model = m |> cpu\n",
    "    ps = Flux.params(m)\n",
    "    stopper = early_stopper(max_iters = max_checkpoints, patience = patience)\n",
    "    training, test, user_features = get_data(hyp.features)\n",
    "    batchloss(x, y) = model_loss(m, x, y)\n",
    "    epoch_size = Int(round(num_users() / hyp.batch_size))\n",
    "\n",
    "    training_batches = Channel(64)\n",
    "    test_batches = Channel(64)\n",
    "    for _ = 1:max(Threads.nthreads() / 2 - 1, 1)\n",
    "        Threads.@spawn generate_batches(\n",
    "            training,\n",
    "            user_features,\n",
    "            hyp.batch_size,\n",
    "            training_batches,\n",
    "        )\n",
    "        Threads.@spawn generate_batches(test, user_features, hyp.batch_size, test_batches)\n",
    "    end\n",
    "\n",
    "    losses = []\n",
    "    loss = Inf\n",
    "    while (!stop!(stopper, loss))\n",
    "        for i = 1:epochs_per_checkpoint\n",
    "            for _ = 1:epoch_size\n",
    "                Flux.train!(batchloss, ps, take!(training_batches), opt)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        loss = split_loss(m, epoch_size, test_batches)\n",
    "        push!(losses, loss)\n",
    "        if loss == minimum(losses)\n",
    "            best_model = m |> cpu\n",
    "        end\n",
    "        if verbose\n",
    "            @info \"loss $loss\"\n",
    "        end\n",
    "    end\n",
    "\n",
    "    close(training_batches)\n",
    "    close(test_batches)\n",
    "    best_model, minimum(losses)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4db189-a655-43dd-b93a-2d449e6a6354",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dadbdc6d-d986-4870-a4e6-6467ac3aea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_hyperparams(hyp, Î»)\n",
    "    hyp = @set hyp.learning_rate = 10^(Î»[1] - 3)\n",
    "    hyp = @set hyp.l2penalty = 10^(Î»[2] - 5)\n",
    "    hyp\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aebe789-d898-4872-b5e2-376ea1074cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_hyperparams(hyp; max_evals)\n",
    "    function nlopt_loss(Î», grad)\n",
    "        # nlopt internally converts to float64 because it calls a c library\n",
    "        Î» = convert.(Float32, Î»)\n",
    "        _, loss = train_model(create_hyperparams(hyp, Î»))\n",
    "        @info \"$Î» $loss\"\n",
    "        loss\n",
    "    end\n",
    "    opt = NLopt.Opt(:LN_NELDERMEAD, 2)\n",
    "    opt.initial_step = 1\n",
    "    opt.maxeval = max_evals\n",
    "    opt.min_objective = nlopt_loss\n",
    "    minf, Î», ret = NLopt.optimize(opt, zeros(Float32, 2))\n",
    "    numevals = opt.numevals\n",
    "\n",
    "    @info (\n",
    "        \"found minimum $minf at point $Î» after $numevals function calls \" *\n",
    "        \"(ended because $ret) and saved model at\"\n",
    "    )\n",
    "    Î»\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd36b5c4-e382-4442-acb7-41e623521076",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd45d6f1-f2a5-4ea9-830e-c93b4c6c3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_alpha(hyp, outdir; tune_hyperparams = true)\n",
    "    set_logging_outdir(outdir)\n",
    "\n",
    "    if tune_hyperparams\n",
    "        @info \"Optimizing hyperparameters...\"\n",
    "        Î» = optimize_hyperparams(hyp; max_evals = 10)\n",
    "    else\n",
    "        Î» = zeros(2)\n",
    "    end\n",
    "    hyp = create_hyperparams(hyp, Î»)\n",
    "\n",
    "    @info \"Training model...\"\n",
    "    m, validation_loss =\n",
    "        train_model(hyp; max_checkpoints = 1000, epochs_per_checkpoint = 1, patience = 10)\n",
    "    @info \"Trained model loss: $validation_loss\"\n",
    "\n",
    "    @info \"Writing alpha...\"\n",
    "    write_params(Dict(\"m\" => m, \"Î»\" => Î», \"hyp\" => hyp), outdir)\n",
    "    @info \"Wrote alpha!\"\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b097d37-efdb-4f18-8ebe-bf2fa8d98bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "restriced_alphas = [\n",
    "    \"Explicit\"\n",
    "    \"LinearImplicit\"\n",
    "    \"ErrorExplicit\"\n",
    "    \"ErrorImplicit\"\n",
    "    \"ExplicitUserItemBiases\"\n",
    "    \"NeuralImplicitUserItemBiases\"\n",
    "]\n",
    "ensemble_alphas = [\n",
    "    \"Explicit\"\n",
    "    \"LinearExplicit\"\n",
    "    \"LinearImplicit\"\n",
    "    \"ErrorExplicit\"\n",
    "    \"ErrorImplicit\"\n",
    "]\n",
    "all_alphas = [\n",
    "    ensemble_alphas\n",
    "    explicit_raw_alphas\n",
    "    implicit_raw_alphas\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcc5b860-1a37-4eff-9801-b4760f74b85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:02:17 Optimizing hyperparameters...\n",
      "\u001b[32mProgress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:02\u001b[39m\n",
      "\u001b[32mProgress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:23\u001b[39m\n",
      "\u001b[32mProgress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:11\u001b[39m\n",
      "\u001b[32mProgress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:01:00\u001b[39m\n",
      "\u001b[32mProgress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:01\u001b[39m\n",
      "\u001b[32mProgress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:04\u001b[39m\n",
      "\u001b[32mProgress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:44\u001b[39m\n",
      "\u001b[32mProgress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:00:27\u001b[39m\n",
      "\u001b[32mProgress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:02:32\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:10:28 loss 0.18645743607507925\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:11:01 loss 0.18450483933838752\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:11:26 loss 0.1831562238756282\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:11:59 loss 0.18378476909267277\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:11:59 Float32[0.0, 0.0] 0.1831562238756282\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:12:32 loss 0.18299275615313332\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:13:05 loss 0.18285738632099802\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:13:30 loss 0.18337821033553148\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:13:30 Float32[1.0, 0.0] 0.18285738632099802\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:14:04 loss 0.18685059153282702\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:14:37 loss 0.18343302334919825\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:15:11 loss 0.1856140611150138\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:15:11 Float32[1.0, 1.0] 0.18343302334919825\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:15:37 loss 0.1862637795555686\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:16:10 loss 0.18408375591185802\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:16:44 loss 0.1833953754558333\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:17:19 loss 0.18228829734418042\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:17:44 loss 0.1819123992579067\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:18:18 loss 0.18326183603827076\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:18:18 Float32[0.0, -1.0] 0.1819123992579067\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:18:52 loss 0.20121324458062895\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:19:26 loss 0.1894552616500171\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:19:52 loss 0.185826589542036\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:20:26 loss 0.1834754343360651\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:21:00 loss 0.1830114432660106\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:21:34 loss 0.18267876554529247\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:22:00 loss 0.1827973493233306\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:22:00 Float32[-0.5, -2.0] 0.18267876554529247\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:22:34 loss 0.18469026856200285\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:23:09 loss 0.18276675335531728\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:23:43 loss 0.18241283760143648\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:24:09 loss 0.18411980042678822\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:24:09 Float32[1.0, -1.0] 0.18241283760143648\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:24:44 loss 0.1862753009130621\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:25:18 loss 0.18308479434159602\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:25:53 loss 0.18353540578773453\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:25:53 Float32[0.0, -2.0] 0.18308479434159602\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:26:20 loss 0.18309284386905533\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:26:54 loss 0.18426273012219713\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:26:54 Float32[0.75, -0.5] 0.18309284386905533\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:27:28 loss 0.18500254775096406\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:28:03 loss 0.18253544313599146\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:28:29 loss 0.18219611320190723\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:29:03 loss 0.18142134540263594\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:29:37 loss 0.1810995103536305\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:30:12 loss 0.1810905312938719\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:30:38 loss 0.18046632260702458\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:31:12 loss 0.1807975210994201\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:31:12 Float32[0.5, -1.0] 0.18046632260702458\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:31:47 loss 0.18493147632258203\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:22 loss 0.184956548361003\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:22 Float32[0.5, -0.5] 0.18493147632258203\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:23 found minimum 0.18046632260702458 at point [0.5, -1.0] after 10 function calls (ended because MAXEVAL_REACHED) and saved model at\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:23 Training model...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:26 loss 0.21319837094769753\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:30 loss 0.2019517269338225\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:34 loss 0.19659011080012326\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:38 loss 0.1888031743576463\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:42 loss 0.18943205628697263\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:46 loss 0.18702394414941845\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:50 loss 0.1837015078174353\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:54 loss 0.18537437232293438\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:32:58 loss 0.1845484718382471\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:01 loss 0.18246708072297577\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:14 loss 0.18293166488847476\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:18 loss 0.18361085456571505\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:22 loss 0.1821450260799815\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:26 loss 0.1814944227150547\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:29 loss 0.1841162464689282\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:33 loss 0.18443464700544343\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:37 loss 0.1820228971970931\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:41 loss 0.1822668392785942\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:45 loss 0.18247639023888926\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:49 loss 0.18221487833368963\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:53 loss 0.18245148705406397\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:33:57 loss 0.18364591702610894\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:01 loss 0.18129683635294191\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:13 loss 0.18203043335678443\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:17 loss 0.18287058102099774\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:21 loss 0.1816148808466988\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:25 loss 0.18170256334311644\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:29 loss 0.18178885771570813\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:33 loss 0.18100554796490118\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:36 loss 0.1810662544805659\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:40 loss 0.18222985414683437\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:44 loss 0.18204527245135363\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:48 loss 0.18157777853481546\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:52 loss 0.18136277743125132\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:34:56 loss 0.18147090619822712\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:35:00 loss 0.18117900050527327\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:35:12 loss 0.18259639844090386\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:35:16 loss 0.18132682500448555\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:35:20 loss 0.18358987298316662\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:35:24 loss 0.18170418193748608\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:35:24 Trained model loss: 0.18100554796490118\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:35:24 Writing alpha...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220702 20:35:25 Wrote alpha!\n"
     ]
    }
   ],
   "source": [
    "train_alpha(\n",
    "    Hyperparams(\n",
    "        batch_size = 1024,\n",
    "        features = restriced_alphas,\n",
    "        learning_rate = NaN,\n",
    "        l2penalty = NaN,\n",
    "        seed = 20220609,\n",
    "    ),\n",
    "    \"BPR\";\n",
    "    tune_hyperparams = true,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "246a41e0-e7d9-41eb-aa90-711d66cfb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ Info: 20220630 21:51:43 Trained model loss: 0.18621169700122517\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71571dc-810f-4448-bbee-a8c4280cc12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
