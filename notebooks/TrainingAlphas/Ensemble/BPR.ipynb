{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df5351f-2171-4365-b750-8984d42c5fe5",
   "metadata": {},
   "source": [
    "# Bayesian Personalized Ranking\n",
    "* Creates a model for pairwise classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b5a19a-24f5-4730-afe2-f451d27d18e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Flux\n",
    "\n",
    "import CUDA\n",
    "import NLopt\n",
    "import Random\n",
    "import NBInclude: @nbinclude\n",
    "import Setfield: @set\n",
    "@nbinclude(\"BPRBase.ipynb\")\n",
    "@nbinclude(\"EnsembleInputs.ipynb\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c844803-8039-40f4-9e6d-d6c32de910ec",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ced8d21-51e8-4193-b66c-2aad73ee9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_kw struct Hyperparams\n",
    "    allow_ptw::Bool\n",
    "    batch_size::Int\n",
    "    features::Vector{String}\n",
    "    l2penalty::Float32\n",
    "    learning_rate::Float32\n",
    "    seed::UInt64\n",
    "end\n",
    "\n",
    "function to_dict(x::Hyperparams)\n",
    "    Dict(string(key) => getfield(x, key) for key ∈ fieldnames(Hyperparams))\n",
    "end\n",
    "\n",
    "function Base.string(x::Hyperparams)\n",
    "    fields = [x for x in fieldnames(Hyperparams)]\n",
    "    max_field_size = maximum(length(string(k)) for k in fields)\n",
    "    ret = \"Hyperparameters:\\n\"\n",
    "    for f in fields\n",
    "        ret *= \"$(rpad(string(f), max_field_size)) => $(getfield(x, f))\\n\"\n",
    "    end\n",
    "    ret\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bfb8c5-fc16-4d8e-b236-94228435451e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2f9f4d-180a-4f2e-b21e-b2e2e15d00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_model(features::Vector{String})\n",
    "    num_inputs = length(features) * 2\n",
    "    Chain(Dense(num_inputs => 256, relu), Dense(256 => 1))\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2afe598-21a3-47b7-8613-b574e25d711b",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef580ec-b209-42a2-b28c-4d4066a01c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "function model_loss(m, x, y)\n",
    "    Flux.logitbinarycrossentropy(m(x), y)\n",
    "end\n",
    "\n",
    "function split_loss(m, iters::Int, batches::Channel)\n",
    "    losses = 0.0\n",
    "    @showprogress for _ = 1:iters\n",
    "        losses += model_loss(m, take!(batches)[1]...)\n",
    "    end\n",
    "    losses / iters\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb55068-feaf-4f29-b6e6-e2ea18451efa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad3244b-38e1-4bae-927d-428018c9d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_batches(user_priorities, user_features, batch_size::Int, c::Channel)\n",
    "    while true\n",
    "        try\n",
    "            put!(c, get_batch(user_priorities, user_features, batch_size))\n",
    "        catch e\n",
    "            if isa(e, InvalidStateException)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "000bad20-6037-4d47-8d18-18d72cd35d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains a model with the given hyperparams and returns its validation loss\n",
    "function train_model(\n",
    "    hyp;\n",
    "    max_checkpoints = 100,\n",
    "    epochs_per_checkpoint = 10,\n",
    "    patience = 0,\n",
    "    verbose = true,\n",
    ")\n",
    "    if verbose\n",
    "        @info \"Getting data\"\n",
    "    end\n",
    "    opt = ADAMW(hyp.learning_rate, (0.9, 0.999), hyp.l2penalty)\n",
    "    Random.seed!(hyp.seed)\n",
    "    m = build_model(hyp.features) |> device\n",
    "    best_model = m |> cpu\n",
    "    ps = Flux.params(m)\n",
    "    stopper = early_stopper(max_iters = max_checkpoints, patience = patience)\n",
    "    training, test, user_features = get_data(hyp.features, hyp.allow_ptw)\n",
    "    batchloss(x, y) = model_loss(m, x, y)\n",
    "    epoch_size = Int(round(num_users() / hyp.batch_size))\n",
    "\n",
    "    if verbose\n",
    "        @info \"Setting up batches\"\n",
    "    end\n",
    "    training_batches = Channel(48)\n",
    "    test_batches = Channel(48)\n",
    "    for _ = 1:max(Threads.nthreads() - 1, 1)\n",
    "        Threads.@spawn generate_batches(\n",
    "            training,\n",
    "            user_features,\n",
    "            hyp.batch_size,\n",
    "            training_batches,\n",
    "        )\n",
    "        Threads.@spawn generate_batches(test, user_features, hyp.batch_size, test_batches)\n",
    "    end\n",
    "\n",
    "    if verbose\n",
    "        @info \"Training...\"\n",
    "    end\n",
    "    losses = []\n",
    "    loss = Inf\n",
    "    while (!stop!(stopper, loss))\n",
    "        for i = 1:epochs_per_checkpoint\n",
    "            for _ = 1:epoch_size\n",
    "                Flux.train!(batchloss, ps, take!(training_batches), opt)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        loss = split_loss(m, epoch_size, test_batches)\n",
    "        push!(losses, loss)\n",
    "        if loss == minimum(losses)\n",
    "            best_model = m |> cpu\n",
    "        end\n",
    "        if verbose\n",
    "            @info \"loss $loss\"\n",
    "        end\n",
    "    end\n",
    "\n",
    "    close(training_batches)\n",
    "    close(test_batches)\n",
    "    best_model, minimum(losses)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4db189-a655-43dd-b93a-2d449e6a6354",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dadbdc6d-d986-4870-a4e6-6467ac3aea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_hyperparams(hyp, λ)\n",
    "    hyp = @set hyp.learning_rate = 10^(λ[1] - 3)\n",
    "    hyp = @set hyp.l2penalty = 10^(λ[2] - 5)\n",
    "    hyp\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aebe789-d898-4872-b5e2-376ea1074cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_hyperparams(hyp; max_evals)\n",
    "    function nlopt_loss(λ, grad)\n",
    "        # nlopt internally converts to float64 because it calls a c library\n",
    "        λ = convert.(Float32, λ)\n",
    "        _, loss = train_model(create_hyperparams(hyp, λ))\n",
    "        @info \"$λ $loss\"\n",
    "        loss\n",
    "    end\n",
    "    opt = NLopt.Opt(:LN_NELDERMEAD, 2)\n",
    "    opt.initial_step = 1\n",
    "    opt.maxeval = max_evals\n",
    "    opt.min_objective = nlopt_loss\n",
    "    minf, λ, ret = NLopt.optimize(opt, zeros(Float32, 2))\n",
    "    numevals = opt.numevals\n",
    "\n",
    "    @info (\n",
    "        \"found minimum $minf at point $λ after $numevals function calls \" *\n",
    "        \"(ended because $ret) and saved model at\"\n",
    "    )\n",
    "    λ\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd36b5c4-e382-4442-acb7-41e623521076",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd45d6f1-f2a5-4ea9-830e-c93b4c6c3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_alpha(hyp, outdir; tune_hyperparams = true)\n",
    "    set_logging_outdir(outdir)\n",
    "\n",
    "    if tune_hyperparams\n",
    "        @info \"Optimizing hyperparameters...\"\n",
    "        λ = optimize_hyperparams(hyp; max_evals = 10)\n",
    "    else\n",
    "        λ = zeros(2)\n",
    "    end\n",
    "    hyp = create_hyperparams(hyp, λ)\n",
    "\n",
    "    @info \"Training model...\"\n",
    "    m, validation_loss =\n",
    "        train_model(hyp; max_checkpoints = 25, epochs_per_checkpoint = 1, patience = 10)\n",
    "    @info \"Trained model loss: $validation_loss\"\n",
    "\n",
    "    @info \"Writing alpha...\"\n",
    "    write_params(Dict(\"m\" => m, \"λ\" => λ, \"hyp\" => hyp), outdir)\n",
    "    @info \"Wrote alpha!\"\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b097d37-efdb-4f18-8ebe-bf2fa8d98bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "const alphas = [\n",
    "    \"Explicit\"\n",
    "    \"LinearImplicit\"\n",
    "    \"NonlinearImplicit\"\n",
    "    explicit_raw_alphas[1]\n",
    "    implicit_raw_alphas[1]\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcc5b860-1a37-4eff-9801-b4760f74b85c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 12:48:54 Training model...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 12:48:54 Getting data\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 12:48:56 getting user features\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:02\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 12:48:58 getting test explicit alphas\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.87 μs/it)\u001b[39mm\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 12:49:02 getting test implicit alphas\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.73 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 12:49:02 getting test negative alphas\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:25 ( 1.47 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:01:49 ( 6.25 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 12:51:31 getting test ptw alphas\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.07 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:49\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 12:53:21 adding priorities for test explicit\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.17 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.57 μs/it)\u001b[39mm\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 12:53:22 adding priorities for test implicit\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.46 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 12:54:16 adding priorities for test negative\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:03:07 (10.70 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:02:29 ( 8.50 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:00:50 adding priorities for test ptw\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.76 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:56\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:01:47 Setting up batches\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:01:47 Training...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:06\u001b[39m:00\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:06:26 loss 0.15948290579026414\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:06:49 loss 0.15887792671076198\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:07:12 loss 0.14987935311412673\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:07:36 loss 0.14993812980746968\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:07:59 loss 0.15683238072367486\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:08:22 loss 0.15090279185084698\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:08:45 loss 0.15205422902130436\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:10:07 loss 0.14862630793580578\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:10:30 loss 0.14890304194209175\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:10:53 loss 0.14894154833503315\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:11:16 loss 0.14960241275521213\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:11:39 loss 0.15005267650700926\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:12:02 loss 0.14864669687460588\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:12:25 loss 0.14791961445973117\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:12:48 loss 0.14769730832571823\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:13:11 loss 0.14799387499099229\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:14:32 loss 0.14816196624763225\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:14:55 loss 0.15119821996555163\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:15:18 loss 0.14741451297579702\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:15:41 loss 0.14742237719138793\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:16:05 loss 0.15352000229240073\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:16:28 loss 0.14731512801385047\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:16:51 loss 0.14785003908589298\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:17:15 loss 0.1500106577298642\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:09\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:19:23 loss 0.14684906841861275\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:19:23 Trained model loss: 0.14684906841861275\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:19:23 Writing alpha...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 13:19:25 Wrote alpha!\n"
     ]
    }
   ],
   "source": [
    "train_alpha(\n",
    "    Hyperparams(\n",
    "        allow_ptw = true,\n",
    "        batch_size = 1024,\n",
    "        features = alphas,\n",
    "        l2penalty = NaN,\n",
    "        learning_rate = NaN,\n",
    "        seed = 20220609,\n",
    "    ),\n",
    "    \"BPR.neural\";\n",
    "    tune_hyperparams = false,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9a083fa-447c-499e-8d99-b6c4a8c94888",
   "metadata": {},
   "outputs": [],
   "source": [
    "const ptw_alphas = [\n",
    "    alphas\n",
    "    [\n",
    "        \"LinearPtw\"\n",
    "        \"NonlinearPtw\"\n",
    "        ptw_raw_alphas[1]\n",
    "    ]\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cdc43b0-9e51-4caf-b808-0f617a099bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:10:12 Training model...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:10:12 Getting data\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:10:14 getting user features\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:02\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:10:16 getting test explicit alphas\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.89 μs/it)\u001b[39mm\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:10:20 getting test implicit alphas\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 0.77 μs/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:10:21 getting test negative alphas\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:28 ( 1.63 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:02:15 ( 7.74 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:02:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:15:31 adding priorities for test explicit\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:00 ( 1.39 μs/it)\u001b[39mm\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:15:32 adding priorities for test implicit\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:01:03 ( 0.16 ms/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:16:36 adding priorities for test negative\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:03:12 (10.96 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:04:25 (15.16 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:25:09 Setting up batches\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:25:09 Training...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:10\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:28:49 loss 0.0876742456315846\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:30:06 loss 0.08110773154608933\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:30:24 loss 0.07980792859183614\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:30:43 loss 0.07818957570803424\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:31:01 loss 0.07782054856434062\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:31:20 loss 0.0768333495313873\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:31:39 loss 0.0766140284359716\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:31:57 loss 0.07655958197349413\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:32:15 loss 0.07691079226434158\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:33:33 loss 0.0785184327700558\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:33:52 loss 0.07793653175883616\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:34:10 loss 0.07633971769523082\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:34:29 loss 0.07607486905501551\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:34:48 loss 0.07610107244340586\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:35:06 loss 0.07656222410052399\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:35:25 loss 0.07626596849381823\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:08\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:35:44 loss 0.07646441675416311\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:36:03 loss 0.0763229093204244\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:37:20 loss 0.07611573295229632\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:37:39 loss 0.0758265767946959\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:37:58 loss 0.0762056196373517\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:38:16 loss 0.07618016727690974\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:38:35 loss 0.07605695428892632\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:38:53 loss 0.07696650720764847\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:07\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:39:12 loss 0.07612228252916144\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:39:12 Trained model loss: 0.0758265767946959\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:39:12 Writing alpha...\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220827 22:39:14 Wrote alpha!\n"
     ]
    }
   ],
   "source": [
    "train_alpha(\n",
    "    Hyperparams(\n",
    "        allow_ptw = false,\n",
    "        batch_size = 1024,\n",
    "        features = ptw_alphas,\n",
    "        l2penalty = NaN,\n",
    "        learning_rate = NaN,\n",
    "        seed = 20220609,\n",
    "    ),\n",
    "    \"BPR.neural.ptw\";\n",
    "    tune_hyperparams = false,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053dcf25-61e2-4c81-8212-26bdd6a6a352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
