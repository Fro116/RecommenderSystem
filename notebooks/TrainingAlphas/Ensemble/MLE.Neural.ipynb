{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df5351f-2171-4365-b750-8984d42c5fe5",
   "metadata": {},
   "source": [
    "# Ranking\n",
    "* This is trained to learn the partial ordering implied by each user's watches\n",
    "* Items that are watched are preferred to items that have not been watched\n",
    "* If two items have been watched, then the impression metadata determines\n",
    "  which one, if any, is liked more\n",
    "* It uses the position aware maximum likehood estimation loss  \n",
    "* The inputs to this model are features generated by other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5a19a-24f5-4730-afe2-f451d27d18e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"MLE.Base.ipynb\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44115132-0caa-40b4-a452-de56b776212d",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0537cf0-2141-405e-9c84-dca26f35c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_kw struct Features\n",
    "    user_features::AbstractMatrix\n",
    "    query_features::AbstractMatrix\n",
    "    priorities::AbstractMatrix\n",
    "    user_to_indexes::Dict\n",
    "    index_to_item::Vector\n",
    "    preprocessing_data::Dict\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479024d7-69f8-4c26-bca0-30acb5c66fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "function training_test_split(f::Features; p::Real = 0.9)\n",
    "    training = Int32[]\n",
    "    test = Int32[]\n",
    "    cutoff = num_users() * p\n",
    "    @showprogress for u in keys(f.user_to_indexes)\n",
    "        if u < cutoff\n",
    "            push!(training, u)\n",
    "        else\n",
    "            push!(test, u)\n",
    "        end\n",
    "    end\n",
    "    training, test\n",
    "end\n",
    "\n",
    "function get_inference_data(f::Features)\n",
    "    f.preprocessing_data\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac16df-12aa-4511-94d4-4c2219187928",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_query_features(alphas::Vector{String}, split::String, content::String)\n",
    "    @info \"getting $split $content alphas\"\n",
    "    df = get_raw_split(split, content)\n",
    "    A = Matrix{Float16}(undef, length(df.user), length(alphas))\n",
    "    @tprogress Threads.@threads for i = 1:length(alphas)\n",
    "        A[:, i] = convert.(Float16, read_raw_alpha(alphas[i], split, content).rating)\n",
    "    end\n",
    "    collect(A')\n",
    "end;\n",
    "\n",
    "function normalize(x::AbstractArray; dims = 1)\n",
    "    T = eltype(x)\n",
    "    x = convert.(Float32, x)\n",
    "    μ = mean(x, dims = dims)\n",
    "    σ = std(x, dims = dims, mean = μ, corrected = false)\n",
    "    convert.(T, (x .- μ) ./ σ), Dict(\"μ\" => μ, \"σ\" => σ)\n",
    "end\n",
    "\n",
    "function get_user_features()\n",
    "    df = get_split(\"training\", \"implicit\")\n",
    "    sparse(df.item, df.user, convert.(Float16, df.rating), num_items(), num_users())\n",
    "end\n",
    "\n",
    "function get_features(alphas::Vector{String}, allow_ptw::Bool)\n",
    "    contents = all_contents\n",
    "    if !allow_ptw\n",
    "        contents = filter(x -> x != \"ptw\", contents)\n",
    "    end\n",
    "    hreduce(f; agg = hcat) = reduce(agg, f(\"test\", content) for content in contents)\n",
    "    user_features = get_user_features()\n",
    "    query_features = hreduce((split, content) -> get_query_features(alphas, split, content))\n",
    "    query_features, preprocessing_data = normalize(query_features; dims = 2)\n",
    "    priorities = hreduce(get_priorities)\n",
    "    user_to_indexes = get_user_to_indexes([(\"test\", content) for content in contents])\n",
    "    index_to_item =\n",
    "        hreduce((split, content) -> get_raw_split(split, content).item; agg = vcat)\n",
    "    Features(\n",
    "        user_features = user_features,\n",
    "        query_features = query_features,\n",
    "        priorities = priorities,\n",
    "        user_to_indexes = user_to_indexes,\n",
    "        index_to_item = index_to_item,\n",
    "        preprocessing_data = preprocessing_data,\n",
    "    )\n",
    "end\n",
    "\n",
    "function get_embedding(u::Integer, q::Integer, f::Features)\n",
    "    f.user_features[:, u], [f.index_to_item[q]], f.query_features[:, q]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a68783-a521-4742-b6fc-371e2c68d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sample(f::Features, users::Vector, list_size::Integer)\n",
    "    comparator = (i, j) -> compare(f.priorities[:, i], f.priorities[:, j])\n",
    "    while true\n",
    "        u = rand(users)\n",
    "        idxs = f.user_to_indexes[u]\n",
    "        if length(idxs) > 1\n",
    "            list = sample(idxs, list_size; replace = false)\n",
    "            if all(f.priorities[1, i] == 0 for i in list)\n",
    "                # if all the objects are unseen, then topological_sort will fail.\n",
    "                # topological_sort takes O(N^2) time, so we use this check to\n",
    "                # fail fast in O(N) time\n",
    "                continue\n",
    "            end\n",
    "            poset, valid = topological_sort(list, comparator)\n",
    "            if !valid\n",
    "                # fail if all objects are non-comparable\n",
    "                continue\n",
    "            end\n",
    "            embs = []\n",
    "            for q in poset\n",
    "                emb = get_embedding(u, q, f)\n",
    "                if length(embs) == 0\n",
    "                    for _ = 1:length(emb)\n",
    "                        push!(embs, [])\n",
    "                    end\n",
    "                end\n",
    "                push!.(embs, emb)\n",
    "            end\n",
    "            for i = 1:length(embs)\n",
    "                if eltype(embs[i][1]) <: Integer\n",
    "                    # for inputs that will be passed into an embedding layer\n",
    "                    embs[i] = convert.(Int32, reduce(vcat, embs[i]))\n",
    "                else\n",
    "                    # for inputs that will be passed into a dense layer\n",
    "                    embs[i] = convert.(Float32, reduce(hcat, embs[i]))\n",
    "                end\n",
    "            end\n",
    "            return embs\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6be697-b831-472e-9c47-de3042c9cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_model(hyp::Hyperparams)\n",
    "    Chain(\n",
    "        Join(\n",
    "            vcat,\n",
    "            Dense(num_items() => 32, bias = false),\n",
    "            Embedding(num_items() => 32),\n",
    "            identity,\n",
    "        ),\n",
    "        Dense(length(hyp.alphas) + 32 * 2, 64, relu),\n",
    "        Dense(64 => 32, relu),\n",
    "        Dense(32, 1),\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a616677-313b-4e3b-847d-662c703dd666",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e6681-572c-41a9-9b4e-08088b2bb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [\n",
    "    \"LinearExplicit\"\n",
    "    \"LinearImplicit\"\n",
    "    \"LinearPtw\"\n",
    "    \"Explicit\"\n",
    "    \"NonlinearImplicit\"\n",
    "    \"NonlinearPtw\"\n",
    "    explicit_raw_alphas\n",
    "    implicit_raw_alphas\n",
    "    ptw_raw_alphas\n",
    "    nondirectional_raw_alphas\n",
    "];\n",
    "hyp = Hyperparams(\n",
    "    allow_ptw = false,\n",
    "    alphas = alphas,\n",
    "    batch_size = 1024,\n",
    "    input_size = -1,\n",
    "    l2penalty = NaN,\n",
    "    learning_rate = NaN,\n",
    "    list_size = 2,\n",
    "    seed = 20220609,\n",
    ")\n",
    "hyp = create_hyperparams(hyp, [0.0f0, 0.0f0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d034a-3777-4530-96ad-8fe07f8bceaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_alpha(hyp, \"MLE.Neural\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98097c60-d064-4faa-9e6d-720707628fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0.18465150250650614 using new mle loss formulation ( -> 64 -> 32 -> 1)\n",
    "# 0.1770052581855019 using input normalization\n",
    "# going wider by 4x didn't help\n",
    "# going deeper by 2 layers didn't help\n",
    "# 0.18148092587706433 using 50% drpout make things worse\n",
    "# 0.05950891730785771 by scaling the loss function down (should be a no-op)\n",
    "# 0.056652724017389744 with double embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704589f-19b3-410e-bc5a-ac58a13a7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_checkpoints::Integer = 100\n",
    "# epochs_per_checkpoint::Integer = 1\n",
    "# patience::Integer = 0\n",
    "# verbose::Bool = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc957cd-b575-4b71-9fa5-123a34c239a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if verbose\n",
    "#     @info \"Initializing model\"\n",
    "# end\n",
    "# opt = ADAMW(hyp.learning_rate, (0.9, 0.999), hyp.l2penalty)\n",
    "# Random.seed!(hyp.seed)\n",
    "# m = build_model(hyp) |> device\n",
    "# best_model = m |> cpu\n",
    "# ps = Flux.params(m)\n",
    "# stopper = early_stopper(\n",
    "#     max_iters = max_checkpoints,\n",
    "#     patience = patience,\n",
    "#     min_rel_improvement = 1e-3,\n",
    "# )\n",
    "# batchloss(x...) = position_aware_list_mle_loss(m, x)\n",
    "# epoch_size = Int(round(num_users() / hyp.batch_size))\n",
    "# function loginfo(x)\n",
    "#     if verbose\n",
    "#         @info x\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# loginfo(\"Getting data\")\n",
    "# f = get_features(hyp.alphas, false)\n",
    "# training_users, test_users = training_test_split(f.user_to_indexes)\n",
    "# setup_channel(users) = setup_batch_channel(\n",
    "#     f,\n",
    "#     users,\n",
    "#     hyp,\n",
    "#     64,\n",
    "# )\n",
    "# training_batches = setup_channel(training_users)\n",
    "# test_batches = setup_channel(test_users)\n",
    "# @info \"Testing channels\"\n",
    "# @info size.(get_batch(training_batches))\n",
    "# @info size.(get_batch(test_batches))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
