{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e701df7-0d06-4631-a382-8228a4d3a845",
   "metadata": {},
   "source": [
    "# Pretrain Dataset\n",
    "* Saves pretraining data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90afc10-1ae5-4665-937e-d58a5859ac94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Data.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd56259-4513-4297-a978-99c765532231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "const name = \"all/Transformer/maskv4\"\n",
    "set_logging_outdir(name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d60ae-6b55-4274-85b1-c0f9c9fc7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import H5Zblosc\n",
    "import HDF5\n",
    "import JSON\n",
    "import MLUtils\n",
    "import Random\n",
    "import StatsBase: mean, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281b189-0d9e-4aa6-aea0-0d7ca80b6db3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# the dataset is too big to load into memory, so we process it in parts\n",
    "partition = 0\n",
    "num_partitions = 1\n",
    "num_epochs = 1\n",
    "mode = \"map\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6434e07-f565-474a-ac05-12c409afe141",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed260a-b454-4690-ab8f-15ba4d686b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sentences(cls_tokens, partition)\n",
    "    sortedvals(x::Dict) = [x[k] for k in sort(collect(keys(x)))]\n",
    "    sentences = sortedvals(get_training_data(cls_tokens, partition, nothing))\n",
    "    Random.shuffle!(sentences)\n",
    "    cutoff = Int(round(0.99 * length(sentences))) # TODO switch to 0.999 for prod\n",
    "    Dict(\"training\" => sentences[1:cutoff], \"validation\" => sentences[cutoff+1:end])\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed0909-a7b0-4b54-ae7b-20dd5b9b789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(sentence::Vector{wordtype}, config, training) = tokenize(;\n",
    "    sentence = sentence,\n",
    "    max_seq_len = config[:max_sequence_length],\n",
    "    vocab_sizes = config[:base_vocab_sizes],\n",
    "    pad_tokens = config[:pad_tokens],\n",
    "    cls_tokens = config[:cls_tokens],\n",
    "    mask_tokens = config[:mask_tokens],\n",
    "    training = training,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b524c73-58a0-4c31-9ff0-48f81eb0fa0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "function tokenize(;\n",
    "    sentence::Vector{wordtype},\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    pad_tokens,\n",
    "    cls_tokens,\n",
    "    mask_tokens,\n",
    "    training,\n",
    ")\n",
    "    tokens = get_token_ids(sentence, max_seq_len, pad_tokens, cls_tokens)\n",
    "    positions = Dict(\n",
    "        x => Dict(y => zeros(Int32, max_seq_len) for y in ALL_METRICS) for x in ALL_MEDIUMS\n",
    "    )\n",
    "    weights = Dict(\n",
    "        x => Dict(y => zeros(Float32, max_seq_len) for y in ALL_METRICS) for\n",
    "        x in ALL_MEDIUMS\n",
    "    )\n",
    "    labels = Dict(\n",
    "        x => Dict(y => zeros(Float32, max_seq_len) for y in ALL_METRICS) for\n",
    "        x in ALL_MEDIUMS\n",
    "    )\n",
    "    userids = Dict(\n",
    "        x => Dict(y => zeros(Int32, max_seq_len) for y in ALL_METRICS) for x in ALL_MEDIUMS\n",
    "    )\n",
    "    for i::Int32 = 1:max_seq_len\n",
    "        is_manga = extract(tokens, :mangaid)[i] != extract(cls_tokens, :mangaid)\n",
    "        is_anime = extract(tokens, :animeid)[i] != extract(cls_tokens, :animeid)\n",
    "        has_rating =\n",
    "            0 < extract(tokens, :rating)[i] <= vocab_sizes[get_wordtype_index(:rating)]\n",
    "        has_watch =\n",
    "            get_status(:plan_to_watch) <\n",
    "            extract(tokens, :status)[i] <=\n",
    "            vocab_sizes[get_wordtype_index(:status)]\n",
    "        has_plantowatch = extract(tokens, :status)[i] == get_status(:plan_to_watch)\n",
    "        has_drop =\n",
    "            get_status(:none) <\n",
    "            extract(tokens, :status)[i] <=\n",
    "            vocab_sizes[get_wordtype_index(:status)]\n",
    "        if !(has_rating || has_watch || has_plantowatch || has_drop)\n",
    "            continue\n",
    "        end\n",
    "        @assert xor(is_manga, is_anime)\n",
    "        if is_manga\n",
    "            medium = \"manga\"\n",
    "            mediaid = extract(tokens, :mangaid)[i]\n",
    "        elseif is_anime\n",
    "            medium = \"anime\"\n",
    "            mediaid = extract(tokens, :animeid)[i]\n",
    "        else\n",
    "            @assert false\n",
    "        end\n",
    "\n",
    "        # make predictions for 15% of tokens\n",
    "        if rand() < 0.15\n",
    "            if has_rating\n",
    "                positions[medium][\"rating\"][i] = mediaid\n",
    "                labels[medium][\"rating\"][i] = extract(tokens, :rating)[i]\n",
    "                weights[medium][\"rating\"][i] = 1\n",
    "                userids[medium][\"rating\"][i] = extract(tokens, :userid)[i]\n",
    "            end\n",
    "            if has_watch\n",
    "                positions[medium][\"watch\"][i] = mediaid\n",
    "                labels[medium][\"watch\"][i] = 1\n",
    "                weights[medium][\"watch\"][i] = 1\n",
    "                userids[medium][\"watch\"][i] = extract(tokens, :userid)[i]\n",
    "            end\n",
    "            if has_plantowatch\n",
    "                positions[medium][\"plantowatch\"][i] = mediaid\n",
    "                labels[medium][\"plantowatch\"][i] = 1\n",
    "                weights[medium][\"plantowatch\"][i] = 1\n",
    "                userids[medium][\"plantowatch\"][i] = extract(tokens, :userid)[i]\n",
    "            end\n",
    "            if has_drop\n",
    "                positions[medium][\"drop\"][i] = mediaid\n",
    "                labels[medium][\"drop\"][i] =\n",
    "                    extract(tokens, :status)[i] <= get_status(:dropped)\n",
    "                weights[medium][\"drop\"][i] = 1\n",
    "                userids[medium][\"drop\"][i] = extract(tokens, :userid)[i]\n",
    "            end\n",
    "            keep_fields = get_wordtype_index.([:updated_at, :position, :userid])\n",
    "            for j = 1:length(tokens)\n",
    "                if j in keep_fields\n",
    "                    continue\n",
    "                end\n",
    "                tokens[j][i] = mask_tokens[j]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if !training\n",
    "        for x in ALL_MEDIUMS\n",
    "            for y in ALL_METRICS\n",
    "                weight_by_user!(weights[x][y], userids[x][y])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    tokens, positions, labels, weights\n",
    "end;\n",
    "\n",
    "function weight_by_user!(weights, userids)\n",
    "    uid_to_count = Dict(i => 0 for i in userids)\n",
    "    for i in userids\n",
    "        uid_to_count[i] += 1\n",
    "    end\n",
    "    for i = 1:length(userids)\n",
    "        if weights[i] != 0\n",
    "            weights[i] /= uid_to_count[userids[i]]\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7ebe9-122d-4b77-a00b-87206686e4b3",
   "metadata": {},
   "source": [
    "# Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40bbef-1d9b-4437-9e1a-fbef99872542",
   "metadata": {},
   "outputs": [],
   "source": [
    "function hdf5_writer(c::Channel)\n",
    "    while true\n",
    "        (d, fn) = take!(c)\n",
    "        HDF5.h5open(fn, \"w\") do f\n",
    "            for (k, v) in d\n",
    "                f[k, blosc = 1] = v\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea2b0a-d24e-4712-a3cf-1b6c03e72ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_tokens(sentences, config, filename, writer, training)\n",
    "    tokens = Any[nothing for _ = 1:length(sentences)]\n",
    "    Threads.@threads for i = 1:length(sentences)\n",
    "        tokens[i] = tokenize(sentences[i], config, training)\n",
    "    end\n",
    "    d = Dict{String,AbstractArray}()\n",
    "    collate = MLUtils.batch\n",
    "    for (i, name) in Iterators.enumerate(config.vocab_names)\n",
    "        d[name] = collate([x[1][i] for x in tokens])\n",
    "    end\n",
    "    for medium in ALL_MEDIUMS\n",
    "        for metric in ALL_METRICS\n",
    "            stem = \"$(medium)_$(metric)\"\n",
    "            d[\"positions_$stem\"] = collate([x[2][medium][metric] for x in tokens])\n",
    "            d[\"labels_$stem\"] = collate([x[3][medium][metric] for x in tokens])\n",
    "            d[\"weights_$stem\"] = collate([x[4][medium][metric] for x in tokens])\n",
    "        end\n",
    "    end\n",
    "    put!(writer, (d, filename))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0deae8-f55a-4293-8ebb-84011b487f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_batch(sentences, state, batch_size, max_sequence_length, max_document_length)\n",
    "    # Constructs an iterator that chunks the data into batches \n",
    "    # of `batch_size` sentences with `max_sequence_length` words\n",
    "    W = wordtype\n",
    "    batch = Vector{Vector{W}}()\n",
    "    sentence = Vector{W}()\n",
    "    for i = state.index:length(sentences)\n",
    "        if state.resuming\n",
    "            s = state.sentence\n",
    "            state = @set state.resuming = false\n",
    "        else\n",
    "            s = subset_sentence(sentences[i], max_document_length; recent = false)\n",
    "        end\n",
    "        for j = 1:length(s)\n",
    "            push!(sentence, s[j])\n",
    "            if length(sentence) == max_sequence_length\n",
    "                push!(batch, sentence)\n",
    "                sentence = Vector{W}()\n",
    "                if length(batch) == batch_size\n",
    "                    return batch,\n",
    "                    (\n",
    "                        resuming = true,\n",
    "                        finished = false,\n",
    "                        index = i,\n",
    "                        sentence = s[j+1:end],\n",
    "                        batch = state.batch + 1,\n",
    "                    )\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    push!(batch, sentence)\n",
    "    batch, (finished = true, batch = state.batch + 1)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11aa3e7-2ffd-4657-9622-ea9ed8cc325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_epoch(sentences, config, epoch, outdir, split, writer)\n",
    "    outdir = joinpath(outdir, split, \"$epoch\")\n",
    "    mkpath(outdir)\n",
    "    Random.shuffle!(sentences)\n",
    "    state = (resuming = false, finished = false, index = 1, batch = 0)\n",
    "    num_tokens = 0\n",
    "    num_sentences = 0\n",
    "    expected_num_tokens = getfield(config, Symbol(\"$(split)_epoch_tokens\"))\n",
    "    expected_num_sentences = getfield(config, Symbol(\"$(split)_epoch_size\"))\n",
    "\n",
    "    p = ProgressMeter.Progress(expected_num_tokens)\n",
    "    while !state.finished\n",
    "        batch, state = get_batch(\n",
    "            sentences,\n",
    "            state,\n",
    "            config.batch_size,\n",
    "            config.max_sequence_length,\n",
    "            config.max_document_length,\n",
    "        )\n",
    "        save_tokens(\n",
    "            batch,\n",
    "            config,\n",
    "            joinpath(outdir, \"$(state.batch-1).h5\"),\n",
    "            writer,\n",
    "            split == \"training\",\n",
    "        )\n",
    "        num_tokens += sum(length.(batch))\n",
    "        num_sentences += length(batch)\n",
    "        ProgressMeter.update!(p, num_tokens)\n",
    "    end\n",
    "    ProgressMeter.finish!(p)\n",
    "    @assert (num_tokens == expected_num_tokens) && (num_sentences == expected_num_sentences)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50183c75-f670-425e-b08e-c72efe4e8988",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb67b6-dabb-4db8-841d-c7ebf43f4a1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "function create_training_config()\n",
    "    max_sequence_length = 1024\n",
    "    base_vocab_sizes = (\n",
    "        num_items(\"manga\") - 1, # mangaid\n",
    "        num_items(\"anime\") - 1, # animeid\n",
    "        Float32(10), # rating\n",
    "        Float32(1), # updated_at\n",
    "        Int32(get_status(:rewatching)), # status\n",
    "        Int32(3), # source\n",
    "        Float32(1), # created_at\n",
    "        Float32(1), # started_at\n",
    "        Float32(1), # finished_at\n",
    "        Float32(1), # progress\n",
    "        Float32(1), # 1 - 1 / (repeat_count + 1)\n",
    "        Float32(1), # 1 - 1 / (priority + 1)\n",
    "        Int32(3), # sentiment\n",
    "        Float32(1), # sentiment_score\n",
    "        Int32(max_sequence_length - 1), # position\n",
    "        Int32(num_users() - 1), # userid\n",
    "    )\n",
    "    d = (\n",
    "        # tokenization\n",
    "        base_vocab_sizes = convert(wordtype, base_vocab_sizes),\n",
    "        cls_tokens = convert(wordtype, base_vocab_sizes .+ 1),\n",
    "        pad_tokens = convert(wordtype, base_vocab_sizes .+ 2),\n",
    "        mask_tokens = convert(wordtype, base_vocab_sizes .+ 3),\n",
    "        vocab_sizes = convert(wordtype, base_vocab_sizes .+ 4),\n",
    "        vocab_types = (\n",
    "            \"int\",\n",
    "            \"int\",\n",
    "            \"float\",\n",
    "            \"float\",\n",
    "            \"int\",\n",
    "            \"int\",\n",
    "            \"float\",\n",
    "            \"float\",\n",
    "            \"float\",\n",
    "            \"float\",\n",
    "            \"float\",\n",
    "            \"float\",\n",
    "            \"int\",\n",
    "            \"float\",\n",
    "            \"int\",\n",
    "            \"none\",\n",
    "        ),\n",
    "        vocab_names = [\n",
    "            \"mangaid\",\n",
    "            \"animeid\",\n",
    "            \"rating\",\n",
    "            \"updated_at\",\n",
    "            \"status\",\n",
    "            \"source\",\n",
    "            \"created_at\",\n",
    "            \"started_at\",\n",
    "            \"finished_at\",\n",
    "            \"progress\",\n",
    "            \"repeat_count\",\n",
    "            \"priority\",\n",
    "            \"sentiment\",\n",
    "            \"sentiment_score\",\n",
    "            \"position\",\n",
    "            \"userid\",\n",
    "        ],\n",
    "        media_sizes = Dict(m => num_items(m) for m in ALL_MEDIUMS),\n",
    "        # data\n",
    "        max_document_length = Inf, # TODO experiment with subsampling\n",
    "        batch_size = 2^11,\n",
    "        training_epoch_size = -1,\n",
    "        training_epoch_tokens = -1,\n",
    "        validation_epoch_size = -1,\n",
    "        validation_epoch_tokens = -1,\n",
    "        # model\n",
    "        max_sequence_length = max_sequence_length,\n",
    "        mode = \"pretrain\",\n",
    "    )\n",
    "    @assert d[:max_document_length] >= d[:max_sequence_length]\n",
    "    @assert length(d[:vocab_sizes]) == length(d[:vocab_types])\n",
    "    @assert length(d[:vocab_sizes]) == length(d[:vocab_names])\n",
    "    d\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e8899-6268-4081-b7d1-d2b09a4b63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_epoch_size(config, sentences)\n",
    "    for t in [\"training\", \"validation\"]\n",
    "        s = sentences[t]\n",
    "        num_tokens = Int64(sum(min.(length.(s), config[:max_document_length])))\n",
    "        @info \"Number of $t tokens: $(num_tokens)\"\n",
    "        @info \"Number of $t sentences: $(length(s))\"\n",
    "        config = @set config[Symbol(\"$(t)_epoch_tokens\")] = num_tokens\n",
    "        config = @set config[Symbol(\"$(t)_epoch_size\")] =\n",
    "            div(num_tokens, config[:max_sequence_length], RoundUp)\n",
    "    end\n",
    "    config\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31c5b2-42e2-455a-bd3f-0ca4eab8cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function setup_training(config, outdir)\n",
    "    mkpath(outdir)\n",
    "    fn = joinpath(outdir, \"config.json\")\n",
    "    open(fn, \"w\") do f\n",
    "        write(f, JSON.json(config))\n",
    "    end\n",
    "    for split in [\"training\", \"validation\"]\n",
    "        fn = joinpath(outdir, split)\n",
    "        mkpath(fn)\n",
    "        for x in readdir(fn, join = true)\n",
    "            rm(x, recursive = true)\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd1b43-c81b-4560-833e-cb829eb32945",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize function get_writer()\n",
    "    writer = Channel(8)\n",
    "    Threads.@spawn hdf5_writer(writer)\n",
    "    writer\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6204205-cd39-44de-91c8-15cfa03abd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_epochs(partition::Int, num_epochs::Int)\n",
    "    Random.seed!(partition)\n",
    "    config = create_training_config()\n",
    "    sentences =\n",
    "        JLD2.load(get_data_path(joinpath(\"alphas\", name, \"sentences.$partition.jld2\")))\n",
    "    config = set_epoch_size(config, sentences)\n",
    "    outdir = get_data_path(joinpath(\"alphas\", name, \"$partition\"))\n",
    "    setup_training(config, outdir)\n",
    "    writer = get_writer()\n",
    "    for epoch = 0:num_epochs-1\n",
    "        for t in [\"validation\", \"training\"]\n",
    "            save_epoch(sentences[t], config, epoch, outdir, t, writer)\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a48e8-3774-4bae-979b-a2c590587ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"map\"\n",
    "    const config = create_training_config()\n",
    "    JLD2.save(\n",
    "        get_data_path(joinpath(\"alphas\", name, \"sentences.$partition.jld2\")),\n",
    "        get_sentences(config[:cls_tokens], (partition, num_partitions));\n",
    "        compress = true,\n",
    "    )\n",
    "elseif mode == \"reduce\"\n",
    "    @time save_epochs(partition, num_epochs)\n",
    "    sleep(60) # wait for final epoch to finish writing\n",
    "else\n",
    "    @assert false\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0-rc3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
