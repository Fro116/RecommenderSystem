{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd56259-4513-4297-a978-99c765532231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"all/Transformer/v0\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90afc10-1ae5-4665-937e-d58a5859ac94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Reference/Data.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d60ae-6b55-4274-85b1-c0f9c9fc7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux\n",
    "import HDF5\n",
    "import JSON\n",
    "import Random\n",
    "import StatsBase: mean, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6434e07-f565-474a-ac05-12c409afe141",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed0909-a7b0-4b54-ae7b-20dd5b9b789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurize(sentence::Vector{wordtype}, config, rng, training::Bool) = featurize(;\n",
    "    sentence = sentence,\n",
    "    max_seq_len = config[\"max_sequence_length\"],\n",
    "    vocab_sizes = config[\"base_vocab_sizes\"],\n",
    "    pad_tokens = config[\"pad_tokens\"],\n",
    "    cls_tokens = config[\"cls_tokens\"],\n",
    "    mask_tokens = config[\"mask_tokens\"],\n",
    "    empty_tokens = config[\"empty_tokens\"],\n",
    "    explicit_baseline = Dict(k => rand(rng, v) for (k, v) in config[\"explicit_baseline\"]),\n",
    "    rng = rng,\n",
    "    training = training,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b524c73-58a0-4c31-9ff0-48f81eb0fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function featurize(;\n",
    "    sentence::Vector{wordtype},\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    pad_tokens,\n",
    "    cls_tokens,\n",
    "    mask_tokens,\n",
    "    empty_tokens,\n",
    "    explicit_baseline,\n",
    "    rng,\n",
    "    training,\n",
    ")\n",
    "    # pad to the largest sequence length\n",
    "    seq_len = max_seq_len\n",
    "    sentence =\n",
    "        subset_sentence(sentence, seq_len; recent = false, keep_first = false, rng = rng)\n",
    "\n",
    "    # get tokenized sentences\n",
    "    tokens =\n",
    "        vec.(\n",
    "            get_token_ids(\n",
    "                [sentence],\n",
    "                seq_len,\n",
    "                extract(vocab_sizes, :position),\n",
    "                pad_tokens,\n",
    "                cls_tokens,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # demean ratings\n",
    "    if !isnothing(explicit_baseline)\n",
    "        demean = (\n",
    "            anime = (\n",
    "                rating = Dict{Int32,Float32}(),\n",
    "                count = Dict{Int32,Int32}(),\n",
    "                weight = Dict{Int32,Float32}(),\n",
    "            ),\n",
    "            manga = (\n",
    "                rating = Dict{Int32,Float32}(),\n",
    "                count = Dict{Int32,Int32}(),\n",
    "                weight = Dict{Int32,Float32}(),\n",
    "            ),\n",
    "        )\n",
    "        demean_item_weights = (\n",
    "            anime = explicit_baseline[:anime][\"weights\"],\n",
    "            manga = explicit_baseline[:manga][\"weights\"],\n",
    "        )\n",
    "    end\n",
    "\n",
    "    positions = (\n",
    "        anime = (item = zeros(Int32, seq_len), rating = zeros(Int32, seq_len)),\n",
    "        manga = (item = zeros(Int32, seq_len), rating = zeros(Int32, seq_len)),\n",
    "    )\n",
    "    weights = (\n",
    "        anime = (item = zeros(Float32, seq_len), rating = zeros(Float32, seq_len)),\n",
    "        manga = (item = zeros(Float32, seq_len), rating = zeros(Float32, seq_len)),\n",
    "    )\n",
    "    labels = (\n",
    "        anime = (item = zeros(Float32, seq_len), rating = zeros(Float32, seq_len)),\n",
    "        manga = (item = zeros(Float32, seq_len), rating = zeros(Float32, seq_len)),\n",
    "    )\n",
    "    userids = (\n",
    "        anime = (item = zeros(Int32, seq_len), rating = zeros(Int32, seq_len)),\n",
    "        manga = (item = zeros(Int32, seq_len), rating = zeros(Int32, seq_len)),\n",
    "    )\n",
    "    for i::Int32 = 1:seq_len\n",
    "        # randomly mask 15% of non-trivial tokens \n",
    "        has_anime =\n",
    "            (extract(tokens, :anime)[i] <= extract(vocab_sizes, :anime)) &&\n",
    "            (extract(tokens, :status)[i] != get_status(:plan_to_watch))\n",
    "        has_manga =\n",
    "            (extract(tokens, :manga)[i] <= extract(vocab_sizes, :manga)) &&\n",
    "            (extract(tokens, :status)[i] != get_status(:plan_to_watch))\n",
    "        has_rating = extract(tokens, :rating)[i] < extract(vocab_sizes, :rating)\n",
    "        if has_anime\n",
    "            medium = :anime\n",
    "        elseif has_manga\n",
    "            medium = :manga\n",
    "        end\n",
    "        should_mask = rand(rng) < 0.15\n",
    "\n",
    "        # prepare to demean ratings\n",
    "        if !should_mask && has_rating\n",
    "            u = extract(tokens, :user)[i]\n",
    "            a = extract(tokens, medium)[i]\n",
    "            if u ∉ keys(demean[medium][:rating])\n",
    "                demean[medium][:rating][u] = 0\n",
    "                demean[medium][:count][u] = 0\n",
    "                demean[medium][:weight][u] = 0\n",
    "            end\n",
    "            weight =\n",
    "                demean_item_weights[medium][a] * powerlawdecay(\n",
    "                    1 .- cast_universal_timestamp(\n",
    "                        extract(tokens, :timestamp)[i],\n",
    "                        String(medium),\n",
    "                    ),\n",
    "                    explicit_baseline[medium][\"λ\"][5],\n",
    "                )\n",
    "            demean[medium][:rating][u] +=\n",
    "                weight * (extract(tokens, :rating)[i] - explicit_baseline[medium][\"a\"][a])\n",
    "            demean[medium][:count][u] += 1\n",
    "            demean[medium][:weight][u] += weight\n",
    "        end\n",
    "\n",
    "        # record tokens before we mask them out\n",
    "        if !(should_mask && (has_anime || has_manga || has_rating))\n",
    "            continue\n",
    "        end\n",
    "        if has_anime || has_manga\n",
    "            positions[medium][:item][i] = extract(tokens, medium)[i]\n",
    "            labels[medium][:item][i] = 1\n",
    "            weights[medium][:item][i] = 1\n",
    "            userids[medium][:item][i] = extract(tokens, :user)[i]\n",
    "        end\n",
    "        if has_rating\n",
    "            positions[medium][:rating][i] = extract(tokens, medium)[i]\n",
    "            labels[medium][:rating][i] = extract(tokens, :rating)[i]\n",
    "            weights[medium][:rating][i] = 1\n",
    "            userids[medium][:rating][i] = extract(tokens, :user)[i]\n",
    "        end\n",
    "\n",
    "        # bert masking\n",
    "        item_allowed_info = get_wordtype_index.([medium, :rating, :timestamp, :position])\n",
    "        item_skip_info = get_wordtype_index.([:anime, :manga, :user])\n",
    "        for j = 1:length(tokens)\n",
    "            if j in item_allowed_info || j in item_skip_info\n",
    "                continue\n",
    "            end\n",
    "            tokens[j][i] = mask_tokens[j]\n",
    "        end\n",
    "        for j in item_allowed_info\n",
    "            if j in get_wordtype_index.([medium, :rating])\n",
    "                cutoffs = (0.8, 0.9)\n",
    "                r = training ? rand(rng) : 0.0\n",
    "            elseif j == get_wordtype_index(:timestamp)\n",
    "                cutoffs = (0.45, 0.9)\n",
    "                r = training ? rand(rng) : 0.0\n",
    "            elseif j == get_wordtype_index(:position)\n",
    "                cutoffs = (0.45, 0.9)\n",
    "                r = training ? rand(rng) : 0.7\n",
    "            else\n",
    "                @assert false\n",
    "            end\n",
    "            if r <= cutoffs[1]\n",
    "                tokens[j][i] = mask_tokens[j]\n",
    "            elseif r <= cutoffs[2]\n",
    "                nothing\n",
    "            else\n",
    "                if eltype(vocab_sizes[j]) == Int32\n",
    "                    tokens[j][i] = rand(rng, 1:vocab_sizes[j])\n",
    "                elseif eltype(tokens[j]) == Float32\n",
    "                    tokens[j][i] = rand(rng) * vocab_sizes[j]\n",
    "                else\n",
    "                    @assert false\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # demean ratings\n",
    "    for medium in [:anime, :manga]\n",
    "        demean_explicit_ratings!(\n",
    "            tokens = tokens,\n",
    "            medium = medium,\n",
    "            demean = demean[medium],\n",
    "            explicit_baseline = explicit_baseline[medium],\n",
    "            vocab_sizes = vocab_sizes,\n",
    "            cls_tokens = cls_tokens,\n",
    "            empty_tokens = empty_tokens,\n",
    "            positions = positions[medium],\n",
    "            labels = labels[medium],\n",
    "            userids = userids[medium],\n",
    "        )\n",
    "    end\n",
    "\n",
    "    if !training\n",
    "        for x in [:anime, :manga]\n",
    "            for y in [:item, :rating]\n",
    "                weight_by_user!(weights[x][y], userids[x][y])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    tokens, positions, labels, weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11afbf8-eda7-4f47-a510-cd36d320f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "function demean_explicit_ratings!(;\n",
    "    tokens,\n",
    "    medium,\n",
    "    demean,\n",
    "    explicit_baseline,\n",
    "    vocab_sizes,\n",
    "    cls_tokens,\n",
    "    empty_tokens,\n",
    "    positions,\n",
    "    labels,\n",
    "    userids,\n",
    ")\n",
    "    user_to_baseline = Dict{Int32,Float32}()\n",
    "    μ_user = mean(explicit_baseline[\"u\"])\n",
    "    μ_item = mean(explicit_baseline[\"a\"])\n",
    "    for u in keys(demean[:rating])\n",
    "        user_weight = powerdecay(demean[:count][u], log(explicit_baseline[\"λ\"][3]))\n",
    "        user_to_baseline[u] =\n",
    "            (demean[:rating][u] * user_weight + μ_user * explicit_baseline[\"λ\"][1]) /\n",
    "            (demean[:weight][u] * user_weight + explicit_baseline[\"λ\"][1])\n",
    "    end\n",
    "    get_user_bias(u) = u in keys(user_to_baseline) ? user_to_baseline[u] : μ_user\n",
    "    get_item_bias(a) =\n",
    "        a in keys(explicit_baseline[\"a\"]) ? explicit_baseline[\"a\"][a] : μ_item\n",
    "\n",
    "    for i::Int32 = 1:size(extract(tokens, medium))[1]\n",
    "        if extract(tokens, medium)[i] == extract(empty_tokens, medium)\n",
    "            continue\n",
    "        end\n",
    "        has_explicit_rating = (extract(tokens, :rating)[i] .< extract(vocab_sizes, :rating))\n",
    "        if has_explicit_rating\n",
    "            extract(tokens, :rating)[i] -=\n",
    "                get_user_bias(extract(tokens, :user)[i]) +\n",
    "                get_item_bias(extract(tokens, medium)[i])\n",
    "        end\n",
    "    end\n",
    "    for i = 1:length(labels[:rating])\n",
    "        if userids[:rating][i] != 0\n",
    "            labels[:rating][i] -=\n",
    "                get_user_bias(userids[:rating][i]) + get_item_bias(positions[:rating][i])\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002d542-1609-4c35-b8f3-32e0e5b16647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function weight_by_user!(weights, userids)\n",
    "    uid_to_count = Dict(i => 0 for i in userids)\n",
    "    for i in userids\n",
    "        uid_to_count[i] += 1\n",
    "    end\n",
    "    for i = 1:length(userids)\n",
    "        if weights[i] != 0\n",
    "            weights[i] /= uid_to_count[userids[i]]\n",
    "        end\n",
    "    end\n",
    "    weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7ebe9-122d-4b77-a00b-87206686e4b3",
   "metadata": {},
   "source": [
    "# Data colleciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0deae8-f55a-4293-8ebb-84011b487f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "function shuffle_training_data(rng, sentences, max_sequence_length, max_document_length)\n",
    "    order = Random.shuffle(rng, 1:length(sentences))\n",
    "    S = eltype(sentences)\n",
    "    W = eltype(sentences[1])\n",
    "\n",
    "    # concatenate all tokens\n",
    "    tokens = Vector{W}()\n",
    "    for i in order\n",
    "        sentence = subset_sentence(\n",
    "            sentences[i],\n",
    "            max_document_length;\n",
    "            recent = false,\n",
    "            keep_first = false,\n",
    "            rng = rng,\n",
    "        )\n",
    "        for token in sentence\n",
    "            push!(tokens, token)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # patition tokens into minibatches\n",
    "    batched_sentences = Vector{S}()\n",
    "    sentence = Vector{W}()\n",
    "    for token in tokens\n",
    "        push!(sentence, token)\n",
    "        if length(sentence) == max_sequence_length\n",
    "            push!(batched_sentences, sentence)\n",
    "            sentence = Vector{W}()\n",
    "        end\n",
    "    end\n",
    "    if length(sentence) > 0\n",
    "        push!(batched_sentences, sentence)\n",
    "    end\n",
    "    batched_sentences\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e8d4d-1997-475f-bc5c-f489c1c45ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_training_data(media, include_ptw, cls_tokens, empty_tokens)\n",
    "    n_tasks = length(ALL_TASKS)\n",
    "    sentences = Vector{Vector{Vector{wordtype}}}(undef, n_tasks)\n",
    "    for i = 1:length(sentences)\n",
    "        data = get_training_data(ALL_TASKS[i], media, include_ptw, cls_tokens, empty_tokens)\n",
    "        sentences[i] = collect(values(data))\n",
    "    end\n",
    "    vcat(sentences...)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed260a-b454-4690-ab8f-15ba4d686b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sentences(rng, training_config)\n",
    "    sentences = get_training_data(\n",
    "        training_config[\"media\"],\n",
    "        training_config[\"include_ptw_impressions\"],\n",
    "        training_config[\"cls_tokens\"],\n",
    "        training_config[\"empty_tokens\"],\n",
    "    )\n",
    "    Random.shuffle!(rng, sentences)\n",
    "    cutoff = Int(round(0.99 * length(sentences)))\n",
    "    training = sentences[1:cutoff]\n",
    "    validation = sentences[cutoff+1:end]\n",
    "    training, validation\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50183c75-f670-425e-b08e-c72efe4e8988",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ac800-38d2-4ed9-9963-05c176baef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_rngs(seed)\n",
    "    rng = Random.Xoshiro(seed)\n",
    "    Random.seed!(rand(rng, UInt64))\n",
    "    rng\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb67b6-dabb-4db8-841d-c7ebf43f4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_training_config()\n",
    "    media = [\"anime\", \"manga\"]\n",
    "    base_vocab_sizes = (\n",
    "        Int32(num_items(\"anime\")),\n",
    "        Int32(num_items(\"manga\")),\n",
    "        Float32(11),\n",
    "        Float32(1),\n",
    "        Int32(5),\n",
    "        Float32(1),\n",
    "        Int32(maximum(num_users(x) for x in media)),\n",
    "        Int32(512), # todo increase\n",
    "    )\n",
    "    d = Dict(\n",
    "        # tokenization\n",
    "        \"base_vocab_sizes\" => base_vocab_sizes,\n",
    "        \"cls_tokens\" => base_vocab_sizes .+ Int32(1),\n",
    "        \"pad_tokens\" => base_vocab_sizes .+ Int32(2),\n",
    "        \"mask_tokens\" => base_vocab_sizes .+ Int32(3),\n",
    "        \"empty_tokens\" => base_vocab_sizes .+ Int32(4),\n",
    "        \"vocab_sizes\" => base_vocab_sizes .+ Int32(4),\n",
    "        # data\n",
    "        \"max_document_length\" => Inf,\n",
    "        \"include_ptw_impressions\" => true,\n",
    "        \"explicit_baseline\" => Dict(\n",
    "            Symbol(x) =>\n",
    "                [read_params(\"$x/$t/ExplicitUserItemBiases\") for t in ALL_TASKS] for\n",
    "            x in [\"anime\", \"manga\"]\n",
    "        ),\n",
    "        \"media\" => media,\n",
    "        \"chunk_size\" => 2^16,\n",
    "        # model\n",
    "        \"max_sequence_length\" => extract(base_vocab_sizes, :position),\n",
    "    )\n",
    "    @assert d[\"max_document_length\"] >= d[\"max_sequence_length\"]\n",
    "    for (k, v) in d[\"explicit_baseline\"]\n",
    "        for i = 1:length(ALL_TASKS)\n",
    "            v[i][\"weights\"] = powerdecay(\n",
    "                get_counts(\n",
    "                    \"training\",\n",
    "                    \"all\",\n",
    "                    \"explicit\",\n",
    "                    String(k),\n",
    "                    by_item = true,\n",
    "                    per_rating = false,\n",
    "                ),\n",
    "                log(v[i][\"λ\"][4]),\n",
    "            )\n",
    "            @assert length(v[i]) == 4\n",
    "            @assert length(v[i][\"λ\"]) == 5\n",
    "        end\n",
    "    end\n",
    "    d\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e8899-6268-4081-b7d1-d2b09a4b63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_epoch_size!(training_config, training_sentences, validation_sentences)\n",
    "    num_tokens =\n",
    "        sum(min.(length.(training_sentences), training_config[\"max_document_length\"]))\n",
    "    @info \"Number of training sentences: $(length(training_sentences))\"\n",
    "    @info \"Number of training tokens: $(num_tokens)\"\n",
    "    training_config[\"tokens_per_epoch\"] = Int(num_tokens)\n",
    "    training_config[\"num_training_sentences\"] = length(training_sentences)\n",
    "    training_config[\"num_validation_sentences\"] = length(validation_sentences)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31c5b2-42e2-455a-bd3f-0ca4eab8cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function setup_training(config, outdir)\n",
    "    if !isdir(outdir)\n",
    "        mkdir(outdir)\n",
    "    end\n",
    "    for x in readdir(outdir, join = true)\n",
    "        if isfile(x)\n",
    "            rm(x)\n",
    "        end\n",
    "    end\n",
    "    fn = joinpath(outdir, \"config.json\")\n",
    "    open(fn * \"~\", \"w\") do f\n",
    "        write(f, JSON.json(config))\n",
    "    end\n",
    "    mv(fn * \"~\", fn)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fa056-8deb-486d-9854-6e0991bc4714",
   "metadata": {},
   "source": [
    "# Disk I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7abb52-f79b-4e89-8611-aa1506d2808e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function save_features(sentences, config, rng, training, outfile)\n",
    "    if training\n",
    "        sentences = shuffle_training_data(\n",
    "            rng,\n",
    "            sentences,\n",
    "            config[\"max_sequence_length\"],\n",
    "            config[\"max_document_length\"],\n",
    "        )\n",
    "    end\n",
    "\n",
    "    features = []\n",
    "    for x in sentences\n",
    "        push!(features, featurize(x, config, rng, training))\n",
    "    end\n",
    "\n",
    "    d = Dict{String,AbstractArray}()\n",
    "    collate = Flux.batch\n",
    "\n",
    "    embed_names = [\n",
    "        \"anime\",\n",
    "        \"manga\",\n",
    "        \"rating\",\n",
    "        \"timestamp\",\n",
    "        \"status\",\n",
    "        \"completion\",\n",
    "        \"user\",\n",
    "        \"position\",\n",
    "    ]\n",
    "    for (i, name) in Iterators.enumerate(embed_names)\n",
    "        d[name] = collate([x[1][i] for x in features])\n",
    "    end\n",
    "    for medium in [\"anime\", \"manga\"]\n",
    "        for task in [\"item\", \"rating\"]\n",
    "            d[\"positions_$(medium)_$(task)\"] =\n",
    "                collate([x[2][Symbol(medium)][Symbol(task)] for x in features])\n",
    "        end\n",
    "    end\n",
    "    for medium in [\"anime\", \"manga\"]\n",
    "        for task in [\"item\", \"rating\"]\n",
    "            d[\"labels_$(medium)_$(task)\"] =\n",
    "                collate([x[3][Symbol(medium)][Symbol(task)] for x in features])\n",
    "        end\n",
    "    end\n",
    "    for medium in [\"anime\", \"manga\"]\n",
    "        for task in [\"item\", \"rating\"]\n",
    "            d[\"weights_$(medium)_$(task)\"] =\n",
    "                collate([x[4][Symbol(medium)][Symbol(task)] for x in features])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    HDF5.h5open(outfile, \"w\") do file\n",
    "        for (k, v) in d\n",
    "            write(file, k, v)\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8cc95c-b656-49d2-a65f-1504fbe4b50e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function spawn_feature_workers(sentences, workers, config, rng, training, outdir)\n",
    "    # writes data to \"$outdir/data.$worker.h5\" in a hot loop\n",
    "    # whenever that file disappears, we populate it with a new batch\n",
    "    # we stop when the file \"$outdir/finished\" appears\n",
    "    chunk_size = config[\"chunk_size\"]\n",
    "    finished = joinpath(outdir, \"finished\")\n",
    "    stem = training ? \"training\" : \"validation\"\n",
    "    rngs = [Random.Xoshiro(rand(rng, UInt64)) for _ = 1:workers]\n",
    "    @sync for (i, batch) in Iterators.enumerate(\n",
    "        Iterators.partition(sentences, div(length(sentences), workers, RoundUp)),\n",
    "    )\n",
    "        Threads.@spawn begin\n",
    "            rng = rngs[i]\n",
    "            while true\n",
    "                Random.shuffle!(rng, batch)\n",
    "                for (j, chunk) in\n",
    "                    Iterators.enumerate(Iterators.partition(batch, chunk_size))\n",
    "                    GC.gc()\n",
    "                    fn = joinpath(outdir, \"$stem.$i.h5\")\n",
    "                    while isfile(fn) && !isfile(finished)\n",
    "                        sleep(1)\n",
    "                    end\n",
    "                    if isfile(finished)\n",
    "                        break\n",
    "                    end\n",
    "                    save_features(chunk, config, rng, training, fn)\n",
    "                    open(\"$fn.complete\", \"w\") do f\n",
    "                        write(f, \"$j\")\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c19b7-612d-464b-a830-519ad2c4924a",
   "metadata": {},
   "source": [
    "# State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bd779-8b8f-4027-ba80-34d147d90a4d",
   "metadata": {
    "papermill": {
     "duration": 1.708366,
     "end_time": "2023-05-15T05:04:49.317062",
     "exception": false,
     "start_time": "2023-05-15T05:04:47.608696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_checkpoint = nothing\n",
    "config_epoch = nothing\n",
    "reset_lr_schedule = true\n",
    "rng = set_rngs(20221221)\n",
    "config = create_training_config()\n",
    "num_workers = 8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece189f1-11f9-4774-b519-635444144d93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-05-15T05:04:49.320685",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@info \"loading data\"\n",
    "training_sentences, validation_sentences = get_sentences(rng, config)\n",
    "set_epoch_size!(config, training_sentences, validation_sentences);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb0a73-ced6-4e7c-abe3-bfadf15a08b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "outdir = get_data_path(joinpath(\"alphas\", name, \"training\"));\n",
    "config[\"num_workers\"] = num_workers\n",
    "setup_training(config, outdir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40387edf-e126-4c06-a0ad-48b749fc0d35",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Threads.@spawn spawn_feature_workers(\n",
    "    training_sentences,\n",
    "    num_workers,\n",
    "    config,\n",
    "    rng,\n",
    "    true,\n",
    "    outdir,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa999b-7bc5-4f94-a562-cf0a773eaecc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Threads.@spawn spawn_feature_workers(\n",
    "    validation_sentences,\n",
    "    num_workers,\n",
    "    config,\n",
    "    rng,\n",
    "    false,\n",
    "    outdir,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367e414-81c9-44c3-b471-ca79920be9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_nb = \"$(pwd())/PretrainPytorch.ipynb\"\n",
    "dest_nb = \"$(outdir)/PretrainPytorch.ipynb\"\n",
    "cmd = `papermill $source_nb $dest_nb --no-progress-bar -p name $name`\n",
    "run(cmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0-rc2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
