{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e701df7-0d06-4631-a382-8228a4d3a845",
   "metadata": {},
   "source": [
    "# Pretraining\n",
    "* Trains a BERT-style transformer model on watch histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90afc10-1ae5-4665-937e-d58a5859ac94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Data.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd56259-4513-4297-a978-99c765532231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"all/Transformer/v1\"\n",
    "set_logging_outdir(name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d60ae-6b55-4274-85b1-c0f9c9fc7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import HDF5\n",
    "import JSON\n",
    "import MLUtils\n",
    "import Random\n",
    "import StatsBase: mean, sample\n",
    "import ThreadPinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0226705-760c-4696-9d9a-4331d9ed4d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThreadPinning.pinthreads(:cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6434e07-f565-474a-ac05-12c409afe141",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed0909-a7b0-4b54-ae7b-20dd5b9b789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurize(sentence::Vector{wordtype}, config, rng) = featurize(;\n",
    "    sentence = sentence,\n",
    "    max_seq_len = config[\"max_sequence_length\"],\n",
    "    vocab_sizes = config[\"base_vocab_sizes\"],\n",
    "    pad_tokens = config[\"pad_tokens\"],\n",
    "    cls_tokens = config[\"cls_tokens\"],\n",
    "    mask_tokens = config[\"mask_tokens\"],\n",
    "    rng = rng,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b524c73-58a0-4c31-9ff0-48f81eb0fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function featurize(;\n",
    "    sentence::Vector{wordtype},\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    pad_tokens,\n",
    "    cls_tokens,\n",
    "    mask_tokens,\n",
    "    rng,\n",
    ")\n",
    "    sentence = subset_sentence(sentence, max_seq_len; recent = false, rng = rng)\n",
    "    tokens = get_token_ids(sentence, max_seq_len, pad_tokens, cls_tokens)\n",
    "\n",
    "    positions = Dict(\n",
    "        x => Dict(y => zeros(Int32, max_seq_len) for y in ALL_METRICS) for x in ALL_MEDIUMS\n",
    "    )\n",
    "    weights = Dict(\n",
    "        x => Dict(y => zeros(Float32, max_seq_len) for y in ALL_METRICS) for\n",
    "        x in ALL_MEDIUMS\n",
    "    )\n",
    "    labels = Dict(\n",
    "        x => Dict(y => zeros(Float32, max_seq_len) for y in ALL_METRICS) for\n",
    "        x in ALL_MEDIUMS\n",
    "    )\n",
    "    userids = Dict(\n",
    "        x => Dict(y => zeros(Int32, max_seq_len) for y in ALL_METRICS) for x in ALL_MEDIUMS\n",
    "    )\n",
    "    for i::Int32 = 1:max_seq_len\n",
    "        is_manga = (extract(tokens, :itemid)[i] < num_items(\"manga\"))\n",
    "        is_anime =\n",
    "            num_items(\"manga\") <=\n",
    "            extract(tokens, :itemid)[i] <\n",
    "            num_items(\"manga\") + num_items(\"anime\")\n",
    "        has_rating =\n",
    "            0 < extract(tokens, :rating)[i] <= vocab_sizes[get_wordtype_index(:rating)]\n",
    "        has_watch =\n",
    "            get_status(:plan_to_watch) <\n",
    "            extract(tokens, :status)[i] <=\n",
    "            vocab_sizes[get_wordtype_index(:status)]\n",
    "        has_plantowatch = extract(tokens, :status)[i] == get_status(:plan_to_watch)\n",
    "        has_drop =\n",
    "            get_status(:none) <\n",
    "            extract(tokens, :status)[i] <=\n",
    "            vocab_sizes[get_wordtype_index(:status)]\n",
    "        if !(has_rating || has_watch || has_plantowatch || has_drop)\n",
    "            continue\n",
    "        end\n",
    "        @assert xor(is_manga, is_anime)\n",
    "        if is_manga\n",
    "            medium = \"manga\"\n",
    "            mediaid = extract(tokens, :itemid)[i]\n",
    "        elseif is_anime\n",
    "            medium = \"anime\"\n",
    "            mediaid = extract(tokens, :itemid)[i] - num_items(\"manga\")\n",
    "        else\n",
    "            @assert false\n",
    "        end\n",
    "\n",
    "        # mask and make predictions for 15% of tokens\n",
    "        if rand(rng) > 0.15\n",
    "            continue\n",
    "        end\n",
    "        if has_rating\n",
    "            positions[medium][\"rating\"][i] = mediaid\n",
    "            labels[medium][\"rating\"][i] = extract(tokens, :rating)[i]\n",
    "            weights[medium][\"rating\"][i] = 1\n",
    "            userids[medium][\"rating\"][i] = extract(tokens, :userid)[i]\n",
    "        end\n",
    "        if has_watch\n",
    "            positions[medium][\"watch\"][i] = mediaid\n",
    "            labels[medium][\"watch\"][i] = 1\n",
    "            weights[medium][\"watch\"][i] = 1\n",
    "            userids[medium][\"watch\"][i] = extract(tokens, :userid)[i]\n",
    "        end\n",
    "        if has_plantowatch\n",
    "            positions[medium][\"plantowatch\"][i] = mediaid\n",
    "            labels[medium][\"plantowatch\"][i] = 1\n",
    "            weights[medium][\"plantowatch\"][i] = 1\n",
    "            userids[medium][\"plantowatch\"][i] = extract(tokens, :userid)[i]\n",
    "        end\n",
    "        if has_drop\n",
    "            positions[medium][\"drop\"][i] = mediaid\n",
    "            labels[medium][\"drop\"][i] = extract(tokens, :status)[i] <= get_status(:dropped)\n",
    "            weights[medium][\"drop\"][i] = 1\n",
    "            userids[medium][\"drop\"][i] = extract(tokens, :userid)[i]\n",
    "        end\n",
    "\n",
    "        # do bert masking        \n",
    "        r = rand(rng)\n",
    "        if r <= 0.8\n",
    "            for j = 1:length(tokens)\n",
    "                if j ∉ get_wordtype_index.([:userid, :position])\n",
    "                    tokens[j][i] = mask_tokens[j]\n",
    "                end\n",
    "            end\n",
    "        elseif r <= 0.9\n",
    "            for j = 1:length(tokens)\n",
    "                if j ∉ get_wordtype_index.([:userid, :position])\n",
    "                    if eltype(vocab_sizes[j]) == Int32\n",
    "                        tokens[j][i] = rand(rng, 0:vocab_sizes[j]-1)\n",
    "                    elseif eltype(tokens[j]) == Float32\n",
    "                        tokens[j][i] = rand(rng) * vocab_sizes[j]\n",
    "                    else\n",
    "                        @assert false\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        else\n",
    "            nothing\n",
    "        end\n",
    "    end\n",
    "\n",
    "    tokens, positions, labels, weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7ebe9-122d-4b77-a00b-87206686e4b3",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0deae8-f55a-4293-8ebb-84011b487f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "function shuffle_training_data(rng, sentences, max_sequence_length, max_document_length)\n",
    "    order = Random.shuffle(rng, 1:length(sentences))\n",
    "    S = eltype(sentences)\n",
    "    W = eltype(sentences[1])\n",
    "\n",
    "    # patition tokens into minibatches\n",
    "    batched_sentences = Vector{S}()\n",
    "    sentence = Vector{W}()\n",
    "    for i in order\n",
    "        subset =\n",
    "            subset_sentence(sentences[i], max_document_length; recent = false, rng = rng)\n",
    "        for token in subset\n",
    "            push!(sentence, token)\n",
    "            if length(sentence) == max_sequence_length\n",
    "                push!(batched_sentences, sentence)\n",
    "                sentence = Vector{W}()\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    if length(sentence) > 0\n",
    "        push!(batched_sentences, sentence)\n",
    "    end\n",
    "    batched_sentences\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed260a-b454-4690-ab8f-15ba4d686b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sentences(rng, training_config)\n",
    "    sentences = collect(values(get_training_data(training_config[\"cls_tokens\"])))\n",
    "    Random.shuffle!(rng, sentences)\n",
    "    cutoff = Int(round(0.99 * length(sentences))) # TODO switch to 0.999 for prod\n",
    "    training = sentences[1:cutoff]\n",
    "    validation = sentences[cutoff+1:end]\n",
    "    training, validation\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50183c75-f670-425e-b08e-c72efe4e8988",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ac800-38d2-4ed9-9963-05c176baef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_rngs(seed)\n",
    "    rng = Random.Xoshiro(seed)\n",
    "    Random.seed!(rand(rng, UInt64))\n",
    "    rng\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb67b6-dabb-4db8-841d-c7ebf43f4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_training_config()\n",
    "    # itemid, rating, updated_at, status, position, userid\n",
    "    max_sequence_length = 1024\n",
    "    base_vocab_sizes = (\n",
    "        Int32(sum(num_items.(ALL_MEDIUMS)) - 1),\n",
    "        Float32(10),\n",
    "        Float32(1),\n",
    "        Int32(get_status(:rewatching)),\n",
    "        Int32(max_sequence_length - 1),\n",
    "        Int32(num_users() - 1),\n",
    "    )\n",
    "    d = Dict(\n",
    "        # tokenization\n",
    "        \"base_vocab_sizes\" => base_vocab_sizes,\n",
    "        \"cls_tokens\" => base_vocab_sizes .+ Int32(1),\n",
    "        \"pad_tokens\" => base_vocab_sizes .+ Int32(2),\n",
    "        \"mask_tokens\" => base_vocab_sizes .+ Int32(3),\n",
    "        \"vocab_sizes\" => base_vocab_sizes .+ Int32(4),\n",
    "        \"vocab_types\" => (\"int\", \"float\", \"float\", \"int\", \"int\", \"none\"),\n",
    "        \"media_sizes\" => Dict(m => num_items(m) for m in ALL_MEDIUMS),\n",
    "        # data\n",
    "        \"max_document_length\" => Inf, # TODO experiment with subsampling\n",
    "        \"chunk_size\" => 2^14,\n",
    "        \"num_training_shards\" => 24,\n",
    "        \"num_validation_shards\" => 8,\n",
    "        # model\n",
    "        \"max_sequence_length\" => max_sequence_length,\n",
    "        \"mode\" => \"pretrain\",\n",
    "    )\n",
    "    @assert d[\"max_document_length\"] >= d[\"max_sequence_length\"]\n",
    "    @assert length(d[\"vocab_sizes\"]) == length(d[\"vocab_types\"])\n",
    "    d\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e8899-6268-4081-b7d1-d2b09a4b63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_epoch_size!(config, training_sentences, validation_sentences)\n",
    "    num_training_tokens =\n",
    "        Int64(sum(min.(length.(training_sentences), config[\"max_document_length\"])))\n",
    "    num_validation_tokens =\n",
    "        Int64(sum(min.(length.(validation_sentences), config[\"max_document_length\"])))\n",
    "    @info \"Number of training tokens: $(num_training_tokens)\"\n",
    "    @info \"Number of training sentences: $(length(training_sentences))\"\n",
    "    @info \"Number of validation tokens: $(num_validation_tokens)\"\n",
    "    @info \"Number of validation sentences: $(length(validation_sentences))\"\n",
    "    config[\"training_epoch_size\"] =\n",
    "        div(num_training_tokens, config[\"max_sequence_length\"], RoundUp)\n",
    "    config[\"validation_epoch_size\"] =\n",
    "        div(num_validation_tokens, config[\"max_sequence_length\"], RoundUp)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31c5b2-42e2-455a-bd3f-0ca4eab8cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function setup_training(config, outdir)\n",
    "    if !isdir(outdir)\n",
    "        mkpath(outdir)\n",
    "    end\n",
    "    for x in readdir(outdir, join = true)\n",
    "        if isfile(x)\n",
    "            rm(x)\n",
    "        end\n",
    "    end\n",
    "    fn = joinpath(outdir, \"..\", \"config.json\")\n",
    "    open(fn, \"w\") do f\n",
    "        write(f, JSON.json(config))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fa056-8deb-486d-9854-6e0991bc4714",
   "metadata": {},
   "source": [
    "# Disk I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7abb52-f79b-4e89-8611-aa1506d2808e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function save_features(sentences, config, rng, filename)\n",
    "    sentences = shuffle_training_data(\n",
    "        rng,\n",
    "        sentences,\n",
    "        config[\"max_sequence_length\"],\n",
    "        config[\"max_document_length\"],\n",
    "    )\n",
    "    features = []\n",
    "    for x in sentences\n",
    "        push!(features, featurize(x, config, rng))\n",
    "    end\n",
    "\n",
    "    d = Dict{String,AbstractArray}()\n",
    "    collate = MLUtils.batch\n",
    "\n",
    "    embed_names = [\"itemid\", \"rating\", \"updated_at\", \"status\", \"position\", \"userid\"]\n",
    "    for (i, name) in Iterators.enumerate(embed_names)\n",
    "        d[name] = collate([x[1][i] for x in features])\n",
    "    end\n",
    "    for medium in ALL_MEDIUMS\n",
    "        for metric in ALL_METRICS\n",
    "            stem = \"$(medium)_$(metric)\"\n",
    "            d[\"positions_$stem\"] = collate([x[2][medium][metric] for x in features])\n",
    "            d[\"labels_$stem\"] = collate([x[3][medium][metric] for x in features])\n",
    "            d[\"weights_$stem\"] = collate([x[4][medium][metric] for x in features])\n",
    "        end\n",
    "    end\n",
    "    HDF5.h5open(filename, \"w\") do file\n",
    "        for (k, v) in d\n",
    "            write(file, k, v)\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6bfa77-0429-4aa0-9bdd-bbbc93a405c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "function advance!(filename)\n",
    "    # check to see if we should write the next shard\n",
    "    outdir = dirname(filename)\n",
    "    files = readdir(outdir)\n",
    "    suffix = basename(filename) * \".read\"\n",
    "    files = [x for x in files if occursin(suffix, basename(x))]\n",
    "    if length(files) == 0\n",
    "        return false\n",
    "    end\n",
    "    world_sizes = Set(split(x, \".\")[end] for x in files)\n",
    "    @assert length(world_sizes) == 1\n",
    "    world_size = parse(Int, first(world_sizes))\n",
    "    advance = length(files) == world_size\n",
    "    if advance\n",
    "        rm(\"$filename.complete\")\n",
    "        rm(filename)\n",
    "        for x in files\n",
    "            rm(joinpath(outdir, x))\n",
    "        end\n",
    "    end\n",
    "    advance\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8cc95c-b656-49d2-a65f-1504fbe4b50e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function spawn_feature_workers(sentences, config, rng, training, outdir)\n",
    "    # writes data to \"$outdir/training/$split.$worker.h5\" in a loop\n",
    "    chunk_size = config[\"chunk_size\"]\n",
    "    workers = training ? config[\"num_training_shards\"] : config[\"num_validation_shards\"]\n",
    "    stem = training ? \"training\" : \"validation\"\n",
    "    rngs = [Random.Xoshiro(rand(rng, UInt64)) for _ = 1:workers]\n",
    "    for (i, batch) in Iterators.enumerate(\n",
    "        Iterators.partition(sentences, div(length(sentences), workers, RoundUp)),\n",
    "    )\n",
    "        Threads.@spawn begin\n",
    "            rng = rngs[i]\n",
    "            while true\n",
    "                Random.shuffle!(rng, batch)\n",
    "                for (j, chunk) in\n",
    "                    Iterators.enumerate(Iterators.partition(batch, chunk_size))\n",
    "                    filename = joinpath(outdir, \"$stem.$i.h5\")\n",
    "                    save_features(chunk, config, rng, filename)\n",
    "                    open(\"$filename.complete\", \"w\") do f\n",
    "                        write(f, \"$j\")\n",
    "                    end\n",
    "                    if i == 1\n",
    "                        GC.gc()\n",
    "                    end\n",
    "                    while isdir(outdir) && !advance!(filename)\n",
    "                        sleep(1)\n",
    "                    end\n",
    "                    if !isdir(outdir)\n",
    "                        return\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c19b7-612d-464b-a830-519ad2c4924a",
   "metadata": {},
   "source": [
    "# State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bd779-8b8f-4027-ba80-34d147d90a4d",
   "metadata": {
    "papermill": {
     "duration": 1.708366,
     "end_time": "2023-05-15T05:04:49.317062",
     "exception": false,
     "start_time": "2023-05-15T05:04:47.608696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_checkpoint = nothing\n",
    "config_epoch = nothing\n",
    "rng = set_rngs(20221221)\n",
    "config = create_training_config();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece189f1-11f9-4774-b519-635444144d93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-05-15T05:04:49.320685",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@info \"loading data\"\n",
    "training_sentences, validation_sentences = get_sentences(rng, config)\n",
    "set_epoch_size!(config, training_sentences, validation_sentences);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb0a73-ced6-4e7c-abe3-bfadf15a08b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "outdir = get_data_path(joinpath(\"alphas\", name, \"training\"))\n",
    "setup_training(config, outdir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40387edf-e126-4c06-a0ad-48b749fc0d35",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spawn_feature_workers(training_sentences, config, rng, true, outdir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa999b-7bc5-4f94-a562-cf0a773eaecc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spawn_feature_workers(validation_sentences, config, rng, false, outdir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e8e56d-06c8-4c0d-9b55-3372479b8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for workers to begin writing\n",
    "while sum(endswith.(readdir(outdir), (\".complete\",))) <\n",
    "      config[\"num_training_shards\"] + config[\"num_validation_shards\"]\n",
    "    sleep(1)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
