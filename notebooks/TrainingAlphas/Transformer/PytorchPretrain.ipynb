{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118952b2-c4a8-4b32-a6e9-4d4a0791431b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401586b-f81b-4eda-b3c0-39448e5921a2",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9cb41-31e4-47ff-98d9-b8d2b34da8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteEmbed(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(DiscreteEmbed, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0cb64-65ce-4ae7-b598-70e27b87b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousEmbed(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(ContinuousEmbed, self).__init__()\n",
    "        hidden_size = math.ceil(embed_size / 64)\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, embed_size),\n",
    "        )\n",
    "        self.scale = 1 / vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x * self.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10b23a-f9d9-4359-9bf4-fc3aaba90822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeEmbedding(nn.Module):\n",
    "    def __init__(self, embeddings, postprocessor):\n",
    "        super(CompositeEmbedding, self).__init__()\n",
    "        self.embeddings = nn.ModuleList(embeddings)\n",
    "        self.postprocessor = postprocessor\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding = sum(embed(x) for (embed, x) in zip(self.embeddings, inputs))\n",
    "        return embedding\n",
    "        # return self.postprocessor(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16660708-4fe9-41ba-b209-3612ffe4748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        embed_size,\n",
    "        num_attention_heads,\n",
    "        intermediate_size,\n",
    "        activation=\"gelu\",\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(Bert, self).__init__()\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_size,\n",
    "                nhead=num_attention_heads,\n",
    "                dim_feedforward=intermediate_size,\n",
    "                dropout=0.1,\n",
    "                activation=activation,\n",
    "                norm_first=True,\n",
    "                batch_first=True,\n",
    "            ),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # see https://stackoverflow.com/questions/68205894/how-to-prepare-data-for-tpytorchs-3d-attn-mask-argument-in-multiheadattention\n",
    "        # for why torch.repeat_interleave is necessary\n",
    "        mask = torch.repeat_interleave(mask, self.num_heads, dim=0)\n",
    "        return self.encoder(x, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd78128-1494-4fc6-a46f-d858c9f800bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # create embeddings\n",
    "        embeddings = []\n",
    "        for size, dtype in zip(config[\"vocab_sizes\"], config[\"vocab_types\"]):\n",
    "            if dtype is None:\n",
    "                continue\n",
    "            elif dtype == int:\n",
    "                embeddings.append(DiscreteEmbed(size, config[\"embed_size\"]))\n",
    "            elif dtype == float:\n",
    "                embeddings.append(ContinuousEmbed(size, config[\"embed_size\"]))\n",
    "            else:\n",
    "                assert False\n",
    "        postprocessor = nn.Sequential(\n",
    "            nn.LayerNorm(config[\"embed_size\"]), nn.Dropout(config[\"dropout\"])\n",
    "        )\n",
    "        self.embed = CompositeEmbedding(embeddings, postprocessor)\n",
    "\n",
    "        # create transformers\n",
    "        self.transformers = Bert(\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            embed_size=config[\"embed_size\"],\n",
    "            num_attention_heads=config[\"num_attention_heads\"],\n",
    "            intermediate_size=config[\"intermediate_size\"],\n",
    "            activation=config[\"activation\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "        )\n",
    "\n",
    "        # create classifier\n",
    "        self.classifier = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        config[\"embed_size\"],\n",
    "                        config[\"vocab_sizes\"][config[\"vocab_names\"].index(\"anime\")],\n",
    "                    ),\n",
    "                    nn.Softmax(dim=-1),\n",
    "                ),\n",
    "                nn.Linear(\n",
    "                    config[\"embed_size\"],\n",
    "                    config[\"vocab_sizes\"][config[\"vocab_names\"].index(\"anime\")],\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        config[\"embed_size\"],\n",
    "                        config[\"vocab_sizes\"][config[\"vocab_names\"].index(\"manga\")],\n",
    "                    ),\n",
    "                    nn.Softmax(dim=-1),\n",
    "                ),\n",
    "                nn.Linear(\n",
    "                    config[\"embed_size\"],\n",
    "                    config[\"vocab_sizes\"][config[\"vocab_names\"].index(\"manga\")],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.losses = [\"mse\", \"mse\", \"crossentropy\", \"crossentropy\"]\n",
    "\n",
    "    def crossentropy_loss(self, e, classifier, positions, labels, weights):\n",
    "        weight_sum = weights.sum()\n",
    "        if not torch.is_nonzero(weight_sum):\n",
    "            return weight_sum\n",
    "        preds = classifier(e).gather(dim=-1, index=positions)\n",
    "        loss = (-torch.log(preds) * labels * weights).sum() / weight_sum\n",
    "        return loss\n",
    "\n",
    "    def rating_loss(self, e, classifier, positions, labels, weights):\n",
    "        weight_sum = weights.sum()\n",
    "        if not torch.is_nonzero(weight_sum):\n",
    "            return weight_sum\n",
    "        preds = classifier(e).gather(dim=-1, index=positions)\n",
    "        loss = (torch.square(preds - labels) * weights).sum() / weight_sum\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, mask, positions, labels, weights):\n",
    "        e = self.embed(inputs)\n",
    "        e = self.transformers(e, mask)\n",
    "        anime_item_loss = self.crossentropy_loss(\n",
    "            e, self.classifier[0], positions[0], labels[0], weights[0]\n",
    "        )\n",
    "        anime_rating_loss = self.rating_loss(\n",
    "            e, self.classifier[1], positions[1], labels[1], weights[1]\n",
    "        )\n",
    "        manga_item_loss = self.crossentropy_loss(\n",
    "            e, self.classifier[2], positions[2], labels[2], weights[2]\n",
    "        )\n",
    "        manga_rating_loss = self.rating_loss(\n",
    "            e, self.classifier[3], positions[3], labels[3], weights[3]\n",
    "        )\n",
    "        return anime_item_loss, anime_rating_loss, manga_item_loss, manga_rating_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e0c0a-29c7-4066-9daa-846640ae452a",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa25e1-d6ff-4420-91fc-85bf9b563e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_config(config_file):\n",
    "    config = json.load(open(config_file, \"r\"))\n",
    "    config = {\n",
    "        # tokenization\n",
    "        \"vocab_sizes\": config[\"vocab_sizes\"],\n",
    "        \"vocab_types\": [int, int, float, float, int, float, None, int],\n",
    "        \"vocab_names\": [\n",
    "            \"anime\",\n",
    "            \"manga\",\n",
    "            \"rating\",\n",
    "            \"timestamp\",\n",
    "            \"status\",\n",
    "            \"completion\",\n",
    "            \"user\",\n",
    "            \"position\",\n",
    "        ],\n",
    "        # model\n",
    "        \"num_layers\": 4,\n",
    "        \"hidden_size\": 512,\n",
    "        \"max_sequence_length\": config[\"max_sequence_length\"],\n",
    "        # training\n",
    "        \"peak_learning_rate\": 3e-4,\n",
    "        \"weight_decay\": 1e-2,\n",
    "        \"num_epochs\": 1,\n",
    "        \"tokens_per_epoch\": config[\"tokens_per_epoch\"],\n",
    "        \"num_validation_sentences\": config[\"num_validation_sentences\"],\n",
    "        \"batch_size\": 16,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "    }\n",
    "    assert len(config[\"vocab_sizes\"]) == len(config[\"vocab_types\"])\n",
    "    assert len(config[\"vocab_sizes\"]) == len(config[\"vocab_names\"])\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8017cd6d-5352-430f-8708-d882c75e0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_config(training_config):\n",
    "    return {\n",
    "        \"dropout\": 0.1,\n",
    "        \"activation\": \"gelu\",\n",
    "        \"num_layers\": training_config[\"num_layers\"],\n",
    "        \"embed_size\": training_config[\"hidden_size\"],\n",
    "        \"max_sequence_length\": training_config[\"max_sequence_length\"],\n",
    "        \"vocab_sizes\": training_config[\"vocab_sizes\"],\n",
    "        \"vocab_types\": training_config[\"vocab_types\"],\n",
    "        \"vocab_names\": training_config[\"vocab_names\"],\n",
    "        \"num_attention_heads\": int(training_config[\"hidden_size\"] / 64),\n",
    "        \"intermediate_size\": training_config[\"hidden_size\"] * 4,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab959d8c-d816-42db-9cc8-02ff0abd9b99",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff0a64-ec41-433b-a134-fa7d17d39d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.filename = file\n",
    "        f = h5py.File(file, \"r\")\n",
    "        self.length = f[\"anime\"].shape[0]\n",
    "        self.embeddings = [\n",
    "            f[\"anime\"][:] - 1,\n",
    "            f[\"manga\"][:] - 1,\n",
    "            f[\"rating\"][:].reshape(*f[\"rating\"].shape, 1),\n",
    "            f[\"timestamp\"][:].reshape(*f[\"timestamp\"].shape, 1),\n",
    "            f[\"status\"][:] - 1,\n",
    "            f[\"completion\"][:].reshape(*f[\"completion\"].shape, 1),\n",
    "            f[\"position\"][:] - 1,\n",
    "        ]\n",
    "        self.mask = f[\"mask\"][:]\n",
    "\n",
    "        def process_position(x):\n",
    "            x = x[:].astype(np.int64) - 1\n",
    "            return x.reshape(*x.shape, 1)\n",
    "\n",
    "        self.positions = [\n",
    "            process_position(f[\"positions_anime_item\"]),\n",
    "            process_position(f[\"positions_anime_rating\"]),\n",
    "            process_position(f[\"positions_manga_item\"]),\n",
    "            process_position(f[\"positions_manga_rating\"]),\n",
    "        ]\n",
    "        self.labels = [\n",
    "            np.expand_dims(f[\"labels_anime_item\"][:], axis=-1),\n",
    "            np.expand_dims(f[\"labels_anime_rating\"][:], axis=-1),\n",
    "            np.expand_dims(f[\"labels_manga_item\"][:], axis=-1),\n",
    "            np.expand_dims(f[\"labels_manga_rating\"][:], axis=-1),\n",
    "        ]\n",
    "        self.weights = [\n",
    "            np.expand_dims(f[\"weights_anime_item\"][:], axis=-1),\n",
    "            np.expand_dims(f[\"weights_anime_rating\"][:], axis=-1),\n",
    "            np.expand_dims(f[\"weights_manga_item\"][:], axis=-1),\n",
    "            np.expand_dims(f[\"weights_manga_rating\"][:], axis=-1),\n",
    "        ]\n",
    "        # TODO remove the disk\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        embeds = [\n",
    "            self.embeddings[0][i, :],\n",
    "            self.embeddings[1][i, :],\n",
    "            self.embeddings[2][i, :, :],\n",
    "            self.embeddings[3][i, :, :],\n",
    "            self.embeddings[4][i, :],\n",
    "            self.embeddings[5][i, :, :],\n",
    "            self.embeddings[6][i, :],\n",
    "        ]\n",
    "\n",
    "        # a true value means that the tokens will not attend to each other\n",
    "        mask = self.mask[i, :]\n",
    "        mask = mask.reshape(1, mask.size) != mask.reshape(mask.size, 1)\n",
    "\n",
    "        positions = [\n",
    "            self.positions[0][:][i, :],\n",
    "            self.positions[1][:][i, :],\n",
    "            self.positions[2][:][i, :],\n",
    "            self.positions[3][:][i, :],\n",
    "        ]\n",
    "        labels = [\n",
    "            self.labels[0][:][i, :],\n",
    "            self.labels[1][:][i, :],\n",
    "            self.labels[2][:][i, :],\n",
    "            self.labels[3][:][i, :],\n",
    "        ]\n",
    "        weights = [\n",
    "            self.weights[0][:][i, :],\n",
    "            self.weights[1][:][i, :],\n",
    "            self.weights[2][:][i, :],\n",
    "            self.weights[3][:][i, :],\n",
    "        ]\n",
    "        return embeds, mask, positions, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8255459-d1e6-4f4a-9d23-8d3f580c04df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73628ae-05bd-4bd6-bd2e-3a11dd306745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloader(outdir, split):\n",
    "    completed = []\n",
    "    while not completed:\n",
    "        time.sleep(1)\n",
    "        completed = glob.glob(\n",
    "            os.path.join(outdir, \"training\", f\"{split}.*.h5.complete\")\n",
    "        )\n",
    "    completion_file = random.choice(completed)\n",
    "    data_file = completion_file[: -len(\".complete\")]\n",
    "    dataloader = DataLoader(\n",
    "        dataset=PretrainDataset(data_file),\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    os.remove(completion_file)\n",
    "    os.remove(data_file)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71ae1e-0119-4b3a-b14c-a15246d62d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_path(file):\n",
    "    path = os.getcwd()\n",
    "    while os.path.basename(path) != \"notebooks\":\n",
    "        path = os.path.dirname(path)\n",
    "    path = os.path.dirname(path)\n",
    "    return os.path.join(path, \"data\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbaa0a-1e6f-409b-ab52-f0efb77a4170",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a8a87-84dc-4dbf-bb9e-7675774d1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, config):\n",
    "    decay_parameters = []\n",
    "    no_decay_parameters = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(\"embed\") or \"norm\" in name or \"bias\" in name:\n",
    "            no_decay_parameters.append(param)\n",
    "        else:\n",
    "            decay_parameters.append(param)\n",
    "\n",
    "    return optim.AdamW(\n",
    "        [\n",
    "            {\"params\": decay_parameters, \"weight_decay\": config[\"weight_decay\"]},\n",
    "            {\"params\": no_decay_parameters, \"weight_decay\": 0.0},\n",
    "        ],\n",
    "        lr=config[\"peak_learning_rate\"],\n",
    "        betas=(0.9, 0.999),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08491998-a01f-46e2-a471-393c76c01f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learning_rate_schedule(optimizer, config):\n",
    "    steps_per_epoch = int(\n",
    "        math.ceil(\n",
    "            config[\"tokens_per_epoch\"]\n",
    "            / (config[\"batch_size\"] * config[\"max_sequence_length\"])\n",
    "        )\n",
    "    )\n",
    "    total_steps = config[\"num_epochs\"] * steps_per_epoch\n",
    "    warmup_ratio = config[\"warmup_ratio\"]\n",
    "    warmup_steps = int(math.ceil(total_steps * warmup_ratio))\n",
    "    warmup_lambda = (\n",
    "        lambda x: x / warmup_steps\n",
    "        if x < warmup_steps\n",
    "        else 1 - (x - warmup_steps) / (total_steps - warmup_steps)\n",
    "    )\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, warmup_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b32994-14bc-4000-a7e1-1a5f686772c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, config, outdir):\n",
    "    losses = [0.0 for _ in range(4)]\n",
    "    steps = 0\n",
    "    sentences_remaining = config[\"num_validation_sentences\"]\n",
    "    while sentences_remaining > 0:\n",
    "        dataloader = get_dataloader(outdir, \"validation\")\n",
    "        sentences_remaining -= len(dataloader)\n",
    "        for data in dataloader:\n",
    "            loss = model(*to_device(data, device))\n",
    "            for i in range(len(losses)):\n",
    "                losses[i] += loss[i].item()\n",
    "            steps += 1\n",
    "    for i in range(len(losses)):\n",
    "        losses[i] /= steps\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6580c90c-662d-4574-94ed-0625290f5f8c",
   "metadata": {},
   "source": [
    "# Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811e03c-1b31-4b07-8b43-e5bda822a485",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3dd45a-8090-4261-b4e9-11c7f138e3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load configs\n",
    "outdir = get_data_path(os.path.join(\"alphas\", name))\n",
    "config_file = os.path.join(outdir, \"training\", \"config.json\")\n",
    "training_config = create_training_config(config_file)\n",
    "model_config = create_model_config(training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f7ad6-594a-4e09-a745-f9609e0cca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ba8f3-2913-4d56-accc-af4d9326456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(model_config).to(device)\n",
    "optimizer = create_optimizer(model, training_config)\n",
    "scheduler = create_learning_rate_schedule(optimizer, training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7cb17-a3dd-40e6-a603-097887acddfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(training_config[\"num_epochs\"]):\n",
    "    training_loss = 0.0\n",
    "    training_steps = 0\n",
    "    tokens_remaining = training_config[\"tokens_per_epoch\"]\n",
    "    while tokens_remaining > 0:\n",
    "        dataloader = get_dataloader(outdir, \"training\")\n",
    "        tokens_remaining -= (\n",
    "            len(dataloader)\n",
    "            * training_config[\"max_sequence_length\"]\n",
    "            * training_config[\"batch_size\"]\n",
    "        )\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = sum(model(*to_device(data, device)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            training_loss += loss.item()\n",
    "            training_steps += 1\n",
    "    print(f\"Epoch: {epoch}, Training Loss: {training_loss / training_steps}\")\n",
    "    validation_loss = evaluate_metrics(model, training_config, outdir)\n",
    "    print(f\"Epoch: {epoch}, Validation Loss: {validation_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
