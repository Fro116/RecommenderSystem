{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9ffa83-ba30-4d94-aa0a-827dd82eb7b0",
   "metadata": {},
   "source": [
    "# Pretrains a tranformer encoder model on watch histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c8376-2d03-4c69-a07d-0ba71801ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"all/Transformer/v19\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90afc10-1ae5-4665-937e-d58a5859ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Reference/CUDA.ipynb\")\n",
    "@nbinclude(\"Reference/Include.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d60ae-6b55-4274-85b1-c0f9c9fc7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux\n",
    "import Flux: cpu, gpu, LayerNorm, logsoftmax\n",
    "import Random\n",
    "import StatsBase: mean, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002401ce-b05d-411b-8583-0d7fe34893e3",
   "metadata": {},
   "source": [
    "# Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13b747-89de-4f87-83b6-d03804ce5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Trainer\n",
    "    model::Any\n",
    "    opt::Any\n",
    "    weightdecay::Any\n",
    "    lr_schedule::Any\n",
    "    training_config::Any\n",
    "    model_config::Any\n",
    "    rng::Any\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924da24-7c19-45a8-bed0-de97e886c818",
   "metadata": {},
   "source": [
    "# Tokenize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e8d4d-1997-475f-bc5c-f489c1c45ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_training_data(media, include_ptw, cls_tokens, empty_tokens)\n",
    "    n_tasks = length(ALL_TASKS)\n",
    "    sentences = Vector{Vector{Vector{wordtype}}}(undef, n_tasks)\n",
    "    for i = 1:length(sentences)\n",
    "        data = get_training_data(ALL_TASKS[i], media, include_ptw, cls_tokens, empty_tokens)\n",
    "        sentences[i] = [data[k] for k in keys(data)]\n",
    "    end\n",
    "    vcat(sentences...)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594317f-813d-4f10-856c-a7df127e146f",
   "metadata": {},
   "source": [
    "# Create minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed0909-a7b0-4b54-ae7b-20dd5b9b789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch(sentences, training::Bool, t::Trainer) = get_batch(;\n",
    "    sentences = sentences,\n",
    "    max_seq_len = t.training_config[\"max_sequence_length\"],\n",
    "    vocab_sizes = t.training_config[\"base_vocab_sizes\"],\n",
    "    pad_tokens = t.training_config[\"pad_tokens\"],\n",
    "    cls_tokens = t.training_config[\"cls_tokens\"],\n",
    "    mask_tokens = t.training_config[\"mask_tokens\"],\n",
    "    empty_tokens = t.training_config[\"empty_tokens\"],\n",
    "    user_weighted_training = t.training_config[\"user_weighted_training\"],\n",
    "    explicit_baseline = Dict(\n",
    "        k => rand(t.rng, v) for (k, v) in t.training_config[\"explicit_baseline\"]\n",
    "    ),\n",
    "    rng = t.rng,\n",
    "    training = training,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b524c73-58a0-4c31-9ff0-48f81eb0fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_batch(;\n",
    "    sentences,\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    pad_tokens,\n",
    "    cls_tokens,\n",
    "    mask_tokens,\n",
    "    empty_tokens,\n",
    "    user_weighted_training,\n",
    "    explicit_baseline,\n",
    "    rng,\n",
    "    training,\n",
    ")\n",
    "    # dynamically pad to the largest sequence length\n",
    "    seq_len = min(maximum(length.(sentences)), max_seq_len)\n",
    "    sentences = [\n",
    "        subset_sentence(s, seq_len; recent = false, keep_first = !training, rng = rng)\n",
    "        for s in sentences\n",
    "    ]\n",
    "\n",
    "    # get tokenized sentences\n",
    "    tokens = get_token_ids(\n",
    "        sentences,\n",
    "        seq_len,\n",
    "        extract(vocab_sizes, :position),\n",
    "        pad_tokens,\n",
    "        cls_tokens,\n",
    "    )\n",
    "\n",
    "    # don't attend across sequences\n",
    "    attention_mask = zeros(Bool, (seq_len, seq_len, length(sentences)))\n",
    "    Threads.@threads for i = 1:seq_len\n",
    "        for j = 1:seq_len\n",
    "            for k = 1:length(sentences)\n",
    "                if (extract(tokens, :user)[i, k] == extract(tokens, :user)[j, k]) &&\n",
    "                   (extract(tokens, :anime)[i, k] != extract(pad_tokens, :anime)) &&\n",
    "                   (extract(tokens, :anime)[j, k] != extract(pad_tokens, :anime))\n",
    "                    attention_mask[i, j, k] = 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # demean ratings\n",
    "    if !isnothing(explicit_baseline)\n",
    "        demean = (\n",
    "            anime = (\n",
    "                rating = Dict{Int32,Float32}(),\n",
    "                count = Dict{Int32,Int32}(),\n",
    "                weight = Dict{Int32,Float32}(),\n",
    "            ),\n",
    "            manga = (\n",
    "                rating = Dict{Int32,Float32}(),\n",
    "                count = Dict{Int32,Int32}(),\n",
    "                weight = Dict{Int32,Float32}(),\n",
    "            ),\n",
    "        )\n",
    "        demean_item_weights = (\n",
    "            anime = powerdecay(\n",
    "                get_counts(\n",
    "                    \"training\",\n",
    "                    \"all\",\n",
    "                    \"explicit\",\n",
    "                    \"anime\";\n",
    "                    by_item = true,\n",
    "                    per_rating = false,\n",
    "                ),\n",
    "                log(explicit_baseline[:anime][\"λ\"][4]),\n",
    "            ),\n",
    "            manga = powerdecay(\n",
    "                get_counts(\n",
    "                    \"training\",\n",
    "                    \"all\",\n",
    "                    \"explicit\",\n",
    "                    \"manga\";\n",
    "                    by_item = true,\n",
    "                    per_rating = false,\n",
    "                ),\n",
    "                log(explicit_baseline[:manga][\"λ\"][4]),\n",
    "            ),\n",
    "        )\n",
    "    end\n",
    "\n",
    "    batch_positions = (\n",
    "        anime = (item = Tuple{Int32,Int32}[], rating = Tuple{Int32,Int32}[]),\n",
    "        manga = (item = Tuple{Int32,Int32}[], rating = Tuple{Int32,Int32}[]),\n",
    "    )\n",
    "    item_positions = (\n",
    "        anime = (item = Tuple{Int32,Int32}[], rating = Tuple{Int32,Int32}[]),\n",
    "        manga = (item = Tuple{Int32,Int32}[], rating = Tuple{Int32,Int32}[]),\n",
    "    )\n",
    "    labels = (\n",
    "        anime = (item = Int32[], rating = Float32[]),\n",
    "        manga = (item = Int32[], rating = Float32[]),\n",
    "    )\n",
    "    userids = (\n",
    "        anime = (item = Int32[], rating = Int32[]),\n",
    "        manga = (item = Int32[], rating = Int32[]),\n",
    "    )\n",
    "    for b::Int32 = 1:length(sentences)\n",
    "        for i::Int32 = 1:seq_len\n",
    "            # randomly mask 15% of non-trivial tokens \n",
    "            has_anime =\n",
    "                (extract(tokens, :anime)[i, b] <= extract(vocab_sizes, :anime)) &&\n",
    "                (extract(tokens, :status)[i, b] != get_status(:plan_to_watch))\n",
    "            has_manga =\n",
    "                (extract(tokens, :manga)[i, b] <= extract(vocab_sizes, :manga)) &&\n",
    "                (extract(tokens, :status)[i, b] != get_status(:plan_to_watch))\n",
    "            has_rating = extract(tokens, :rating)[i, b] < extract(vocab_sizes, :rating)\n",
    "            if has_anime\n",
    "                medium = :anime\n",
    "            elseif has_manga\n",
    "                medium = :manga\n",
    "            end\n",
    "            should_mask = rand(rng) < 0.15\n",
    "\n",
    "            # prepare to demean ratings\n",
    "            if !should_mask && has_rating\n",
    "                u = extract(tokens, :user)[i, b]\n",
    "                a = extract(tokens, medium)[i, b]\n",
    "                if u ∉ keys(demean[medium][:rating])\n",
    "                    demean[medium][:rating][u] = 0\n",
    "                    demean[medium][:count][u] = 0\n",
    "                    demean[medium][:weight][u] = 0\n",
    "                end\n",
    "                weight =\n",
    "                    demean_item_weights[medium][a] * powerlawdecay(\n",
    "                        1 .- cast_universal_timestamp(\n",
    "                            extract(tokens, :timestamp)[i, b],\n",
    "                            String(medium),\n",
    "                        ),\n",
    "                        explicit_baseline[medium][\"λ\"][5],\n",
    "                    )\n",
    "                demean[medium][:rating][u] +=\n",
    "                    weight *\n",
    "                    (extract(tokens, :rating)[i, b] - explicit_baseline[medium][\"a\"][a])\n",
    "                demean[medium][:count][u] += 1\n",
    "                demean[medium][:weight][u] += weight\n",
    "            end\n",
    "\n",
    "            # record tokens before we mask them out\n",
    "            if !(should_mask && (has_anime || has_manga || has_rating))\n",
    "                continue\n",
    "            end\n",
    "            if has_anime || has_manga\n",
    "                push!(batch_positions[medium][:item], (i, b))\n",
    "                push!(\n",
    "                    item_positions[medium][:item],\n",
    "                    (\n",
    "                        extract(tokens, medium)[i, b],\n",
    "                        Int32(length(item_positions[medium][:item]) + 1),\n",
    "                    ),\n",
    "                )\n",
    "                push!(userids[medium][:item], extract(tokens, :user)[i, b])\n",
    "            end\n",
    "            if has_rating\n",
    "                push!(batch_positions[medium][:rating], (i, b))\n",
    "                push!(\n",
    "                    item_positions[medium][:rating],\n",
    "                    (\n",
    "                        extract(tokens, medium)[i, b],\n",
    "                        Int32(length(item_positions[medium][:rating]) + 1),\n",
    "                    ),\n",
    "                )\n",
    "                push!(labels[medium][:rating], extract(tokens, :rating)[i, b])\n",
    "                push!(labels[medium][:item], extract(tokens, medium)[i, b])\n",
    "                push!(userids[medium][:rating], extract(tokens, :user)[i, b])\n",
    "            end\n",
    "\n",
    "            # bert masking\n",
    "            item_allowed_info =\n",
    "                get_wordtype_index.([medium, :rating, :timestamp, :position])\n",
    "            item_skip_info = get_wordtype_index.([:anime, :manga, :user])\n",
    "            for j = 1:length(tokens)\n",
    "                if j in item_allowed_info || j in item_skip_info\n",
    "                    continue\n",
    "                end\n",
    "                tokens[j][i, b] = mask_tokens[j]\n",
    "            end\n",
    "            for j in item_allowed_info\n",
    "                if j in get_wordtype_index.([medium, :rating])\n",
    "                    cutoffs = (0.8, 0.9)\n",
    "                    r = training ? rand(rng) : 0.0\n",
    "                elseif j == get_wordtype_index(:timestamp)\n",
    "                    cutoffs = (0.45, 0.9)\n",
    "                    r = training ? rand(rng) : 0.0\n",
    "                elseif j == get_wordtype_index(:position)\n",
    "                    cutoffs = (0.45, 0.9)\n",
    "                    r = training ? rand(rng) : 0.7\n",
    "                else\n",
    "                    @assert false\n",
    "                end\n",
    "                if r <= cutoffs[1]\n",
    "                    tokens[j][i, b] = mask_tokens[j]\n",
    "                elseif r <= cutoffs[2]\n",
    "                    nothing\n",
    "                else\n",
    "                    if eltype(vocab_sizes[j]) == Int32\n",
    "                        tokens[j][i, b] = rand(rng, 1:vocab_sizes[j])\n",
    "                    elseif eltype(tokens[j]) == Float32\n",
    "                        tokens[j][i, b] = rand(rng) * vocab_sizes[j]\n",
    "                    else\n",
    "                        @assert false\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # demean ratings\n",
    "    for medium in [:anime, :manga]\n",
    "        demean_explicit_ratings!(\n",
    "            tokens = tokens,\n",
    "            medium = medium,\n",
    "            demean = demean[medium],\n",
    "            explicit_baseline = explicit_baseline[medium],\n",
    "            vocab_sizes = vocab_sizes,\n",
    "            cls_tokens = cls_tokens,\n",
    "            empty_tokens = empty_tokens,\n",
    "            labels = labels[medium],\n",
    "            userids = userids[medium],\n",
    "        )\n",
    "    end\n",
    "\n",
    "    # get weights\n",
    "    processed_weights = (\n",
    "        anime = map(x -> uids_to_weights(x), userids[:anime]),\n",
    "        manga = map(x -> uids_to_weights(x), userids[:manga]),\n",
    "    )\n",
    "    if training && !user_weighted_training\n",
    "        processed_weights[:anime][:item] .= 1\n",
    "        processed_weights[:anime][:rating] .= 1\n",
    "        processed_weights[:manga][:item] .= 1\n",
    "        processed_weights[:manga][:rating] .= 1\n",
    "    end\n",
    "\n",
    "    tokens, attention_mask, batch_positions, item_positions, labels, processed_weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11afbf8-eda7-4f47-a510-cd36d320f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "function demean_explicit_ratings!(;\n",
    "    tokens,\n",
    "    medium,\n",
    "    demean,\n",
    "    explicit_baseline,\n",
    "    vocab_sizes,\n",
    "    cls_tokens,\n",
    "    empty_tokens,\n",
    "    labels,\n",
    "    userids,\n",
    ")\n",
    "    user_to_baseline = Dict{Int32,Float32}()\n",
    "    μ_user = mean(explicit_baseline[\"u\"])\n",
    "    μ_item = mean(explicit_baseline[\"a\"])\n",
    "    for u in keys(demean[:rating])\n",
    "        user_weight = powerdecay(demean[:count][u], log(explicit_baseline[\"λ\"][3]))\n",
    "        user_to_baseline[u] =\n",
    "            (demean[:rating][u] * user_weight + μ_user * explicit_baseline[\"λ\"][1]) /\n",
    "            (demean[:weight][u] * user_weight + explicit_baseline[\"λ\"][1])\n",
    "    end\n",
    "    get_user_bias(u) = u in keys(user_to_baseline) ? user_to_baseline[u] : μ_user\n",
    "    get_item_bias(a) =\n",
    "        a in keys(explicit_baseline[\"a\"]) ? explicit_baseline[\"a\"][a] : μ_item\n",
    "\n",
    "    for b::Int32 = 1:size(extract(tokens, medium))[2]\n",
    "        for i::Int32 = 1:size(extract(tokens, medium))[1]\n",
    "            if extract(tokens, medium)[i, b] == extract(empty_tokens, medium)\n",
    "                continue\n",
    "            end\n",
    "            has_explicit_rating =\n",
    "                (extract(tokens, :rating)[i, b] .< extract(vocab_sizes, :rating))\n",
    "            if has_explicit_rating\n",
    "                extract(tokens, :rating)[i, b] -=\n",
    "                    get_user_bias(extract(tokens, :user)[i, b]) +\n",
    "                    get_item_bias(extract(tokens, medium)[i, b])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    for i = 1:length(labels[:rating])\n",
    "        labels[:rating][i] -=\n",
    "            get_user_bias(userids[:rating][i]) + get_item_bias(labels[:item][i])\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f2d32-a313-4e0b-a650-0fc049e90e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function uids_to_weights(uids)\n",
    "    uid_to_count = Dict(i => 0 for i in uids)\n",
    "    for i in uids\n",
    "        uid_to_count[i] += 1\n",
    "    end\n",
    "    weights = zeros(Float32, length(uids))\n",
    "    for i = 1:length(uids)\n",
    "        weights[i] = 1 / uid_to_count[uids[i]]\n",
    "    end\n",
    "    weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c291ac-2a93-4b11-9b11-724146fb02bc",
   "metadata": {},
   "source": [
    "# Manipulate minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0deae8-f55a-4293-8ebb-84011b487f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "function shuffle_training_data(rng, sentences, max_sequence_length, max_document_length)\n",
    "    order = Random.shuffle(rng, 1:length(sentences))\n",
    "    max_sequence_length = max_sequence_length\n",
    "    max_document_length = max_document_length\n",
    "    S = eltype(sentences)\n",
    "    W = eltype(sentences[1])\n",
    "\n",
    "    # concatenate all tokens\n",
    "    tokens = Vector{W}()\n",
    "    for i in order\n",
    "        sentence = subset_sentence(\n",
    "            sentences[i],\n",
    "            max_document_length;\n",
    "            recent = false,\n",
    "            keep_first = true,\n",
    "            rng = rng,\n",
    "        )\n",
    "        for token in sentence\n",
    "            push!(tokens, token)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # patition tokens into minibatches\n",
    "    batched_sentences = Vector{S}()\n",
    "    sentence = Vector{W}()\n",
    "    for token in tokens\n",
    "        push!(sentence, token)\n",
    "        if length(sentence) == max_sequence_length\n",
    "            push!(batched_sentences, sentence)\n",
    "            sentence = Vector{W}()\n",
    "        end\n",
    "    end\n",
    "    if length(sentence) > 0\n",
    "        push!(batched_sentences, sentence)\n",
    "    end\n",
    "    batched_sentences\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46dc8a-7814-448c-a54f-051ab5ecdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "function device(x::NamedTuple)\n",
    "    fields = fieldnames(typeof(x))\n",
    "    NamedTuple{fields}(gpu(x[k]) for k in fields)\n",
    "end\n",
    "\n",
    "function device(batch)\n",
    "    gpu.(batch[1]),\n",
    "    gpu(batch[2]),\n",
    "    device(batch[3]),\n",
    "    device(batch[4]),\n",
    "    device(batch[5]),\n",
    "    device(batch[6])\n",
    "end\n",
    "\n",
    "CUDA.unsafe_free!(::Nothing) = nothing\n",
    "device_free!(x) = CUDA.unsafe_free!(x)\n",
    "function device_free!(x::NamedTuple)\n",
    "    fields = fieldnames(typeof(x))\n",
    "    for f in fields\n",
    "        device_free!(x[f])\n",
    "    end\n",
    "end\n",
    "function device_free!(batch::Tuple)\n",
    "    if !CUDA.functional()\n",
    "        return\n",
    "    end\n",
    "    CUDA.unsafe_free!.(batch[1])\n",
    "    CUDA.unsafe_free!(batch[2])\n",
    "    device_free!(batch[3])\n",
    "    device_free!(batch[4])\n",
    "    device_free!(batch[5])\n",
    "    device_free!(batch[6])\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59101cfb-147f-491b-b48a-e942ddbae8b3",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e8ee3-c369-41c8-ba85-267e2757d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_bert(config)\n",
    "    bert = Bert(\n",
    "        hidden_size = config[\"hidden_size\"],\n",
    "        num_attention_heads = config[\"num_attention_heads\"],\n",
    "        intermediate_size = config[\"intermediate_size\"],\n",
    "        num_layers = config[\"num_hidden_layers\"];\n",
    "        activation_fn = config[\"hidden_act\"],\n",
    "        dropout = config[\"dropout\"],\n",
    "        attention_dropout = config[\"attention_dropout\"],\n",
    "    )\n",
    "\n",
    "    anime_emb = DiscreteEmbed(config[\"hidden_size\"], extract(config[\"vocab_sizes\"], :anime))\n",
    "    manga_emb = DiscreteEmbed(config[\"hidden_size\"], extract(config[\"vocab_sizes\"], :manga))\n",
    "    rating_emb = ContinuousEmbed(config[\"hidden_size\"])\n",
    "    timestamp_emb = ContinuousEmbed(config[\"hidden_size\"])\n",
    "    status_emb =\n",
    "        DiscreteEmbed(config[\"hidden_size\"], extract(config[\"vocab_sizes\"], :status))\n",
    "    completion_emb = ContinuousEmbed(config[\"hidden_size\"])\n",
    "    position_emb =\n",
    "        DiscreteEmbed(config[\"hidden_size\"], extract(config[\"vocab_sizes\"], :position))\n",
    "    emb_post = Chain(LayerNorm(config[\"hidden_size\"]), Dropout(config[\"dropout\"]))\n",
    "    emb = CompositeEmbedding(\n",
    "        anime = anime_emb,\n",
    "        manga = manga_emb,\n",
    "        rating = rating_emb,\n",
    "        timestamp = timestamp_emb,\n",
    "        status = status_emb,\n",
    "        completion = completion_emb,\n",
    "        position = position_emb,\n",
    "        postprocessor = emb_post,\n",
    "    )\n",
    "\n",
    "    clf = (\n",
    "        anime = (\n",
    "            item = Dense(config[\"hidden_size\"], extract(config[\"vocab_sizes\"], :anime)),\n",
    "            rating = Dense(config[\"hidden_size\"], extract(config[\"vocab_sizes\"], :anime)),\n",
    "        ),\n",
    "        manga = (\n",
    "            item = Dense(config[\"hidden_size\"], extract(config[\"vocab_sizes\"], :manga)),\n",
    "            rating = Dense(config[\"hidden_size\"], extract(config[\"vocab_sizes\"], :manga)),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    TransformerModel(emb, bert, clf)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc3c56-2786-4752-befc-e9f241ae0f08",
   "metadata": {},
   "source": [
    "# Loss metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32348a0a-b6ef-4944-a28f-6b9bc038296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function masklm_losses(model, batch)\n",
    "    tokens, attention_mask, batch_positions, item_positions, labels, weights = batch\n",
    "    X = model.embed(\n",
    "        anime = extract(tokens, :anime),\n",
    "        manga = extract(tokens, :manga),\n",
    "        rating = extract(tokens, :rating),\n",
    "        timestamp = extract(tokens, :timestamp),\n",
    "        status = extract(tokens, :status),\n",
    "        completion = extract(tokens, :completion),\n",
    "        position = extract(tokens, :position),\n",
    "    )\n",
    "    X = model.transformers(X, attention_mask)\n",
    "\n",
    "    if length(item_positions[:anime][:item]) > 0\n",
    "        anime_item_pred = logsoftmax(\n",
    "            model.classifier.anime.item(gather(X, batch_positions[:anime][:item])),\n",
    "        )\n",
    "        anime_item_loss =\n",
    "            -(\n",
    "                weights[:anime][:item]' *\n",
    "                gather(anime_item_pred, item_positions[:anime][:item])\n",
    "            ) / sum(weights[:anime][:item])\n",
    "    else\n",
    "        anime_item_loss = 0.0f0\n",
    "    end\n",
    "    if length(item_positions[:anime][:rating]) > 0\n",
    "        anime_rating_pred =\n",
    "            model.classifier.anime.rating(gather(X, batch_positions[:anime][:rating]))\n",
    "        anime_rating_loss =\n",
    "            (\n",
    "                weights[:anime][:rating]' *\n",
    "                (\n",
    "                    gather(anime_rating_pred, item_positions[:anime][:rating]) -\n",
    "                    labels[:anime][:rating]\n",
    "                ) .^ 2\n",
    "            ) / sum(weights[:anime][:rating])\n",
    "    else\n",
    "        anime_rating_loss = 0.0f0\n",
    "    end\n",
    "\n",
    "    if length(item_positions[:manga][:item]) > 0\n",
    "        manga_item_pred = logsoftmax(\n",
    "            model.classifier.manga.item(gather(X, batch_positions[:manga][:item])),\n",
    "        )\n",
    "        manga_item_loss =\n",
    "            -(\n",
    "                weights[:manga][:item]' *\n",
    "                gather(manga_item_pred, item_positions[:manga][:item])\n",
    "            ) / sum(weights[:manga][:item])\n",
    "    else\n",
    "        manga_item_loss = 0.0f0\n",
    "    end\n",
    "    if length(item_positions[:manga][:rating]) > 0\n",
    "        manga_rating_pred =\n",
    "            model.classifier.manga.rating(gather(X, batch_positions[:manga][:rating]))\n",
    "        manga_rating_loss =\n",
    "            (\n",
    "                weights[:manga][:rating]' *\n",
    "                (\n",
    "                    gather(manga_rating_pred, item_positions[:manga][:rating]) -\n",
    "                    labels[:manga][:rating]\n",
    "                ) .^ 2\n",
    "            ) / sum(weights[:manga][:rating])\n",
    "    else\n",
    "        manga_rating_loss = 0.0f0\n",
    "    end\n",
    "\n",
    "    anime_item_loss, anime_rating_loss, manga_item_loss, manga_rating_loss\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eca42c-7356-49c2-8cbf-5baf86dec544",
   "metadata": {},
   "outputs": [],
   "source": [
    "function evaluate_metrics(sentences, t::Trainer)\n",
    "    losses = zeros(Float32, 4)\n",
    "    weights = zeros(Float32, 4)\n",
    "    Random.shuffle!(t.rng, sentences)\n",
    "    sentence_batches =\n",
    "        collect(Iterators.partition(sentences, t.training_config[\"batch_size\"]))\n",
    "    @showprogress for sbatch in sentence_batches\n",
    "        batch = get_batch(sbatch, false, t) |> device\n",
    "        w =\n",
    "            sum.([\n",
    "                batch[6][:anime][:item],\n",
    "                batch[6][:anime][:rating],\n",
    "                batch[6][:manga][:item],\n",
    "                batch[6][:manga][:rating],\n",
    "            ])\n",
    "        losses .+= masklm_losses(t.model, batch) .* w\n",
    "        weights .+= w\n",
    "        device_free!(batch)\n",
    "    end\n",
    "    names = [\n",
    "        \"Anime Crossentropy Loss\",\n",
    "        \"Anime Rating Loss\",\n",
    "        \"Manga Crossentropy Loss\",\n",
    "        \"Manga Rating Loss\",\n",
    "    ]\n",
    "    Dict(names .=> losses ./ weights)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb18c7a-64c9-4817-888b-3866fbff4ec6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c0fd3-6beb-47fb-95d2-94fb1d5cdb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_epoch!(sentences, t::Trainer)\n",
    "    Random.shuffle!(t.rng, sentences)\n",
    "    losses = []\n",
    "    sentence_chunks = 128\n",
    "    @showprogress for sentence_batch in Iterators.partition(\n",
    "        sentences,\n",
    "        div(length(sentences), sentence_chunks),\n",
    "    )\n",
    "        sentence_batch = shuffle_training_data(\n",
    "            t.rng,\n",
    "            sentence_batch,\n",
    "            t.training_config[\"max_sequence_length\"],\n",
    "            t.training_config[\"max_document_length\"],\n",
    "        )\n",
    "        minibatches =\n",
    "            collect(Iterators.partition(sentence_batch, t.training_config[\"batch_size\"]))\n",
    "        for minibatch in minibatches\n",
    "            schedule_learning_rate!(\n",
    "                t.opt,\n",
    "                t.weightdecay,\n",
    "                t.lr_schedule,\n",
    "                t.training_config[\"weight_decay\"],\n",
    "            )\n",
    "            batch = get_batch(minibatch, true, t) |> device\n",
    "            tloss, grads = Flux.withgradient(t.model) do m\n",
    "                sum(masklm_losses(m, batch))\n",
    "            end\n",
    "            push!(losses, tloss)\n",
    "            batch |> device_free!\n",
    "            Flux.update!(t.opt, t.model, grads[1])\n",
    "            Flux.update!(t.weightdecay, t.model, grads[1])\n",
    "        end\n",
    "    end\n",
    "    mean(losses)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f95adc-4281-4fb4-a126-957bd5431e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function checkpoint(sentences, t::Trainer, training_loss, epoch, name)\n",
    "    @info \"evaluating metrics\"\n",
    "    metrics = evaluate_metrics(sentences, t)\n",
    "    metrics[\"Validation Loss\"] = sum(values(metrics))\n",
    "    metrics[\"Training Loss\"] = training_loss\n",
    "    write_params(\n",
    "        Dict(\n",
    "            \"m\" => t.model |> cpu,\n",
    "            \"opt\" => t.opt |> cpu,\n",
    "            \"weightdecay\" => t.weightdecay |> cpu,\n",
    "            \"lr_schedule\" => t.lr_schedule,\n",
    "            \"epoch\" => epoch,\n",
    "            \"metrics\" => metrics,\n",
    "            \"training_config\" => t.training_config,\n",
    "            \"model_config\" => t.model_config,\n",
    "            \"rng\" => t.rng,\n",
    "        ),\n",
    "        \"$name/checkpoints/$epoch\",\n",
    "    )\n",
    "    @info \"saving model after $epoch epochs with metrics $metrics\"\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50183c75-f670-425e-b08e-c72efe4e8988",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ac800-38d2-4ed9-9963-05c176baef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_rngs(seed)\n",
    "    rng = Random.Xoshiro(seed)\n",
    "    Random.seed!(rand(rng, UInt64))\n",
    "    if CUDA.functional()\n",
    "        Random.seed!(CUDA.default_rng(), rand(rng, UInt64))\n",
    "        Random.seed!(CUDA.CURAND.default_rng(), rand(rng, UInt64))\n",
    "    end\n",
    "    rng\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed260a-b454-4690-ab8f-15ba4d686b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sentences(rng, training_config)\n",
    "    sentences = get_training_data(\n",
    "        training_config[\"media\"],\n",
    "        training_config[\"include_ptw_impressions\"],\n",
    "        training_config[\"cls_tokens\"],\n",
    "        training_config[\"empty_tokens\"],\n",
    "    )\n",
    "    Random.shuffle!(rng, sentences)\n",
    "    cutoff = Int(round(0.99 * length(sentences)))\n",
    "    training = sentences[1:cutoff]\n",
    "    validation = sentences[cutoff+1:end]\n",
    "    training, validation\n",
    "end;\n",
    "\n",
    "function set_epoch_size!(training_config, training_sentences)\n",
    "    num_tokens =\n",
    "        sum(min.(length.(training_sentences), training_config[\"max_document_length\"]))\n",
    "    @info \"Number of training sentences: $(length(training_sentences))\"\n",
    "    @info \"Number of training tokens: $(num_tokens)\"\n",
    "    training_config[\"iters_per_epoch\"] =\n",
    "        Int(ceil(num_tokens / training_config[\"max_sequence_length\"]))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb67b6-dabb-4db8-841d-c7ebf43f4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_training_config()\n",
    "    media = [\"anime\", \"manga\"]\n",
    "    base_vocab_sizes = (\n",
    "        Int32(num_items(\"anime\")),\n",
    "        Int32(num_items(\"manga\")),\n",
    "        Float32(11),\n",
    "        Float32(1),\n",
    "        Int32(5),\n",
    "        Float32(1),\n",
    "        Int32(maximum(num_users(x) for x in media)),\n",
    "        Int32(512), \n",
    "    )\n",
    "    d = Dict(\n",
    "        # tokenization\n",
    "        \"base_vocab_sizes\" => base_vocab_sizes,\n",
    "        \"cls_tokens\" => base_vocab_sizes .+ Int32(1),\n",
    "        \"pad_tokens\" => base_vocab_sizes .+ Int32(2),\n",
    "        \"mask_tokens\" => base_vocab_sizes .+ Int32(3),\n",
    "        \"empty_tokens\" => base_vocab_sizes .+ Int32(4),\n",
    "        \"vocab_sizes\" => base_vocab_sizes .+ Int32(4),\n",
    "        # training\n",
    "        \"batch_size\" => 16,\n",
    "        \"user_weighted_training\" => false,\n",
    "        \"peak_learning_rate\" => 3f-4,\n",
    "        \"weight_decay\" => 1f-2,\n",
    "        # data\n",
    "        \"max_document_length\" => Inf,\n",
    "        \"include_ptw_impressions\" => true,\n",
    "        \"explicit_baseline\" => Dict(\n",
    "            Symbol(x) =>\n",
    "                [read_params(\"$x/$t/ExplicitUserItemBiases\") for t in ALL_TASKS] for\n",
    "            x in [\"anime\", \"manga\"]\n",
    "        ),\n",
    "        \"media\" => media,\n",
    "        # model\n",
    "        \"num_layers\" => 4, \n",
    "        \"hidden_size\" => 512, \n",
    "        \"max_sequence_length\" => extract(base_vocab_sizes, :position),\n",
    "    )\n",
    "    d[\"num_epochs\"] = 64\n",
    "    @assert d[\"max_document_length\"] >= d[\"max_sequence_length\"]\n",
    "    # we embed the baseline we're residualizing against in the cls token's status field\n",
    "    for (k, v) in d[\"explicit_baseline\"]\n",
    "        @assert length(v) <= extract(d[\"base_vocab_sizes\"], :status)\n",
    "        for i = 1:length(ALL_TASKS)\n",
    "            v[i][\"task\"] = i\n",
    "            @assert length(v[i]) == 4\n",
    "            @assert length(v[i][\"λ\"]) == 5\n",
    "        end\n",
    "    end\n",
    "    d\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545bdf1-5fad-4de2-bbc8-dbcf63da3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_model_config(training_config)\n",
    "    # follows the recipe in Section 5 of [Well-Read Students Learn Better: On the \n",
    "    # Importance of Pre-training Compact Models](https://arxiv.org/pdf/1908.08962.pdf)\n",
    "    Dict(\n",
    "        \"attention_dropout\" => 0.1,\n",
    "        \"hidden_act\" => gelu,\n",
    "        \"num_hidden_layers\" => training_config[\"num_layers\"],\n",
    "        \"hidden_size\" => training_config[\"hidden_size\"],\n",
    "        \"max_sequence_length\" => training_config[\"max_sequence_length\"],\n",
    "        \"vocab_sizes\" => training_config[\"vocab_sizes\"],\n",
    "        \"num_attention_heads\" => Int(training_config[\"hidden_size\"] / 64),\n",
    "        \"dropout\" => 0.1,\n",
    "        \"intermediate_size\" => training_config[\"hidden_size\"] * 4,\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14633c-55e0-4044-8429-705b4f005bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_from_checkpoint(\n",
    "    training_config,\n",
    "    outdir::String,\n",
    "    epoch::Integer,\n",
    "    reset_lr_schedule,\n",
    "    rng,\n",
    ")\n",
    "    params = read_params(\"$outdir/$epoch\")\n",
    "    model = params[\"m\"] |> gpu\n",
    "    if training_config != params[\"training_config\"]\n",
    "        @info \"training config differs from stored params\"\n",
    "        training_config = params[\"training_config\"]\n",
    "    end\n",
    "    model_config = params[\"model_config\"]\n",
    "    opt = params[\"opt\"] |> gpu\n",
    "    weightdecay = params[\"weightdecay\"] |> gpu\n",
    "    if reset_lr_schedule\n",
    "        lr_schedule = get_lr_schedule(config)\n",
    "    else\n",
    "        lr_schedule = params[\"lr_schedule\"]\n",
    "    end\n",
    "    rng = params[\"rng\"]\n",
    "    Trainer(model, opt, weightdecay, lr_schedule, training_config, model_config, rng), epoch\n",
    "end\n",
    "\n",
    "function load_from_checkpoint(training_config, ::Nothing, ::Nothing, reset_lr_schedule, rng)\n",
    "    model_config = create_model_config(training_config)\n",
    "    model = create_bert(model_config) |> gpu\n",
    "    lr = Float32(config[\"peak_learning_rate\"])\n",
    "    wd = Float32(config[\"weight_decay\"])\n",
    "    opt = Optimisers.setup(Adam(lr, (0.9f0, 0.999f0)), model)\n",
    "    weightdecay = Optimisers.setup(PureWeightDecay(lr * wd), model)\n",
    "    initialize_weight_decay!(weightdecay, model)\n",
    "    lr_schedule = get_lr_schedule(config)\n",
    "    Trainer(model, opt, weightdecay, lr_schedule, training_config, model_config, rng), 0\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fa056-8deb-486d-9854-6e0991bc4714",
   "metadata": {},
   "source": [
    "# Actually Train Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969e640-d857-474e-b015-bbb11892d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_checkpoint = nothing\n",
    "config_epoch = nothing\n",
    "reset_lr_schedule = true\n",
    "config_rng = set_rngs(20221221)\n",
    "config = create_training_config();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd4b56-48d2-4d45-bdab-27eecaac3f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_sentences, validation_sentences = get_sentences(config_rng, config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a065c25-75b0-4e7b-a48d-7d27dcd0c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_epoch_size!(config, training_sentences)\n",
    "trainer, starting_epoch = load_from_checkpoint(\n",
    "    config,\n",
    "    config_checkpoint,\n",
    "    config_epoch,\n",
    "    reset_lr_schedule,\n",
    "    config_rng,\n",
    ")\n",
    "@info \"Num epochs: $(config[\"num_epochs\"])\"\n",
    "@info \"Training model with $(sum(length, Flux.params(trainer.model))) total parameters\"\n",
    "@info \"Embedding parameters: $(sum(length, Flux.params(trainer.model.embed)))\"\n",
    "@info \"Transformer parameters: $(sum(length, Flux.params(trainer.model.transformers)))\"\n",
    "@info \"Classifier parameters: $(sum(length, Flux.params(trainer.model.classifier)))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98bbf0-dae0-431e-a75c-b7c01343be9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_loss = Inf\n",
    "for i = 1:trainer.training_config[\"num_epochs\"]\n",
    "    GC.gc()\n",
    "    training_loss = train_epoch!(training_sentences, trainer)\n",
    "    checkpoint(validation_sentences, trainer, training_loss, starting_epoch + i, name)\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0-rc2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
