{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314d5be-9cf0-4f39-904c-b297f2d7b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux\n",
    "import NeuralAttentionlib\n",
    "import Optimisers\n",
    "import Optimisers: Adam\n",
    "import ParameterSchedulers\n",
    "import ParameterSchedulers: Sequence, Triangle, Shifted, Stateful\n",
    "import Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328eddd2-0fb7-4956-9ca4-df8a4a90d1c8",
   "metadata": {},
   "source": [
    "# Learning rate schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc7b25-2c8d-491a-ad13-ebf7c8e02fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "function schedule_learning_rate!(opt, weightdecay, lr_schedule, weightdecay_frac)\n",
    "    lr = Float32(ParameterSchedulers.next!(lr_schedule))\n",
    "    Optimisers.adjust!(opt, eta = lr)\n",
    "    Optimisers.adjust!(weightdecay, gamma = lr * weightdecay_frac)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12eb28a-053a-4448-b34a-dbb62551f43c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function LinearWarmupSchedule(lr, iters, warmup_perc)\n",
    "    # TODO cosine annealing\n",
    "    warmup_steps = Int(round(iters * warmup_perc))\n",
    "    remaining_steps = iters - warmup_steps\n",
    "    Stateful(\n",
    "        Sequence(\n",
    "            Triangle(λ0 = 0.0f0, λ1 = lr, period = 2 * warmup_steps) => warmup_steps,\n",
    "            Shifted(\n",
    "                Triangle(λ0 = 0.0f0, λ1 = lr, period = 2 * remaining_steps),\n",
    "                remaining_steps,\n",
    "            ) => remaining_steps,\n",
    "        ),\n",
    "    )\n",
    "end\n",
    "\n",
    "function get_lr_schedule(config; num_epochs = nothing, peak_learning_rate = nothing)\n",
    "    if isnothing(peak_learning_rate)\n",
    "        lr = Float32(config[\"peak_learning_rate\"])\n",
    "    else\n",
    "        lr = peak_learning_rate\n",
    "    end\n",
    "    if isnothing(num_epochs)\n",
    "        num_epochs = config[\"num_epochs\"]\n",
    "    end\n",
    "    max_batches = Int(ceil(num_epochs * config[\"iters_per_epoch\"] / config[\"batch_size\"]))\n",
    "    LinearWarmupSchedule(lr, max_batches, 0.06)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d0c8d-dd7c-491e-bd1f-2d0c9def45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "function current(iter::ParameterSchedulers.Stateful)\n",
    "    return iter.schedule(iter.state)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816f5fe-3846-43da-8969-f495a557cf60",
   "metadata": {},
   "source": [
    "# Weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08994772-46d5-4e3e-bc9d-6431c535844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay, but don't decay on embeddings, biases, or layer norms\n",
    "# need to call initialize_weight_decay! before after setup\n",
    "struct PureWeightDecay{T} <: Optimisers.AbstractRule\n",
    "  gamma::T\n",
    "end\n",
    "PureWeightDecay() = PureWeightDecay(5f-4)\n",
    "Optimisers.init(o::PureWeightDecay, x::AbstractArray) = nothing\n",
    "function Optimisers.apply!(o::PureWeightDecay, state, x, dx)\n",
    "  return state, o.gamma * x\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b2345-a00b-4a57-9143-59d885c0b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "function initialize_weight_decay!(opt, m)\n",
    "    if typeof(opt) <: Optimisers.Leaf || length(opt) == 0\n",
    "        return\n",
    "    end\n",
    "    disabled = disable_weight_decay(m)\n",
    "    for f in disabled\n",
    "        Optimisers.freeze!(opt[f])\n",
    "    end\n",
    "    for f in fieldnames(typeof(m))\n",
    "        if f ∉ disabled && f in fieldnames(typeof(opt))\n",
    "            initialize_weight_decay!(opt[f], getfield(m, f))\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba219a20-9b43-4da8-8371-6162f115a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers with special weightdecay semantics\n",
    "disable_weight_decay(x::Flux.Dense) = [:bias]\n",
    "disable_weight_decay(x::Transformers.Layers.Dense) =  [:b]\n",
    "disable_weight_decay(x::Transformers.Layers.LayerNorm) = [:α, :β]\n",
    "disable_weight_decay(x::Flux.LayerNorm) = [:diag]\n",
    "\n",
    "# as a safety check, whitelist layers\n",
    "disable_weight_decay(x) = @assert false typeof(x)\n",
    "disable_weight_decay(x::Function) = Symbol[]\n",
    "disable_weight_decay(x::NamedTuple) = Symbol[]\n",
    "disable_weight_decay(x::Tuple) = Symbol[]\n",
    "disable_weight_decay(x::Flux.Chain) = Symbol[]\n",
    "disable_weight_decay(x::Transformers.Transformer) = Symbol[]\n",
    "disable_weight_decay(x::Transformers.Layers.PreNormTransformerBlock) = Symbol[]\n",
    "disable_weight_decay(x::Transformers.Transformer) = Symbol[]\n",
    "disable_weight_decay(x::Transformers.Layers.PreNormResidual) = Symbol[]\n",
    "disable_weight_decay(x::Transformers.Layers.DropoutLayer) = Symbol[]\n",
    "disable_weight_decay(x::Transformers.Layers.SelfAttention) = Symbol[]\n",
    "disable_weight_decay(x::NeuralAttentionlib.MultiheadQKVAttenOp) = Symbol[]\n",
    "disable_weight_decay(x::Transformers.Layers.NSplit) = Symbol[]\n",
    "disable_weight_decay(x::Transformers.Layers.Chain) = Symbol[]\n",
    "disable_weight_decay(x::Transformers.Dropout) = Symbol[];"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0-rc2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
