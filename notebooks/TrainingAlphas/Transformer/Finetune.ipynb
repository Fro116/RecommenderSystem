{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dda7f55-4c2c-40ff-be28-c6a19829f6a0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "task = \"causal\"\n",
    "content = \"implicit\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07463b45-3148-4a5a-b119-ed43d468d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"$task/Transformer/$content\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ab492d-c494-4117-bed2-bb7fa7e7d8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: method definition for CuArray at /home/kundan/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable A but does not use it.\n",
      "WARNING: method definition for CuArray at /home/kundan/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable N+1 but does not use it.\n",
      "WARNING: method definition for CuArray at /home/kundan/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable K but does not use it.\n"
     ]
    }
   ],
   "source": [
    "import NBInclude: @nbinclude\n",
    "import SparseArrays: AbstractSparseArray, sparse\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Transformer.ipynb\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba6cc6-c7a5-49a6-83c8-208f318eba5e",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2ecba6-e4c4-4d4f-a8c6-4a9847e7bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_kw struct Trainer\n",
    "    # finetuning domain\n",
    "    task::Any\n",
    "    content::Any\n",
    "    # data\n",
    "    sentences::Any\n",
    "    masked_items::Any\n",
    "    labels::Any\n",
    "    weights::Any\n",
    "    priors::Any\n",
    "    # model\n",
    "    model::Any\n",
    "    max_seq_len::Any\n",
    "    mask_tokens::Any\n",
    "    pad_tokens::Any\n",
    "    # training\n",
    "    batch_size::Any\n",
    "    opt::Any\n",
    "    rng::Any\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b753540-f3ec-4fda-af5d-9aaeb7d8a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_labels(task, content)\n",
    "    df = cat(get_split(\"validation\", task, content), get_split(\"test\", task, content))\n",
    "    sparse(df.item, df.user, df.rating, num_items() + 4, num_users())\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fafc960-525a-421e-93d8-d92885061ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_weights(task, content)\n",
    "    df = cat(get_split(\"validation\", task, content), get_split(\"test\", task, content))\n",
    "    w = vcat(\n",
    "        powerdecay(get_counts(\"validation\", task, content), weighting_scheme(\"inverse\")),\n",
    "        powerdecay(get_counts(\"test\", task, content), weighting_scheme(\"inverse\")),\n",
    "    )\n",
    "    sparse(df.item, df.user, w, num_items() + 4, num_users())\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a0bbc2-5747-480f-88e9-857073e1a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_priors(task, content)\n",
    "    df = cat(\n",
    "        read_alpha(\"$task/ExplicitUserItemBiases\", \"validation\", task, content),\n",
    "        read_alpha(\"$task/ExplicitUserItemBiases\", \"test\", task, content),\n",
    "    )\n",
    "    sparse(df.item, df.user, df.rating, num_items() + 4, num_users())\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da320391-65ea-44d6-8288-afa36bf3ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_masked_items(task, content)\n",
    "    df = cat(get_split(\"validation\", task, content), get_split(\"test\", task, content))\n",
    "    user_to_items = Dict()\n",
    "    @showprogress for i = 1:length(df.user)\n",
    "        if df.user[i] ∉ keys(user_to_items)\n",
    "            user_to_items[df.user[i]] = Tuple{Int32,Float32}[]\n",
    "        end\n",
    "        push!(user_to_items[df.user[i]], (df.item[i], df.timestamp[i]))\n",
    "    end\n",
    "    user_to_items\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "307e849d-5097-4c20-8bd7-f8d8792995de",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sentence(sentences, x)\n",
    "    try\n",
    "        return copy(sentences[x])\n",
    "    catch KeyError\n",
    "        return eltype(values(trainer.sentences))(undef, 0)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa01f7-57c6-47c4-bea1-b8702ea700ad",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "381e1005-04ef-49fa-a004-d5ce6bec972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function replace_item(word, item_timestamp)\n",
    "    # TODO also replace the ts\n",
    "    item, timestamp = item_timestamp\n",
    "    ts = encode_raw_timestamp(timestamp)\n",
    "    (item, word[2:end]...)\n",
    "end\n",
    "\n",
    "function replace_timestamp(word, timestamp)\n",
    "    ts = encode_raw_timestamp(timestamp)\n",
    "    (word[1:2]..., ts, word[4:end]...)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "686d2a71-7dd2-45a5-8cbc-daadc9d26142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_timestamp (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_timestamp(item_timestamps)\n",
    "    minimum(map(x -> x[2], item_timestamps))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7061a381-da74-4070-8db7-5564cd3ca06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function random_dropout(rng, sentence, p)\n",
    "#     new_sentence = eltype(sentence)[]\n",
    "#     for i = 1:length(sentence)\n",
    "#         if rand(rng) >= p\n",
    "#             push!(new_sentence, sentence[i])\n",
    "#         end\n",
    "#     end\n",
    "#     new_sentence\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12aa0de7-d44b-4374-ace9-028e59b26f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_batch(\n",
    "    users,\n",
    "    training,\n",
    "    task,\n",
    "    content,\n",
    "    sentences,\n",
    "    masked_items,\n",
    "    labels,\n",
    "    weights,\n",
    "    priors,\n",
    "    max_seq_len,\n",
    "    mask_tokens,\n",
    "    pad_tokens,\n",
    "    rng,\n",
    ")\n",
    "    raw_sentences = [get_sentence(sentences, x) for x in users]\n",
    "    # TODO test to completion\n",
    "    # if training\n",
    "    #     raw_sentences = random_dropout.(rng, raw_sentences, 0.1)\n",
    "    # end\n",
    "    processed_sentences = eltype(values(sentences))[]\n",
    "    masked_token_positions = Tuple{Int32,Int32}[]\n",
    "    if content == \"explicit\"\n",
    "        output_labels = Float32[]\n",
    "        output_weights = Float32[]\n",
    "        output_priors = Float32[]\n",
    "    elseif content == \"implicit\"\n",
    "        output_labels = labels[:, users]\n",
    "        output_weights = weights[:, users]\n",
    "        output_priors = priors[:, users]\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "\n",
    "    function sample_item(user, weight)\n",
    "        item, timestamp = rand(rng, masked_items[user])\n",
    "        push!(output_labels, labels[item, user])\n",
    "        push!(output_priors, priors[item, user])\n",
    "        push!(output_weights, weight)\n",
    "        item, timestamp\n",
    "    end\n",
    "\n",
    "    for i::Int32 = 1:length(raw_sentences)\n",
    "        s = raw_sentences[i]\n",
    "        if task == \"random\"\n",
    "            s = subset_sentence(s, max_seq_len - 1; recent = false, rng = rng)\n",
    "            insert!(s, 1, mask_tokens)\n",
    "            push!(masked_token_positions, (1, i))\n",
    "            if content == \"explicit\"\n",
    "                s[1] = replace_item(s[1], sample_item(users[i], 1))\n",
    "            end\n",
    "        elseif task in [\"causal\", \"temporal\"]\n",
    "            s = subset_sentence(s, max_seq_len - 1; recent = true, rng = rng)\n",
    "            masked_word = mask_tokens\n",
    "            if task == \"temporal\"\n",
    "                masked_word = replace_timestamp(mask_tokens, 1)\n",
    "            else\n",
    "                masked_word =\n",
    "                    replace_timestamp(mask_tokens, get_timestamp(masked_items[users[i]]))\n",
    "            end\n",
    "            if content == \"explicit\"\n",
    "                masked_word = replace_item(word, sample_item(users[i], 1))\n",
    "            end\n",
    "            push!(s, masked_word)\n",
    "            push!(masked_token_positions, (Int32(length(s)), i))\n",
    "        else\n",
    "            @assert false\n",
    "        end\n",
    "        push!(processed_sentences, s)\n",
    "    end\n",
    "\n",
    "\n",
    "    inputs = get_inputs(processed_sentences, max_seq_len, pad_tokens, rng)\n",
    "    if content == \"explicit\"\n",
    "        to_explicit_output(x) = convert.(Float32, collect(x'))\n",
    "        output_labels = to_explicit_output(output_labels)\n",
    "        output_weights = to_explicit_output(output_weights)\n",
    "        output_priors = to_explicit_output(output_priors)\n",
    "    end\n",
    "    (inputs..., output_labels, output_weights, output_priors, masked_token_positions)\n",
    "end\n",
    "\n",
    "get_batch(users, t::Trainer, training::Bool) = get_batch(\n",
    "    users,\n",
    "    training,\n",
    "    t.task,\n",
    "    t.content,\n",
    "    t.sentences,\n",
    "    t.masked_items,\n",
    "    t.labels,\n",
    "    t.weights,\n",
    "    t.priors,\n",
    "    t.max_seq_len,\n",
    "    t.mask_tokens,\n",
    "    t.pad_tokens,\n",
    "    t.rng,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0f3c260-0e44-418e-9577-95c55d5ce566",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_inputs(sentences, max_seq_len, pad_tokens, rng)\n",
    "    # dynamically pad to the largest sequence length\n",
    "    seq_len = min(maximum(length.(sentences)) + 1, max_seq_len)\n",
    "\n",
    "    # get tokenized sentences\n",
    "    tokens = get_token_ids(sentences, seq_len, pad_tokens; rng = rng)\n",
    "\n",
    "    # don't attend to padding tokens\n",
    "    attention_mask = reshape(\n",
    "        convert.(Float32, tokens[1] .!= pad_tokens[1]),\n",
    "        (1, seq_len, length(sentences)),\n",
    "    )\n",
    "\n",
    "    tokens, attention_mask\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b370ea21-04af-456f-b3e0-351be33c861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device(x) = gpu(x)\n",
    "device(x::AbstractSparseArray) = CUDA.functional() ? CUDA.CuArray(gpu(x)) : collect(x)\n",
    "device(batch::Tuple) = device.(batch)\n",
    "\n",
    "function device_free!(x)\n",
    "    if !CUDA.functional()\n",
    "        return\n",
    "    end\n",
    "    CUDA.unsafe_free!.(x[1])\n",
    "    for i = 2:5\n",
    "        CUDA.unsafe_free!(x[i])\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e3cac-dc1c-465a-bdf0-f5d10307e61d",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cc2a10f-bcc9-41c1-9a7d-5a20d6056383",
   "metadata": {},
   "outputs": [],
   "source": [
    "function lm_loss(model, batch, content)\n",
    "    tokens, attention_mask, labels, weights, priors, masked_token_positions = batch\n",
    "    X = model.embed(\n",
    "        item = tokens[1],\n",
    "        rating = tokens[2],\n",
    "        timestamp = tokens[3],\n",
    "        status = tokens[4],\n",
    "        completion = tokens[5],\n",
    "        position = tokens[1],\n",
    "    )\n",
    "    X = model.transformers(X, attention_mask)\n",
    "    X = gather(X, masked_token_positions)\n",
    "\n",
    "    # TODO avg losses instead of sum loss\n",
    "\n",
    "    if content == \"explicit\"\n",
    "        rating_pred = model.classifier.rating(X)\n",
    "        return sum((rating_pred - labels) .^ 2 .* weights)\n",
    "    elseif content == \"implicit\"\n",
    "        item_pred = logsoftmax(\n",
    "            transpose(model.embed.embeddings.item.embedding) *\n",
    "            model.classifier.item.transform(X) .+ model.classifier.item.output_bias.b,\n",
    "        )\n",
    "        return -sum(labels .* weights .* item_pred)\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cea0e5c-2d02-4b74-8636-653b806fe9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "function split_losses(users, t::Trainer)\n",
    "    losses = 0.0\n",
    "    loss_weights = 0.0\n",
    "    user_batches = collect(Iterators.partition(users, t.batch_size))\n",
    "    @showprogress for user_batch in user_batches\n",
    "        batch = get_batch(user_batch, t, false) |> device\n",
    "        loss_weights += sum(batch[4])\n",
    "        losses += lm_loss(t.model, batch, t.content)\n",
    "        batch |> device_free!\n",
    "    end\n",
    "    losses / loss_weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc47f8f3-2380-4d7c-a83e-38ea1867cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_epoch!(users, t::Trainer)\n",
    "    users = Random.shuffle(t.rng, users)\n",
    "    user_batches = collect(Iterators.partition(users, t.batch_size))\n",
    "    @showprogress for user_batch in user_batches\n",
    "        batch = get_batch(user_batch, t, true) |> device\n",
    "        grads = Flux.gradient(t.model) do m\n",
    "            lm_loss(m, batch, t.content)\n",
    "        end\n",
    "        batch |> device_free!\n",
    "        Flux.update!(t.opt, t.model, grads[1])\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b22acc9f-a168-4afe-ad5a-a6be2f9dbe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "function checkpoint(users, t::Trainer, epoch, name)\n",
    "    @info \"evaluating metrics\"\n",
    "    metrics = split_losses(users, t)\n",
    "    write_params(\n",
    "        Dict(\"m\" => t.model |> cpu, \"epoch\" => epoch, \"metrics\" => metrics),\n",
    "        \"$name/checkpoints/$epoch\",\n",
    "    )\n",
    "    @info \"saving model after $epoch epochs with metrics $metrics\"\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d86b2-9354-4297-bcfa-b7e75f08c137",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cab8299-7c30-4f60-a409-47ab5a660122",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_model(checkpoint)\n",
    "    params = read_params(checkpoint)\n",
    "    m = params[\"m\"]\n",
    "    # config = params[\"model_config\"]\n",
    "    # item_cls = (\n",
    "    #     transform = Chain(\n",
    "    #         Dense(config[\"hidden_size\"], config[\"hidden_size\"], tanh),\n",
    "    #         Dropout(0.1),\n",
    "    #         Dense(config[\"hidden_size\"], config[\"hidden_size\"]),\n",
    "    #         LayerNorm(config[\"hidden_size\"]),\n",
    "    #     ),\n",
    "    #     output_bias = BiasLayer(config[\"vocab_sizes\"][1]),\n",
    "    # )\n",
    "    # rating_cls = Dense(config[\"hidden_size\"], 1)\n",
    "    # clf = (item = item_cls, rating = rating_cls)\n",
    "    # TransformerModel(m.embed, m.transformers, clf)\n",
    "    m\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20a0ac00-9e5d-491c-b5a0-f15a340ff23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_pretrained_model(checkpoint, task, content)\n",
    "    params = read_params(checkpoint)\n",
    "    config = params[\"training_config\"]\n",
    "    use_ptw = config[\"include_ptw_impressions\"]\n",
    "    sentences = get_training_data(task, use_ptw; show_progress_bar = true)\n",
    "    labels = get_labels(task, content)\n",
    "    weights = get_weights(task, content)\n",
    "    priors = get_priors(task, content)\n",
    "    masked_items = get_masked_items(task, content)\n",
    "    model = get_model(checkpoint) |> gpu\n",
    "    lr = 3e-5\n",
    "    opt = Optimisers.setup(\n",
    "        OptimiserChain(Adam(lr, (0.9f0, 0.999f0)), WeightDecay(lr * 1f-2)),\n",
    "        model,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        # finetuning domain\n",
    "        task = task,\n",
    "        content = content,\n",
    "        # data\n",
    "        sentences = sentences,\n",
    "        masked_items = masked_items,\n",
    "        labels = labels,\n",
    "        weights = weights,\n",
    "        priors = priors,\n",
    "        # model\n",
    "        model = model,\n",
    "        max_seq_len = config[\"max_sequence_length\"],\n",
    "        mask_tokens = config[\"mask_tokens\"],\n",
    "        pad_tokens = config[\"pad_tokens\"],\n",
    "        # training\n",
    "        batch_size = config[\"batch_size\"],\n",
    "        opt = opt,\n",
    "        rng = Random.Xoshiro(20230102),\n",
    "    )\n",
    "    trainer\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d2eeb2a-806a-4cbf-9c2e-8bbe5c17124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_users(rng, task, content)\n",
    "    training = collect(Set(get_split(\"validation\", task, content).user))\n",
    "    test = collect(Set(get_split(\"test\", task, content).user))\n",
    "    training, test\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2724548d-cf46-4897-b865-0bdcf56b304b",
   "metadata": {},
   "source": [
    "# Actually Train Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc5b0c71-0dff-42d3-b0dc-3d9da3af65c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:04:18 ( 1.33 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:06\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "pretrain_checkpoint = \"all/Transformer/mask/checkpoints/8\"\n",
    "trainer = load_pretrained_model(pretrain_checkpoint, task, content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3296c806-793b-4945-b187-bdf0a874967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation = get_users(trainer.rng, trainer.task, trainer.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caad534-15d7-4909-bce7-825c283099e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  74%|██████████████████████████████▍          |  ETA: 0:18:39\u001b[39mm55\u001b[39mm"
     ]
    }
   ],
   "source": [
    "for epoch = 1:100\n",
    "    train_epoch!(training, trainer)        \n",
    "    checkpoint(validation, trainer, epoch, name)            \n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
