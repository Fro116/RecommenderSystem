{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda7f55-4c2c-40ff-be28-c6a19829f6a0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "task = \"temporal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07463b45-3148-4a5a-b119-ed43d468d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"$task/Transformer/\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab492d-c494-4117-bed2-bb7fa7e7d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "import Flux\n",
    "import Flux: cpu, gpu, LayerNorm, logsoftmax\n",
    "import Random\n",
    "import SparseArrays: AbstractSparseArray, sparse\n",
    "import StatsBase: mean, sample\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Reference/CUDA.ipynb\")\n",
    "@nbinclude(\"Reference/Include.ipynb\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba6cc6-c7a5-49a6-83c8-208f318eba5e",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ecba6-e4c4-4d4f-a8c6-4a9847e7bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_kw struct Trainer\n",
    "    task::Any\n",
    "    # data\n",
    "    sentences::Any\n",
    "    labels::Any\n",
    "    weights::Any\n",
    "    timestamps::Any\n",
    "    # model\n",
    "    model::Any\n",
    "    max_seq_len::Any\n",
    "    vocab_sizes::Any\n",
    "    mask_tokens::Any\n",
    "    pad_tokens::Any\n",
    "    cls_tokens::Any\n",
    "    # training\n",
    "    minibatch_size::Any\n",
    "    batch_size::Any\n",
    "    opt::Any\n",
    "    rng::Any\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b753540-f3ec-4fda-af5d-9aaeb7d8a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_labels(task, content, num_items)\n",
    "    df = cat(get_split(\"validation\", task, content), get_split(\"test\", task, content))\n",
    "    sparse(df.item, df.user, df.rating, num_items, num_users())\n",
    "end\n",
    "\n",
    "get_labels(task, num_items) =\n",
    "    get_labels(task, \"implicit\", num_items), get_labels(task, \"explicit\", num_items);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafc960-525a-421e-93d8-d92885061ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_weights(task, content, num_items)\n",
    "    df = cat(get_split(\"validation\", task, content), get_split(\"test\", task, content))\n",
    "    w = vcat(\n",
    "        powerdecay(get_counts(\"validation\", task, content), weighting_scheme(\"inverse\")),\n",
    "        powerdecay(get_counts(\"test\", task, content), weighting_scheme(\"inverse\")),\n",
    "    )\n",
    "    sparse(df.item, df.user, w, num_items, num_users())\n",
    "end\n",
    "\n",
    "get_weights(task, num_items) =\n",
    "    get_weights(task, \"implicit\", num_items), get_weights(task, \"explicit\", num_items);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf0317-682d-49f8-8cc1-fb73bee068ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_timestamps(task, content, num_items)\n",
    "    df = cat(get_split(\"validation\", task, content), get_split(\"test\", task, content))\n",
    "    sparse(df.item, df.user, df.timestamp, num_items, num_users())\n",
    "end\n",
    "\n",
    "get_timestamps(task, num_items) = get_timestamps(task, \"implicit\", num_items);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2eeb2a-806a-4cbf-9c2e-8bbe5c17124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_users(rng, task, content)\n",
    "    training = collect(Set(get_split(\"validation\", task, content).user))\n",
    "    test = collect(Set(get_split(\"test\", task, content).user))\n",
    "    training, test\n",
    "end\n",
    "\n",
    "get_users(rng, task) = get_users(rng, task, \"implicit\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e849d-5097-4c20-8bd7-f8d8792995de",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sentence(sentences, x, cls_tokens)\n",
    "    try\n",
    "        return copy(sentences[x])\n",
    "    catch KeyError\n",
    "        return [cls_tokens]\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa01f7-57c6-47c4-bea1-b8702ea700ad",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa0de7-d44b-4374-ace9-028e59b26f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_batch(\n",
    "    users,\n",
    "    training;\n",
    "    task,\n",
    "    sentences,\n",
    "    labels,\n",
    "    weights,\n",
    "    timestamps,\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    mask_tokens,\n",
    "    pad_tokens,\n",
    "    cls_tokens,\n",
    "    rng,\n",
    ")\n",
    "    sentences = [get_sentence(sentences, x, cls_tokens) for x in users]\n",
    "    processed_sentences = eltype(values(sentences))[]\n",
    "    batch_positions = Tuple{Int32,Int32}[]\n",
    "    output_labels = map(x -> x[:, users], labels)\n",
    "    output_weights = map(x -> x[:, users], weights)\n",
    "\n",
    "    for i::Int32 = 1:length(sentences)\n",
    "        s = sentences[i]\n",
    "        if task == \"random\"\n",
    "            s = subset_sentence(s, max_seq_len - 1; recent = false, rng = rng)\n",
    "            masked_word = mask_tokens\n",
    "        elseif task in [\"temporal\"]\n",
    "            s = subset_sentence(s, max_seq_len - 1; recent = true, rng = rng)\n",
    "            # TODO set pos\n",
    "            masked_word = replace(mask_tokens, :timestamp, 1)\n",
    "        else\n",
    "            @assert false\n",
    "        end\n",
    "        push!(s, masked_word)\n",
    "        push!(batch_positions, (Int32(length(s)), i))\n",
    "        push!(processed_sentences, s)\n",
    "    end\n",
    "\n",
    "    inputs = get_inputs(\n",
    "        processed_sentences,\n",
    "        max_seq_len,\n",
    "        vocab_sizes,\n",
    "        pad_tokens,\n",
    "        cls_tokens,\n",
    "        rng,\n",
    "    )\n",
    "    (inputs..., output_labels, output_weights, batch_positions)\n",
    "end\n",
    "\n",
    "get_batch(users, training::Bool, t::Trainer) = get_batch(\n",
    "    users,\n",
    "    training,\n",
    "    task = t.task,\n",
    "    sentences = t.sentences,\n",
    "    labels = t.labels,\n",
    "    weights = t.weights,\n",
    "    timestamps = t.timestamps,\n",
    "    max_seq_len = t.max_seq_len,\n",
    "    vocab_sizes = t.vocab_sizes,\n",
    "    mask_tokens = t.mask_tokens,\n",
    "    pad_tokens = t.pad_tokens,\n",
    "    cls_tokens = t.cls_tokens,\n",
    "    rng = t.rng,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3c260-0e44-418e-9577-95c55d5ce566",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_inputs(sentences, max_seq_len, vocab_sizes, pad_tokens, cls_tokens, rng)\n",
    "    # dynamically pad to the largest sequence length\n",
    "    seq_len = min(maximum(length.(sentences)), max_seq_len)\n",
    "\n",
    "    # get tokenized sentences\n",
    "    tokens =\n",
    "        get_token_ids(sentences, seq_len, vocab_sizes[7], pad_tokens, cls_tokens; rng = rng)\n",
    "\n",
    "    # don't attend to padding tokens\n",
    "    attention_mask = reshape(\n",
    "        convert.(Float32, extract(tokens, :item) .!= extract(pad_tokens, :item)),\n",
    "        (1, seq_len, length(sentences)),\n",
    "    )\n",
    "    attention_mask = attention_mask .* permutedims(attention_mask, (2, 1, 3))\n",
    "\n",
    "    tokens, attention_mask\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370ea21-04af-456f-b3e0-351be33c861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device(x) = gpu(x)\n",
    "device(x::AbstractSparseArray) = CUDA.functional() ? CUDA.CuArray(gpu(x)) : collect(x)\n",
    "function device(x::Tuple)\n",
    "    (\n",
    "        device(x[1][1]),\n",
    "        device(x[1][2]),\n",
    "        device(x[1][3]),\n",
    "        device(x[1][4]),\n",
    "        device(x[1][5]),\n",
    "        nothing,\n",
    "        device(x[1][7]),\n",
    "    ),\n",
    "    device(x[2]),\n",
    "    device.(x[3]),\n",
    "    device.(x[4]),\n",
    "    device(x[5])\n",
    "end\n",
    "\n",
    "CUDA.unsafe_free!(::Nothing) = nothing\n",
    "\n",
    "function device_free!(x)\n",
    "    if !CUDA.functional()\n",
    "        return\n",
    "    end\n",
    "    CUDA.unsafe_free!.(x[1])\n",
    "    CUDA.unsafe_free!(x[2])\n",
    "    CUDA.unsafe_free!.(x[3])\n",
    "    CUDA.unsafe_free!.(x[4])\n",
    "    CUDA.unsafe_free!(x[5])\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e3cac-dc1c-465a-bdf0-f5d10307e61d",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20554889-80a5-4a09-bc5e-f13b9d713e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function lm_preds(model, batch)\n",
    "    tokens, attention_mask, _, _, batch_positions = batch\n",
    "    X = model.embed(\n",
    "        item = extract(tokens, :item),\n",
    "        rating = extract(tokens, :rating),\n",
    "        timestamp = extract(tokens, :timestamp),\n",
    "        status = extract(tokens, :status),\n",
    "        completion = extract(tokens, :completion),\n",
    "        position = extract(tokens, :position),\n",
    "    )\n",
    "    X = model.transformers(X, attention_mask)\n",
    "    X = gather(X, batch_positions)\n",
    "\n",
    "    item_preds =\n",
    "        transpose(model.embed.embeddings.item.embedding) *\n",
    "        model.classifier.item.transform(X) .+ model.classifier.item.output_bias.b\n",
    "    rating_preds = model.classifier.rating.transform(X)\n",
    "    item_preds, rating_preds\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2a10f-bcc9-41c1-9a7d-5a20d6056383",
   "metadata": {},
   "outputs": [],
   "source": [
    "function lm_losses(model, batch)\n",
    "    item_preds, rating_preds = lm_preds(model, batch)\n",
    "    labels = batch[3]\n",
    "    weights = batch[4]\n",
    "\n",
    "    if sum(weights[1]) > 0\n",
    "        item_loss =\n",
    "            -sum(labels[1] .* weights[1] .* logsoftmax(item_preds)) / sum(weights[1])\n",
    "    else\n",
    "        item_loss = 0.0f0\n",
    "    end\n",
    "    if sum(weights[2]) > 0\n",
    "        rating_loss = sum((rating_preds - labels[2]) .^ 2 .* weights[2]) / sum(weights[2])\n",
    "    else\n",
    "        rating_loss = 0.0f0\n",
    "    end\n",
    "\n",
    "    item_loss, rating_loss\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea0e5c-2d02-4b74-8636-653b806fe9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "function evaluate_losses(users, t::Trainer)\n",
    "    losses = zeros(2)\n",
    "    loss_weights = zeros(2)\n",
    "    user_batches = collect(Iterators.partition(users, t.batch_size))\n",
    "    @showprogress for user_batch in user_batches\n",
    "        batch = get_batch(user_batch, false, t) |> device\n",
    "        weights = sum.(batch[4])\n",
    "        loss_weights .+= weights\n",
    "        losses .+= lm_losses(t.model, batch) .* weights\n",
    "        batch |> device_free!\n",
    "    end\n",
    "    losses = losses ./ loss_weights\n",
    "    Dict(\"Item Crossentropy Loss\" => losses[1], \"Rating MSE Loss\" => losses[2])\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27317bbc-7e8c-4a81-9c54-34a4ccffd603",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47f8f3-2380-4d7c-a83e-38ea1867cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_epoch!(users, t::Trainer)\n",
    "    users = Random.shuffle(t.rng, users)\n",
    "    user_batches = collect(Iterators.partition(users, t.batch_size))\n",
    "    losses = []\n",
    "    @showprogress for user_batch in user_batches\n",
    "        minibatches = collect(Iterators.partition(user_batch, t.minibatch_size))\n",
    "        total_grads = nothing\n",
    "        for minibatch in minibatches\n",
    "            batch = get_batch(minibatch, true, t) |> device\n",
    "            loss, grads = Flux.withgradient(t.model) do m\n",
    "                sum(lm_losses(m, batch))\n",
    "            end\n",
    "            total_grads = tuplesum(total_grads, grads[1])\n",
    "            push!(losses, loss)\n",
    "            batch |> device_free!\n",
    "        end\n",
    "        total_grads = tupledivide(total_grads, length(minibatches))\n",
    "        Flux.update!(t.opt, t.model, total_grads)\n",
    "    end\n",
    "    mean(losses)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22acc9f-a168-4afe-ad5a-a6be2f9dbe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "function checkpoint(users, t::Trainer, training_loss, epoch, pretrain_checkpoint, name)\n",
    "    @info \"evaluating metrics\"\n",
    "    metrics = evaluate_losses(users, t)\n",
    "    metrics[\"training_loss\"] = training_loss\n",
    "    write_params(\n",
    "        Dict(\n",
    "            \"m\" => t.model |> cpu,\n",
    "            \"epoch\" => epoch,\n",
    "            \"metrics\" => metrics,\n",
    "            \"pretrain_checkpoint\" => pretrain_checkpoint,\n",
    "        ),\n",
    "        \"$name/checkpoints/$epoch\",\n",
    "    )\n",
    "    @info \"saving model after $epoch epochs with metrics $metrics\"\n",
    "    metrics\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d86b2-9354-4297-bcfa-b7e75f08c137",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0ac00-9e5d-491c-b5a0-f15a340ff23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_pretrained_model(checkpoint, task)\n",
    "    params = read_params(checkpoint)\n",
    "    config = params[\"training_config\"]\n",
    "    use_ptw = config[\"include_ptw_impressions\"]\n",
    "    model = params[\"m\"] |> gpu\n",
    "    N = size(model.embed.embeddings.item.embedding)[2]\n",
    "    sentences =\n",
    "        get_training_data(task, use_ptw, config[\"cls_tokens\"]; show_progress_bar = true)\n",
    "    labels = get_labels(task, N)\n",
    "    weights = get_weights(task, N)\n",
    "    timestamps = get_timestamps(task, N)\n",
    "    lr = 1e-5\n",
    "    opt = Optimisers.setup(\n",
    "        OptimiserChain(Adam(lr, (0.9f0, 0.999f0)), WeightDecay(lr * 1f-2)),\n",
    "        model,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        # finetuning domain\n",
    "        task = task,\n",
    "        # data\n",
    "        sentences = sentences,\n",
    "        labels = labels,\n",
    "        weights = weights,\n",
    "        timestamps = timestamps,\n",
    "        # model\n",
    "        model = model,\n",
    "        max_seq_len = config[\"max_sequence_length\"],\n",
    "        vocab_sizes = config[\"base_vocab_sizes\"],\n",
    "        mask_tokens = config[\"mask_tokens\"],\n",
    "        pad_tokens = config[\"pad_tokens\"],\n",
    "        cls_tokens = config[\"cls_tokens\"],\n",
    "        # training\n",
    "        minibatch_size = config[\"minibatch_size\"],\n",
    "        batch_size = config[\"batch_size\"],\n",
    "        opt = opt,\n",
    "        rng = Random.Xoshiro(20230102),\n",
    "    )\n",
    "    trainer\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2724548d-cf46-4897-b865-0bdcf56b304b",
   "metadata": {},
   "source": [
    "# Actually Train Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b0c71-0dff-42d3-b0dc-3d9da3af65c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrain_model_tag = \"v8\"\n",
    "pretrain_dir = \"all/Transformer/$pretrain_model_tag/checkpoints/\"\n",
    "pretrain_epoch = sort(parse.(Int, readdir(get_data_path(\"alphas/$pretrain_dir\"))))[end]\n",
    "pretrain_checkpoint = joinpath(pretrain_dir, string(pretrain_epoch))\n",
    "@info \"loading pretrained model from $pretrain_checkpoint\"\n",
    "trainer = load_pretrained_model(pretrain_checkpoint, task);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296c806-793b-4945-b187-bdf0a874967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation = get_users(trainer.rng, trainer.task);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caad534-15d7-4909-bce7-825c283099e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopper = early_stopper(max_iters = 20, patience=0)\n",
    "test_loss = Inf\n",
    "best_model = nothing\n",
    "while (!stop!(stopper, test_loss))\n",
    "    best_model = trainer.model |> cpu\n",
    "    training_loss = train_epoch!(training, trainer)        \n",
    "    metrics = checkpoint(validation, trainer, training_loss, stopper.iters, pretrain_checkpoint, name)    \n",
    "    test_loss = metrics[\"Item Crossentropy Loss\"] + metrics[\"Rating MSE Loss\"]\n",
    "end;\n",
    "trainer = @set trainer.model = best_model |> gpu;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf158b9-d63c-4efb-921b-555add5a607d",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebdc6c-7441-4eb5-859a-56bc9e461e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a vector that maps a user to the list of items to predict\n",
    "function user_to_items(users::Vector, items::Vector)\n",
    "    user_to_count = zeros(Int32, num_users(), Threads.nthreads())\n",
    "    @tprogress Threads.@threads for u in users\n",
    "        user_to_count[u, Threads.threadid()] += 1\n",
    "    end\n",
    "    user_to_count = convert.(Int32, vec(sum(user_to_count, dims = 2)))\n",
    "\n",
    "    utoa = Vector{Vector{Int32}}()\n",
    "    @showprogress for u = 1:num_users()\n",
    "        push!(utoa, Vector{Int32}(undef, user_to_count[u]))\n",
    "    end\n",
    "\n",
    "    @showprogress for i = 1:length(users)\n",
    "        u = users[i]\n",
    "        a = items[i]\n",
    "        utoa[u][user_to_count[u]] = a\n",
    "        user_to_count[u] -= 1\n",
    "    end\n",
    "    utoa\n",
    "end\n",
    "\n",
    "function evaluate_model(users, items, t::Trainer)\n",
    "    CUDA.math_mode!(CUDA.FAST_MATH; precision = :TensorFloat32)\n",
    "    utoa = user_to_items(users, items)\n",
    "    out_users = Vector{Int32}(undef, length(users))\n",
    "    out_items = Vector{Int32}(undef, length(users))\n",
    "    out_implicit_ratings = fill(NaN32, length(out_users))\n",
    "    out_explicit_ratings = fill(NaN32, length(out_users))\n",
    "    out_idx = 1\n",
    "\n",
    "    # compute predictions    \n",
    "    user_batches = collect(Iterators.partition(Set(users), t.minibatch_size))\n",
    "    @showprogress for sampled_users in Set(user_batches)\n",
    "        batch = get_batch(sampled_users, false, t) |> device\n",
    "        item_preds, rating_preds = lm_preds(t.model, batch)\n",
    "        item_preds = softmax(item_preds) |> cpu\n",
    "        rating_preds = rating_preds |> cpu\n",
    "        for j = 1:length(sampled_users)\n",
    "            u = sampled_users[j]\n",
    "            if length(utoa[u]) > 0\n",
    "                item_mask = utoa[u]\n",
    "                next_idx = out_idx + length(item_mask)\n",
    "                out_users[out_idx:next_idx-1] .= u\n",
    "                out_items[out_idx:next_idx-1] = item_mask\n",
    "                out_implicit_ratings[out_idx:next_idx-1] = item_preds[item_mask, j]\n",
    "                out_explicit_ratings[out_idx:next_idx-1] = rating_preds[item_mask, j]\n",
    "                out_idx = next_idx\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    CUDA.math_mode!(CUDA.FAST_MATH; precision = :BFloat16)\n",
    "    RatingsDataset(user = out_users, item = out_items, rating = out_implicit_ratings),\n",
    "    RatingsDataset(user = out_users, item = out_items, rating = out_explicit_ratings)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f674970-6330-416d-ac48-046d60865ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "function write_alpha(t::Trainer, outdir::String, task::String)\n",
    "    master_dfs = []\n",
    "    @showprogress for split in ALL_SPLITS\n",
    "        for content in ALL_CONTENTS\n",
    "            push!(master_dfs, get_raw_split(split, task, content; fields = [:user, :item]))\n",
    "        end\n",
    "    end\n",
    "    master_df = reduce(cat, master_dfs)\n",
    "    imp_p, exp_p = sparse.(evaluate_model(master_df.user, master_df.item, trainer))\n",
    "    function model(p, users, items)\n",
    "        r = zeros(length(users))\n",
    "        @tprogress Threads.@threads for j = 1:length(r)\n",
    "            r[j] = p[items[j], users[j]]\n",
    "        end\n",
    "        r\n",
    "    end\n",
    "    for (content, p) in [(\"implicit\", imp_p), (\"explicit\", exp_p)]\n",
    "        write_alpha(\n",
    "            (users, items) -> model(p, users, items),\n",
    "            \"$outdir/$content\";\n",
    "            task = task,\n",
    "            log = true,\n",
    "            log_task = task,\n",
    "            log_content = content,\n",
    "            log_alphas = String[],\n",
    "        )\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969ef30-d148-4d7e-ab8f-73f0e82e28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_alpha(trainer, name, task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
