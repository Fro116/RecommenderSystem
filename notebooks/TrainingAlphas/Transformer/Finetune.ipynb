{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dda7f55-4c2c-40ff-be28-c6a19829f6a0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"random\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = \"random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07463b45-3148-4a5a-b119-ed43d468d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"$task/Transformer/\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ab492d-c494-4117-bed2-bb7fa7e7d8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: method definition for CuArray at /home/kundan/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable A but does not use it.\n",
      "WARNING: method definition for CuArray at /home/kundan/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable N+1 but does not use it.\n",
      "WARNING: method definition for CuArray at /home/kundan/.julia/packages/PrimitiveOneHot/M7M4C/src/gpu.jl:29 declares type variable K but does not use it.\n"
     ]
    }
   ],
   "source": [
    "import NBInclude: @nbinclude\n",
    "import SparseArrays: AbstractSparseArray, sparse\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Transformer.ipynb\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba6cc6-c7a5-49a6-83c8-208f318eba5e",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2ecba6-e4c4-4d4f-a8c6-4a9847e7bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_kw struct Trainer\n",
    "    task::Any\n",
    "    # data\n",
    "    sentences::Any\n",
    "    masked_items::Any\n",
    "    labels::Any\n",
    "    weights::Any\n",
    "    explicit_prior::Any\n",
    "    # model\n",
    "    model::Any\n",
    "    max_seq_len::Any\n",
    "    vocab_sizes::Any\n",
    "    mask_tokens::Any\n",
    "    pad_tokens::Any\n",
    "    # training\n",
    "    minibatch_size::Any\n",
    "    batch_size::Any\n",
    "    opt::Any\n",
    "    rng::Any\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b753540-f3ec-4fda-af5d-9aaeb7d8a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_labels(task, content, prior = nothing)\n",
    "    df = cat(get_split(\"validation\", task, content), get_split(\"test\", task, content))\n",
    "    if prior != nothing\n",
    "        @tprogress Threads.@threads for i = 1:length(df.rating)\n",
    "            df.rating[i] -= prior.user[df.user[i]] + prior.item[df.item[i]]\n",
    "        end\n",
    "    end\n",
    "    sparse(df.item, df.user, df.rating, num_items() + 4, num_users())\n",
    "end\n",
    "\n",
    "get_labels(task, explicit_prior) =\n",
    "    get_labels(task, \"implicit\", nothing), get_labels(task, \"explicit\", explicit_prior);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fafc960-525a-421e-93d8-d92885061ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_weights(task, content)\n",
    "    df = cat(get_split(\"validation\", task, content), get_split(\"test\", task, content))\n",
    "    w = vcat(\n",
    "        powerdecay(get_counts(\"validation\", task, content), weighting_scheme(\"inverse\")),\n",
    "        powerdecay(get_counts(\"test\", task, content), weighting_scheme(\"inverse\")),\n",
    "    )\n",
    "    sparse(df.item, df.user, w, num_items() + 4, num_users())\n",
    "end\n",
    "\n",
    "get_weights(task) = get_weights(task, \"implicit\"), get_weights(task, \"explicit\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da320391-65ea-44d6-8288-afa36bf3ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_masked_items(task, content)\n",
    "    df = cat(get_split(\"validation\", task, content), get_split(\"test\", task, content))\n",
    "    user_to_items = Dict()\n",
    "    @showprogress for i = 1:length(df.user)\n",
    "        if df.user[i] ∉ keys(user_to_items)\n",
    "            user_to_items[df.user[i]] = Tuple{Int32,Float32}[]\n",
    "        end\n",
    "        push!(user_to_items[df.user[i]], (df.item[i], df.timestamp[i]))\n",
    "    end\n",
    "    user_to_items\n",
    "end\n",
    "\n",
    "get_masked_items(task) =\n",
    "    get_masked_items(task, \"implicit\"), get_masked_items(task, \"explicit\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "307e849d-5097-4c20-8bd7-f8d8792995de",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sentence(sentences, x)\n",
    "    try\n",
    "        return copy(sentences[x])\n",
    "    catch KeyError\n",
    "        return eltype(values(trainer.sentences))(undef, 0)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d2eeb2a-806a-4cbf-9c2e-8bbe5c17124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_users(rng, task, content)\n",
    "    training = collect(Set(get_split(\"validation\", task, content).user))\n",
    "    test = collect(Set(get_split(\"test\", task, content).user))\n",
    "    training, test\n",
    "end\n",
    "\n",
    "get_users(rng, task) = get_users(rng, task, \"implicit\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa01f7-57c6-47c4-bea1-b8702ea700ad",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "381e1005-04ef-49fa-a004-d5ce6bec972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_random_timestamp(rng, item_timestamps) = rand(rng, map(x -> x[2], item_timestamps));\n",
    "replace_timestamp(word, timestamp) = (word[1:2]..., timestamp, word[4:end]...);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7061a381-da74-4070-8db7-5564cd3ca06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function random_dropout(rng, sentence, p)\n",
    "#     new_sentence = eltype(sentence)[]\n",
    "#     for i = 1:length(sentence)\n",
    "#         if rand(rng) >= p\n",
    "#             push!(new_sentence, sentence[i])\n",
    "#         end\n",
    "#     end\n",
    "#     new_sentence\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12aa0de7-d44b-4374-ace9-028e59b26f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_batch(\n",
    "    users,\n",
    "    training;\n",
    "    task,\n",
    "    sentences,\n",
    "    masked_items,\n",
    "    labels,\n",
    "    weights,\n",
    "    explicit_prior,\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    mask_tokens,\n",
    "    pad_tokens,\n",
    "    rng,\n",
    ")\n",
    "    raw_sentences = [get_sentence(sentences, x) for x in users]\n",
    "    # TODO test to completion\n",
    "    # if training\n",
    "    #     raw_sentences = random_dropout.(rng, raw_sentences, 0.1)\n",
    "    # end\n",
    "    processed_sentences = eltype(values(sentences))[]\n",
    "    batch_positions = Tuple{Int32,Int32}[]\n",
    "    output_labels = map(x -> x[:, users], labels)\n",
    "    output_weights = map(x -> x[:, users], weights)\n",
    "\n",
    "    for i::Int32 = 1:length(raw_sentences)\n",
    "        s = raw_sentences[i]\n",
    "        if task == \"random\"\n",
    "            s = subset_sentence(s, max_seq_len - 1; recent = false, rng = rng)\n",
    "            insert!(s, 1, mask_tokens)\n",
    "            push!(batch_positions, (1, i))\n",
    "        elseif task in [\"causal\", \"temporal\"]\n",
    "            s = subset_sentence(s, max_seq_len - 1; recent = true, rng = rng)\n",
    "            masked_word = mask_tokens\n",
    "            if task == \"temporal\"\n",
    "                masked_word = replace_timestamp(mask_tokens, 1)\n",
    "            else\n",
    "                if training && rang(rng) < 0.5\n",
    "                    masked_word = replace_timestamp(\n",
    "                        mask_tokens,\n",
    "                        get_random_timestamp(rng, masked_items[users[i]]),\n",
    "                    )\n",
    "                end\n",
    "            end\n",
    "            push!(s, masked_word)\n",
    "            push!(batch_positions, (Int32(length(s)), i))\n",
    "        else\n",
    "            @assert false\n",
    "        end\n",
    "        push!(processed_sentences, s)\n",
    "    end\n",
    "\n",
    "    inputs = get_inputs(\n",
    "        processed_sentences,\n",
    "        max_seq_len,\n",
    "        vocab_sizes,\n",
    "        pad_tokens,\n",
    "        mask_tokens,\n",
    "        explicit_prior,\n",
    "        rng,\n",
    "    )\n",
    "    (inputs..., output_labels, output_weights, batch_positions)\n",
    "end\n",
    "\n",
    "get_batch(users, training::Bool, t::Trainer) = get_batch(\n",
    "    users,\n",
    "    training,\n",
    "    task = t.task,\n",
    "    sentences = t.sentences,\n",
    "    masked_items = t.masked_items,\n",
    "    labels = t.labels,\n",
    "    weights = t.weights,\n",
    "    explicit_prior = t.explicit_prior,\n",
    "    max_seq_len = t.max_seq_len,\n",
    "    vocab_sizes = t.vocab_sizes,\n",
    "    mask_tokens = t.mask_tokens,\n",
    "    pad_tokens = t.pad_tokens,\n",
    "    rng = t.rng,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0f3c260-0e44-418e-9577-95c55d5ce566",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_inputs(\n",
    "    sentences,\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    pad_tokens,\n",
    "    mask_tokens,\n",
    "    explicit_prior,\n",
    "    rng,\n",
    ")\n",
    "    # dynamically pad to the largest sequence length\n",
    "    seq_len = min(maximum(length.(sentences)) + 1, max_seq_len)\n",
    "\n",
    "    # get tokenized sentences\n",
    "    tokens = get_token_ids(sentences, seq_len, pad_tokens; rng = rng)\n",
    "\n",
    "    # don't attend to padding tokens\n",
    "    attention_mask = reshape(\n",
    "        convert.(Float32, tokens[1] .!= pad_tokens[1]),\n",
    "        (1, seq_len, length(sentences)),\n",
    "    )\n",
    "\n",
    "    # demean explicit ratings\n",
    "    for b::Int32 = 1:length(sentences)\n",
    "        seq_len = Int(sum(attention_mask[:, :, b]))\n",
    "        for i::Int32 = 1:seq_len\n",
    "            if tokens[2][i, b] .< vocab_sizes[2]\n",
    "                prior =\n",
    "                    explicit_prior.user[tokens[6][i, b]] +\n",
    "                    explicit_prior.item[tokens[1][i, b]]\n",
    "                tokens[2][i, b] -= prior\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    tokens, attention_mask\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b370ea21-04af-456f-b3e0-351be33c861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device(x) = gpu(x)\n",
    "device(x::AbstractSparseArray) = CUDA.functional() ? CUDA.CuArray(gpu(x)) : collect(x)\n",
    "device(batch::Tuple) = device.(batch)\n",
    "\n",
    "function device_free!(x)\n",
    "    if !CUDA.functional()\n",
    "        return\n",
    "    end\n",
    "    CUDA.unsafe_free!.(x[1])\n",
    "    CUDA.unsafe_free!(x[2])\n",
    "    CUDA.unsafe_free!.(x[3])\n",
    "    CUDA.unsafe_free!.(x[4])\n",
    "    CUDA.unsafe_free!(x[5])\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e3cac-dc1c-465a-bdf0-f5d10307e61d",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20554889-80a5-4a09-bc5e-f13b9d713e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function lm_preds(model, batch)\n",
    "    tokens, attention_mask, labels, weights, batch_positions = batch\n",
    "    X = model.embed(\n",
    "        item = tokens[1],\n",
    "        rating = tokens[2],\n",
    "        timestamp = tokens[3],\n",
    "        status = tokens[4],\n",
    "        completion = tokens[5],\n",
    "        position = tokens[1],\n",
    "    )\n",
    "    X = model.transformers(X, attention_mask)\n",
    "    X = gather(X, batch_positions)\n",
    "\n",
    "    item_preds =\n",
    "        transpose(model.embed.embeddings.item.embedding) *\n",
    "        model.classifier.item.transform(X) .+ model.classifier.item.output_bias.b\n",
    "    rating_preds = model.classifier.rating.transform(X)\n",
    "    item_preds, rating_preds\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cc2a10f-bcc9-41c1-9a7d-5a20d6056383",
   "metadata": {},
   "outputs": [],
   "source": [
    "function lm_losses(model, batch)\n",
    "    tokens, attention_mask, labels, weights, batch_positions = batch\n",
    "    item_preds, rating_preds = lm_preds(model, batch)\n",
    "\n",
    "    if sum(weights[1]) > 0\n",
    "        item_loss =\n",
    "            -sum(labels[1] .* weights[1] .* logsoftmax(item_preds)) / sum(weights[1])\n",
    "    else\n",
    "        item_loss = 0.0f0\n",
    "    end\n",
    "    if sum(weights[2]) > 0\n",
    "        rating_loss = sum((rating_preds - labels[2]) .^ 2 .* weights[2]) / sum(weights[2])\n",
    "    else\n",
    "        rating_loss = 0.0f0\n",
    "    end\n",
    "\n",
    "    item_loss, rating_loss\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cea0e5c-2d02-4b74-8636-653b806fe9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "function split_losses(users, t::Trainer)\n",
    "    losses = zeros(2)\n",
    "    loss_weights = zeros(2)\n",
    "    user_batches = collect(Iterators.partition(users, t.batch_size))\n",
    "    @showprogress for user_batch in user_batches\n",
    "        batch = get_batch(user_batch, false, t) |> device\n",
    "        weights = sum.(batch[4])\n",
    "        loss_weights .+= weights\n",
    "        losses .+= lm_losses(t.model, batch) .* weights\n",
    "        batch |> device_free!\n",
    "    end\n",
    "    losses = losses ./ loss_weights\n",
    "    Dict(\"Item Crossentropy Loss\" => losses[1], \"Rating MSE Loss\" => losses[2])\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27317bbc-7e8c-4a81-9c54-34a4ccffd603",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc47f8f3-2380-4d7c-a83e-38ea1867cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_epoch!(users, t::Trainer)\n",
    "    users = Random.shuffle(t.rng, users)\n",
    "    user_batches = collect(Iterators.partition(users, t.batch_size))\n",
    "    losses = []\n",
    "    @showprogress for user_batch in user_batches\n",
    "        minibatches = collect(Iterators.partition(user_batch, t.minibatch_size))\n",
    "        total_grads = nothing\n",
    "        for minibatch in minibatches\n",
    "            batch = get_batch(minibatch, true, t) |> device\n",
    "            loss, grads = Flux.withgradient(t.model) do m\n",
    "                sum(lm_losses(m, batch))\n",
    "            end\n",
    "            total_grads = tuplesum(total_grads, grads[1])\n",
    "            push!(losses, loss)\n",
    "            batch |> device_free!\n",
    "        end\n",
    "        total_grads = tupledivide(total_grads, length(minibatches))\n",
    "        Flux.update!(t.opt, t.model, total_grads)\n",
    "    end\n",
    "    mean(losses)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b22acc9f-a168-4afe-ad5a-a6be2f9dbe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "function checkpoint(users, t::Trainer, training_loss, epoch, name)\n",
    "    @info \"evaluating metrics\"\n",
    "    metrics = split_losses(users, t)\n",
    "    metrics[\"training_loss\"] = training_loss\n",
    "    write_params(\n",
    "        Dict(\"m\" => t.model |> cpu, \"epoch\" => epoch, \"metrics\" => metrics),\n",
    "        \"$name/checkpoints/$epoch\",\n",
    "    )\n",
    "    @info \"saving model after $epoch epochs with metrics $metrics\"\n",
    "    metrics\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d86b2-9354-4297-bcfa-b7e75f08c137",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a0ac00-9e5d-491c-b5a0-f15a340ff23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_pretrained_model(checkpoint, task)\n",
    "    params = read_params(checkpoint)\n",
    "    config = params[\"training_config\"]\n",
    "    use_ptw = config[\"include_ptw_impressions\"]\n",
    "    sentences = get_training_data(task, use_ptw; show_progress_bar = true)\n",
    "    explicit_prior = config[\"explicit_prior\"] # TODO task specific priors    \n",
    "    labels = get_labels(task, explicit_prior)\n",
    "    weights = get_weights(task)\n",
    "    masked_items = get_masked_items(task)\n",
    "    model = params[\"m\"] |> gpu\n",
    "    lr = 3e-5\n",
    "    opt = Optimisers.setup(\n",
    "        OptimiserChain(Adam(lr, (0.9f0, 0.999f0)), WeightDecay(lr * 1f-2)),\n",
    "        model,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        # finetuning domain\n",
    "        task = task,\n",
    "        # data\n",
    "        sentences = sentences,\n",
    "        masked_items = masked_items,\n",
    "        labels = labels,\n",
    "        weights = weights,\n",
    "        explicit_prior = explicit_prior,\n",
    "        # model\n",
    "        model = model,\n",
    "        max_seq_len = config[\"max_sequence_length\"],\n",
    "        vocab_sizes = config[\"base_vocab_sizes\"],\n",
    "        mask_tokens = config[\"mask_tokens\"],\n",
    "        pad_tokens = config[\"pad_tokens\"],\n",
    "        # training\n",
    "        minibatch_size = config[\"minibatch_size\"],\n",
    "        batch_size = config[\"batch_size\"],\n",
    "        opt = opt,\n",
    "        rng = Random.Xoshiro(20230102),\n",
    "    )\n",
    "    trainer\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2724548d-cf46-4897-b865-0bdcf56b304b",
   "metadata": {},
   "source": [
    "# Actually Train Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc5b0c71-0dff-42d3-b0dc-3d9da3af65c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:03:53 ( 1.32 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:22\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:13\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "pretrain_checkpoint = \"all/Transformer/norm/checkpoints/8\"\n",
    "trainer = load_pretrained_model(pretrain_checkpoint, task);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3296c806-793b-4945-b187-bdf0a874967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation = get_users(trainer.rng, trainer.task);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caad534-15d7-4909-bce7-825c283099e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  42%|█████████████████▏                       |  ETA: 0:46:59\u001b[39mm57\u001b[39mm"
     ]
    }
   ],
   "source": [
    "stopper = early_stopper(max_iters = 25)\n",
    "test_loss = Inf\n",
    "while (!stop!(stopper, test_loss))\n",
    "    training_loss = train_epoch!(training, trainer)        \n",
    "    metrics = checkpoint(validation, trainer, training_loss, epoch, name)    \n",
    "    test_loss = metrics[\"Item Crossentropy Loss\"] + metrics[\"Rating MSE Loss\"]\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf158b9-d63c-4efb-921b-555add5a607d",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebdc6c-7441-4eb5-859a-56bc9e461e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # returns a vector that maps a user to the list of items to predict\n",
    "# function user_to_items(users::Vector, items::Vector)\n",
    "#     user_to_count = zeros(Int32, num_users(), Threads.nthreads())\n",
    "#     @tprogress Threads.@threads for u in users\n",
    "#         user_to_count[u, Threads.threadid()] += 1\n",
    "#     end\n",
    "#     user_to_count = convert.(Int32, vec(sum(user_to_count, dims = 2)))\n",
    "\n",
    "#     utoa = Vector{Vector{Int32}}()\n",
    "#     @showprogress for u = 1:num_users()\n",
    "#         push!(utoa, Vector{Int32}(undef, user_to_count[u]))\n",
    "#     end\n",
    "\n",
    "#     @showprogress for i = 1:length(users)\n",
    "#         u = users[i]\n",
    "#         a = items[i]\n",
    "#         utoa[u][user_to_count[u]] = a\n",
    "#         user_to_count[u] -= 1\n",
    "#     end\n",
    "#     utoa\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f674970-6330-416d-ac48-046d60865ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function evaluate_model(users, items, t::Trainer, content::String)\n",
    "#     utoa = user_to_items(users, items)\n",
    "\n",
    "#     activation = content == \"implicit\" ? softmax : identity\n",
    "#     out_users = Vector{Int32}(undef, length(users))\n",
    "#     out_items = Vector{Int32}(undef, length(users))\n",
    "#     out_ratings = fill(NaN32, length(out_users))\n",
    "#     out_idx = 1\n",
    "\n",
    "#     # compute predictions    \n",
    "#     user_batches = collect(Iterators.partition(Set(users), t.minibatch_size))\n",
    "#     @showprogress for sampled_users in Set(user_batches)\n",
    "#         batch = get_batch(sampled_users, false, t) |> device\n",
    "#         item_preds, rating_preds = lm_preds(model, batch)\n",
    "#         alpha = (content == \"implicit\" ? item_preds : rating_preds) |> cpu\n",
    "#         for j = 1:length(sampled_users)\n",
    "#             u = sampled_users[j]\n",
    "#             if length(utoa[u]) > 0\n",
    "#                 item_mask = utoa[u]\n",
    "#                 next_idx = out_idx + length(item_mask)\n",
    "#                 out_users[out_idx:next_idx-1] .= u\n",
    "#                 out_items[out_idx:next_idx-1] = item_mask\n",
    "#                 out_ratings[out_idx:next_idx-1] = alpha[item_mask, j]\n",
    "#                 out_idx = next_idx\n",
    "#             end\n",
    "#         end\n",
    "#     end\n",
    "#     RatingsDataset(user = out_users, item = out_items, rating = out_ratings)\n",
    "# end\n",
    "\n",
    "# function write_alpha(t::Trainer, outdir::String, content::String, task::String)\n",
    "#     function model(users, items)\n",
    "#         p = sparse(evaluate_model(users, items, t, content))\n",
    "#         r = zeros(length(users))\n",
    "#         @tprogress Threads.@threads for j = 1:length(r)\n",
    "#             r[j] = p[items[j], users[j]]\n",
    "#         end\n",
    "#         r\n",
    "#     end\n",
    "#     if content == \"explicit\"\n",
    "#         residual_alphas = [\"random/ExplicitUserItemBiases\"] # TODO task specific\n",
    "#     else\n",
    "#         residual_alphas = String[]\n",
    "#     end\n",
    "#     write_alpha(\n",
    "#         model,\n",
    "#         outdir;\n",
    "#         task = task,\n",
    "#         log = true,\n",
    "#         log_task = task,\n",
    "#         log_content = content,\n",
    "#         log_alphas = residual_alphas,\n",
    "#     )\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20228a5-4645-45fb-8b2f-910bfa5a1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for content in [\"implicit\", \"explicit\"]\n",
    "#     write_alpha(t, name, content, task)\n",
    "# end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
