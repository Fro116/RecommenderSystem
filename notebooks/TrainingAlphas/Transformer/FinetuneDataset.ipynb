{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37131d29-4151-410e-866f-761db4964bbb",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "* Finetunes a transformer model to predict recent watches\n",
    "* A recent watch is defined as an interaction that occurred within the past $D$ days and is one of the $N$ most recent interactions for that user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90afc10-1ae5-4665-937e-d58a5859ac94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Data.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d60ae-6b55-4274-85b1-c0f9c9fc7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import HDF5\n",
    "import JSON\n",
    "import MLUtils\n",
    "import NNlib: sigmoid\n",
    "import Random\n",
    "import SparseArrays: AbstractSparseArray, sparse, spzeros\n",
    "import StatsBase: mean, sample\n",
    "import ThreadPinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478f496-3095-4fc0-9db7-699277589073",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "medium = \"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd56259-4513-4297-a978-99c765532231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "version = \"v2layer\"\n",
    "pretrain_name = \"all/Transformer/$version\"\n",
    "name = \"$medium/Transformer/$version\"\n",
    "set_logging_outdir(name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a7882c-990d-470e-a3d3-3ebc4d1c1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThreadPinning.pinthreads(:cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50183c75-f670-425e-b08e-c72efe4e8988",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b753540-f3ec-4fda-af5d-9aaeb7d8a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_labels(metric, medium)\n",
    "    df = cat(\n",
    "        get_split(\"validation\", metric, medium, [:userid, :itemid, :metric]),\n",
    "        get_split(\"test\", metric, medium, [:userid, :itemid, :metric]),\n",
    "    )\n",
    "    sparse(df, medium)\n",
    "end\n",
    "\n",
    "function get_weights(metric, medium)\n",
    "    df = cat(\n",
    "        get_split(\"validation\", metric, medium, [:userid, :itemid]),\n",
    "        get_split(\"test\", metric, medium, [:userid, :itemid]),\n",
    "    )\n",
    "    df = @set df.metric = powerdecay(get_counts(df.userid), -1.0f0)\n",
    "    sparse(df, medium)\n",
    "end\n",
    "\n",
    "get_users(split, medium) =\n",
    "    collect(Set(get_raw_split(split, medium, [:userid], nothing).userid));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ac800-38d2-4ed9-9963-05c176baef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_rngs(seed)\n",
    "    rng = Random.Xoshiro(seed)\n",
    "    Random.seed!(rand(rng, UInt64))\n",
    "    rng\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb67b6-dabb-4db8-841d-c7ebf43f4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_training_config(pretrain_name, medium)\n",
    "    file = joinpath(get_data_path(\"alphas/$pretrain_name\"), \"config.json\")\n",
    "    open(file) do f\n",
    "        d = JSON.parse(f)\n",
    "        d[\"mode\"] = \"finetune\"\n",
    "        d[\"medium\"] = medium\n",
    "        return d\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c35ee-317c-4c0a-bd28-9c0462f9d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_epoch_size!(config, training_users, validation_users)\n",
    "    @info \"Number of training sentences: $(length(training_users))\"\n",
    "    @info \"Number of validation sentences: $(length(validation_users))\"\n",
    "    config[\"training_epoch_size\"] = length(training_users)\n",
    "    config[\"validation_epoch_size\"] = length(validation_users)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31c5b2-42e2-455a-bd3f-0ca4eab8cb8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function setup_training(config, outdir)\n",
    "    if !isdir(outdir)\n",
    "        mkpath(outdir)\n",
    "    end\n",
    "    for x in readdir(outdir, join = true)\n",
    "        if isfile(x)\n",
    "            rm(x)\n",
    "        end\n",
    "    end\n",
    "    fn = joinpath(outdir, \"..\", \"config.json\")\n",
    "    open(fn, \"w\") do f\n",
    "        write(f, JSON.json(config))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fa056-8deb-486d-9854-6e0991bc4714",
   "metadata": {},
   "source": [
    "# Disk I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed0909-a7b0-4b54-ae7b-20dd5b9b789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function featurize(sentences, labels, weights, medium, userid, config)\n",
    "    if userid in keys(sentences)\n",
    "        sentence = copy(sentences[userid])\n",
    "    else\n",
    "        sentence = Vector{wordtype}()\n",
    "        push!(sentence, replace(config[\"cls_tokens\"], :userid, userid))\n",
    "    end\n",
    "    featurize(;\n",
    "        sentence = sentence,\n",
    "        labels = map(x -> x[:, userid+1], labels),\n",
    "        weights = map(x -> x[:, userid+1], weights),\n",
    "        medium = medium,\n",
    "        userid = userid,\n",
    "        max_seq_len = config[\"max_sequence_length\"],\n",
    "        vocab_sizes = config[\"base_vocab_sizes\"],\n",
    "        pad_tokens = config[\"pad_tokens\"],\n",
    "        cls_tokens = config[\"cls_tokens\"],\n",
    "        mask_tokens = config[\"mask_tokens\"],\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002d542-1609-4c35-b8f3-32e0e5b16647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function featurize(;\n",
    "    sentence::Vector{wordtype},\n",
    "    labels,\n",
    "    weights,\n",
    "    medium,\n",
    "    userid,\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    pad_tokens,\n",
    "    cls_tokens,\n",
    "    mask_tokens,\n",
    ")\n",
    "    # get inputs\n",
    "    sentence = subset_sentence(\n",
    "        sentence,\n",
    "        min(length(sentence), max_seq_len - 1);\n",
    "        recent = true,\n",
    "        rng = nothing,\n",
    "    )\n",
    "    # masked_word = replace(mask_tokens, :updated_at, 1) # TODO\n",
    "    masked_word = config[\"mask_tokens\"]\n",
    "    masked_word = replace(masked_word, :position, length(sentence) - 1)\n",
    "    masked_word = replace(masked_word, :userid, userid)\n",
    "    push!(sentence, masked_word)\n",
    "    tokens = get_token_ids(sentence, max_seq_len, pad_tokens, cls_tokens)\n",
    "\n",
    "    # get outputs\n",
    "    positions = [length(sentence) - 1]\n",
    "    featurized_labels = Dict(\n",
    "        x => Dict(y => spzeros(Float32, num_items(x)) for y in ALL_METRICS) for\n",
    "        x in ALL_MEDIUMS\n",
    "    )\n",
    "    featurized_weights = Dict(\n",
    "        x => Dict(y => spzeros(Float32, num_items(x)) for y in ALL_METRICS) for\n",
    "        x in ALL_MEDIUMS\n",
    "    )\n",
    "    for i = 1:length(ALL_METRICS)\n",
    "        featurized_labels[medium][ALL_METRICS[i]] .= labels[i]\n",
    "        featurized_weights[medium][ALL_METRICS[i]] .= weights[i]\n",
    "    end\n",
    "    tokens, positions, featurized_labels, featurized_weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7abb52-f79b-4e89-8611-aa1506d2808e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function save_features(sentences, labels, weights, users, config, filename)\n",
    "    features = []\n",
    "    for x in users\n",
    "        push!(features, featurize(sentences, labels, weights, medium, x, config))\n",
    "    end\n",
    "\n",
    "    d = Dict{String,AbstractArray}()\n",
    "    collate = MLUtils.batch\n",
    "    embed_names = [\"itemid\", \"rating\", \"updated_at\", \"status\", \"position\", \"userid\"]\n",
    "    for (i, name) in Iterators.enumerate(embed_names)\n",
    "        d[name] = collate([x[1][i] for x in features])\n",
    "    end\n",
    "    d[\"positions\"] = collate([x[2] for x in features])\n",
    "    for medium in ALL_MEDIUMS\n",
    "        for metric in ALL_METRICS\n",
    "            record_sparse_array!(\n",
    "                d,\n",
    "                \"labels_$(medium)_$(metric)\",\n",
    "                collate([x[3][medium][metric] for x in features]),\n",
    "            )\n",
    "            record_sparse_array!(\n",
    "                d,\n",
    "                \"weights_$(medium)_$(metric)\",\n",
    "                collate([x[4][medium][metric] for x in features]),\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "\n",
    "    HDF5.h5open(filename, \"w\") do file\n",
    "        for (k, v) in d\n",
    "            write(file, k, v)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function record_sparse_array!(d::Dict, name::String, x::AbstractSparseArray)\n",
    "    i, j, v = SparseArrays.findnz(x)\n",
    "    d[name*\"_i\"] = i .- 1\n",
    "    d[name*\"_j\"] = j .- 1\n",
    "    d[name*\"_v\"] = v\n",
    "    d[name*\"_size\"] = collect(size(x))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18314729-7634-4b7e-a452-1c73a0922d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function advance!(filename)\n",
    "    # check to see if we should write the next shard\n",
    "    outdir = dirname(filename)\n",
    "    files = readdir(outdir)\n",
    "    suffix = basename(filename) * \".read\"\n",
    "    files = [x for x in files if occursin(suffix, basename(x))]\n",
    "    if length(files) == 0\n",
    "        return false\n",
    "    end\n",
    "    world_sizes = Set(split(x, \".\")[end] for x in files)\n",
    "    @assert length(world_sizes) == 1\n",
    "    world_size = parse(Int, first(world_sizes))\n",
    "    advance = length(files) == world_size\n",
    "    if advance\n",
    "        rm(\"$filename.complete\")\n",
    "        rm(filename)\n",
    "        for x in files\n",
    "            rm(joinpath(outdir, x))\n",
    "        end\n",
    "    end\n",
    "    advance\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8cc95c-b656-49d2-a65f-1504fbe4b50e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function spawn_feature_workers(\n",
    "    sentences,\n",
    "    labels,\n",
    "    weights,\n",
    "    users,\n",
    "    config,\n",
    "    rng,\n",
    "    training,\n",
    "    outdir,\n",
    ")\n",
    "    # writes data to \"$outdir/training/$split.$worker.h5\" in a loop\n",
    "    chunk_size = config[\"chunk_size\"]\n",
    "    workers = training ? config[\"num_training_shards\"] : config[\"num_validation_shards\"]\n",
    "    stem = training ? \"training\" : \"validation\"\n",
    "    rngs = [Random.Xoshiro(rand(rng, UInt64)) for _ = 1:workers]\n",
    "    for (i, batch) in Iterators.enumerate(\n",
    "        Iterators.partition(users, div(length(users), workers, RoundUp)),\n",
    "    )\n",
    "        Threads.@spawn begin\n",
    "            rng = rngs[i]\n",
    "            while true\n",
    "                Random.shuffle!(rng, batch)\n",
    "                for (j, chunk) in\n",
    "                    Iterators.enumerate(Iterators.partition(batch, chunk_size))\n",
    "                    filename = joinpath(outdir, \"$stem.$i.h5\")\n",
    "                    save_features(sentences, labels, weights, chunk, config, filename)\n",
    "                    open(\"$filename.complete\", \"w\") do f\n",
    "                        write(f, \"$j\")\n",
    "                    end\n",
    "                    if i == 1\n",
    "                        GC.gc()\n",
    "                    end\n",
    "                    while isdir(outdir) && !advance!(filename)\n",
    "                        sleep(1)\n",
    "                    end\n",
    "                    if !isdir(outdir)\n",
    "                        return\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c19b7-612d-464b-a830-519ad2c4924a",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bd779-8b8f-4027-ba80-34d147d90a4d",
   "metadata": {
    "papermill": {
     "duration": 1.708366,
     "end_time": "2023-05-15T05:04:49.317062",
     "exception": false,
     "start_time": "2023-05-15T05:04:47.608696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_checkpoint = nothing\n",
    "config_epoch = nothing\n",
    "rng = set_rngs(20221221)\n",
    "config = create_training_config(pretrain_name, medium);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece189f1-11f9-4774-b519-635444144d93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-05-15T05:04:49.320685",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_users, test_users = get_users.([\"validation\", \"test\"], (medium,))\n",
    "sentences = get_training_data(config[\"cls_tokens\"], 1, vcat(training_users, test_users));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e570c-ca23-46fd-8151-4c75577ed996",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels.(ALL_METRICS, (medium,));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c6e03-e932-40c1-bd00-ae5296d70dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = get_weights.(ALL_METRICS, (medium,));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364915d-bf04-45fb-b8d4-877a81cf90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_epoch_size!(config, training_users, test_users);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb0a73-ced6-4e7c-abe3-bfadf15a08b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "outdir = get_data_path(joinpath(\"alphas\", name, \"training\"))\n",
    "setup_training(config, outdir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b33f5c-3a91-49d2-b202-d0dad70916da",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDF5.h5open(joinpath(outdir, \"users.h5\"), \"w\") do file\n",
    "    write(file, \"training\", training_users)\n",
    "    write(file, \"test\", test_users)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40387edf-e126-4c06-a0ad-48b749fc0d35",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spawn_feature_workers(sentences, labels, weights, training_users, config, rng, true, outdir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa999b-7bc5-4f94-a562-cf0a773eaecc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spawn_feature_workers(sentences, labels, weights, test_users, config, rng, false, outdir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407af37-2ab2-495a-9fe8-6695c7d15295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for workers to begin writing\n",
    "while sum(endswith.(readdir(outdir), (\".complete\",))) <\n",
    "      config[\"num_training_shards\"] + config[\"num_validation_shards\"]\n",
    "    sleep(1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9abf5e-ae0f-4e4c-967c-5058b90dcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(`python3 Pytorch.py --outdir $name --initialize $pretrain_name --epochs 8`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184b49c-a8b8-44d5-8672-a6cce64fb09f",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe2c46-60f5-41b6-a1ec-2d0c1177ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = HDF5.h5open(get_data_path(joinpath(\"alphas\", name, \"embeddings.h5\")), \"r\")\n",
    "embeddings = read(file[\"embedding\"])\n",
    "users = read(file[\"users\"])\n",
    "weights = Dict(x => read(file[\"$(medium)_$(x)_weight\"])' for x in ALL_METRICS)\n",
    "biases = Dict(x => read(file[\"$(medium)_$(x)_bias\"]) for x in ALL_METRICS)\n",
    "close(file)\n",
    "\n",
    "user_to_index = Dict()\n",
    "for (i, u) in Iterators.enumerate(users)\n",
    "    user_to_index[u] = i\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2207253-2413-4538-b1c5-0b8d7d09ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function model(users, items, user_cache)\n",
    "    p = zeros(Float32, length(users))\n",
    "    @showprogress for i = 1:length(p)\n",
    "        u = user_to_index[users[i]]\n",
    "        p[i] = user_cache[u][items[i]+1]\n",
    "    end\n",
    "    p\n",
    "end\n",
    "\n",
    "function get_cache(metric, user_to_index, weights, biases)\n",
    "    cache = Dict()\n",
    "    E = weights[metric] * embeddings .+ biases[metric]\n",
    "    @showprogress for u in values(user_to_index)\n",
    "        e = E[:, u]\n",
    "        if metric in [\"watch\", \"plantowatch\"]\n",
    "            e = softmax(e)\n",
    "        elseif metric == \"drop\"\n",
    "            e = sigmoid(e)\n",
    "        end\n",
    "        cache[u] = e\n",
    "    end\n",
    "    cache\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3447cfa-d1a6-4c50-8aeb-8452053bc420",
   "metadata": {},
   "outputs": [],
   "source": [
    "@showprogress for metric in ALL_METRICS\n",
    "    cache = get_cache(metric, user_to_index, weights, biases)\n",
    "    write_alpha(\n",
    "        (users, items) -> model(users, items, cache),\n",
    "        medium,\n",
    "        \"$name/$metric\",\n",
    "        [\"test\", \"negative\"],\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a072cc1-6dfe-4144-befc-82dd3a87576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ALL_METRICS\n",
    "    for split in [\"test\"]\n",
    "        val = compute_loss(metric, medium, [\"$name/$metric\"], split)\n",
    "        @info \"$metric $split loss = $val\"\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
