{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37131d29-4151-410e-866f-761db4964bbb",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "* Finetunes a transformer model to predict recent watches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90afc10-1ae5-4665-937e-d58a5859ac94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Data.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d60ae-6b55-4274-85b1-c0f9c9fc7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import HDF5\n",
    "import JSON\n",
    "import MLUtils\n",
    "import NNlib: sigmoid\n",
    "import Random\n",
    "import SparseArrays: AbstractSparseArray, sparse, spzeros\n",
    "import StatsBase: mean, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478f496-3095-4fc0-9db7-699277589073",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "medium = \"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd56259-4513-4297-a978-99c765532231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "const version = \"v1\"\n",
    "const pretrain_name = \"all/Transformer/$version\"\n",
    "const name = \"$medium/Transformer/$version\"\n",
    "set_logging_outdir(name);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50183c75-f670-425e-b08e-c72efe4e8988",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b753540-f3ec-4fda-af5d-9aaeb7d8a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_labels(metric, medium)\n",
    "    df = cat(\n",
    "        get_split(\"validation\", metric, medium, [:userid, :itemid, :metric]),\n",
    "        get_split(\"test\", metric, medium, [:userid, :itemid, :metric]),\n",
    "    )\n",
    "    sparse(df, medium)\n",
    "end\n",
    "\n",
    "function get_weights(metric, medium)\n",
    "    df = cat(\n",
    "        get_split(\"validation\", metric, medium, [:userid, :itemid]),\n",
    "        get_split(\"test\", metric, medium, [:userid, :itemid]),\n",
    "    )\n",
    "    df = @set df.metric = powerdecay(get_counts(df.userid), -1.0f0)\n",
    "    sparse(df, medium)\n",
    "end\n",
    "\n",
    "get_users(split, medium) =\n",
    "    collect(Set(get_raw_split(split, medium, [:userid], nothing).userid));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed0909-a7b0-4b54-ae7b-20dd5b9b789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function tokenize(sentences, labels, weights, medium, userid, config)\n",
    "    if userid in keys(sentences)\n",
    "        sentence = copy(sentences[userid])\n",
    "    else\n",
    "        sentence = Vector{wordtype}()\n",
    "        push!(sentence, replace(config[:cls_tokens], :userid, userid))\n",
    "    end\n",
    "    tokenize(;\n",
    "        sentence = sentence,\n",
    "        labels = map(x -> x[:, userid+1], labels),\n",
    "        weights = map(x -> x[:, userid+1], weights),\n",
    "        medium = medium,\n",
    "        userid = userid,\n",
    "        max_seq_len = config[:max_sequence_length],\n",
    "        vocab_sizes = config[:base_vocab_sizes],\n",
    "        pad_tokens = config[:pad_tokens],\n",
    "        cls_tokens = config[:cls_tokens],\n",
    "        mask_tokens = config[:mask_tokens],\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002d542-1609-4c35-b8f3-32e0e5b16647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function tokenize(;\n",
    "    sentence::Vector{wordtype},\n",
    "    labels,\n",
    "    weights,\n",
    "    medium,\n",
    "    userid,\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    pad_tokens,\n",
    "    cls_tokens,\n",
    "    mask_tokens,\n",
    ")\n",
    "    # get inputs\n",
    "    sentence =\n",
    "        subset_sentence(sentence, min(length(sentence), max_seq_len - 1); recent = true)\n",
    "    # masked_word = replace(mask_tokens, :updated_at, 1) # TODO\n",
    "    masked_word = mask_tokens\n",
    "    masked_word = replace(masked_word, :position, length(sentence) - 1)\n",
    "    masked_word = replace(masked_word, :userid, userid)\n",
    "    push!(sentence, masked_word)\n",
    "    tokens = get_token_ids(sentence, max_seq_len, pad_tokens, cls_tokens)\n",
    "\n",
    "    # get outputs\n",
    "    positions = [length(sentence) - 1]\n",
    "    tokenized_labels = Dict(\n",
    "        x => Dict(y => spzeros(Float32, num_items(x)) for y in ALL_METRICS) for\n",
    "        x in ALL_MEDIUMS\n",
    "    )\n",
    "    tokenized_weights = Dict(\n",
    "        x => Dict(y => spzeros(Float32, num_items(x)) for y in ALL_METRICS) for\n",
    "        x in ALL_MEDIUMS\n",
    "    )\n",
    "    for i = 1:length(ALL_METRICS)\n",
    "        tokenized_labels[medium][ALL_METRICS[i]] .= labels[i]\n",
    "        tokenized_weights[medium][ALL_METRICS[i]] .= weights[i]\n",
    "    end\n",
    "    tokens, positions, tokenized_labels, tokenized_weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdccd72-b45a-4987-aed5-4c2eb5a27a54",
   "metadata": {},
   "source": [
    "# Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91170e88-c492-4dc0-93a1-7701c1a961f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "function hdf5_writer(c::Channel)\n",
    "    while true\n",
    "        (d, fn) = take!(c)\n",
    "        HDF5.h5open(fn, \"w\") do f\n",
    "            for (k, v) in d\n",
    "                f[k, deflate = 3] = v\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b8550-0340-4abf-9782-ed908f206687",
   "metadata": {},
   "outputs": [],
   "source": [
    "function record_sparse_array!(d::Dict, name::String, x::AbstractSparseArray)\n",
    "    i, j, v = SparseArrays.findnz(x)\n",
    "    d[name*\"_i\"] = i .- 1\n",
    "    d[name*\"_j\"] = j .- 1\n",
    "    d[name*\"_v\"] = v\n",
    "    d[name*\"_size\"] = collect(size(x))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea2b0a-d24e-4712-a3cf-1b6c03e72ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_tokens(sentences, labels, weights, users, config, filename, writer)\n",
    "    tokens = Any[nothing for _ = 1:length(users)]\n",
    "    Threads.@threads for i = 1:length(users)\n",
    "        tokens[i] = tokenize(sentences, labels, weights, medium, users[i], config)\n",
    "    end\n",
    "\n",
    "    d = Dict{String,AbstractArray}()\n",
    "    collate = MLUtils.batch\n",
    "    embed_names = [\"itemid\", \"rating\", \"updated_at\", \"status\", \"position\", \"userid\"]\n",
    "    for (i, name) in Iterators.enumerate(embed_names)\n",
    "        d[name] = collate([x[1][i] for x in tokens])\n",
    "    end\n",
    "    d[\"positions\"] = collate([x[2] for x in tokens])\n",
    "    for medium in ALL_MEDIUMS\n",
    "        for metric in ALL_METRICS\n",
    "            record_sparse_array!(\n",
    "                d,\n",
    "                \"labels_$(medium)_$(metric)\",\n",
    "                collate([x[3][medium][metric] for x in tokens]),\n",
    "            )\n",
    "            record_sparse_array!(\n",
    "                d,\n",
    "                \"weights_$(medium)_$(metric)\",\n",
    "                collate([x[4][medium][metric] for x in tokens]),\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "    put!(writer, (d, filename))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11aa3e7-2ffd-4657-9622-ea9ed8cc325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_epoch(sentences, labels, weights, users, config, epoch, outdir, split, writer)\n",
    "    outdir = joinpath(outdir, split, \"$epoch\")\n",
    "    mkpath(outdir)\n",
    "    Random.shuffle!(users)\n",
    "    num_sentences = 0\n",
    "    expected_num_sentences = getfield(config, Symbol(\"$(split)_epoch_size\"))\n",
    "    @showprogress enabled = epoch == 0 for (i, batch) in collect(\n",
    "        Iterators.enumerate(Iterators.partition(users, config.batch_size)),\n",
    "    )\n",
    "        num_sentences += length(batch)\n",
    "        save_tokens(\n",
    "            sentences,\n",
    "            labels,\n",
    "            weights,\n",
    "            batch,\n",
    "            config,\n",
    "            joinpath(outdir, \"$(i-1).h5\"),\n",
    "            writer,\n",
    "        )\n",
    "    end\n",
    "    @assert num_sentences == expected_num_sentences\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb3995-af54-4bbd-8e4c-694a56f45cfb",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb67b6-dabb-4db8-841d-c7ebf43f4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_training_config(pretrain_name, medium)\n",
    "    file = joinpath(get_data_path(\"alphas/$pretrain_name\"), \"config.json\")\n",
    "    open(file) do f\n",
    "        d = JSON.parse(f)\n",
    "        d[\"mode\"] = \"finetune\"\n",
    "        d[\"medium\"] = medium\n",
    "        for split in [\"training\", \"validation\"]\n",
    "            d[\"$(split)_epoch_size\"] = nothing\n",
    "            d[\"$(split)_epoch_tokens\"] = nothing\n",
    "        end\n",
    "        return NamedTuple(Symbol.(keys(d)) .=> values(d))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c35ee-317c-4c0a-bd28-9c0462f9d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_epoch_size(config, training_users, validation_users)\n",
    "    @info \"Number of training sentences: $(length(training_users))\"\n",
    "    @info \"Number of validation sentences: $(length(validation_users))\"\n",
    "    config = @set config.training_epoch_size = length(training_users)\n",
    "    config = @set config.validation_epoch_size = length(validation_users)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bb2ec-1636-4880-9c87-3891e30db421",
   "metadata": {},
   "outputs": [],
   "source": [
    "function setup_training(config, outdir)\n",
    "    mkpath(outdir)\n",
    "    fn = joinpath(outdir, \"config.json\")\n",
    "    open(fn, \"w\") do f\n",
    "        write(f, JSON.json(config))\n",
    "    end\n",
    "    for split in [\"training\", \"validation\"]\n",
    "        fn = joinpath(outdir, split)\n",
    "        mkpath(fn)\n",
    "        for x in readdir(fn, join = true)\n",
    "            rm(x, recursive = true)\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e689ae-0270-4d04-944f-76a127a2c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_epochs(num_epochs, pretrain_name)\n",
    "    Random.seed!(20221221)\n",
    "    config = create_training_config(pretrain_name, medium)\n",
    "    @info \"loading data\"\n",
    "    training_users, validation_users = get_users.([\"validation\", \"test\"], (medium,))\n",
    "    sentences =\n",
    "        get_training_data(config[:cls_tokens], vcat(training_users, validation_users))\n",
    "    config = set_epoch_size(config, training_users, validation_users)\n",
    "    labels = get_labels.(ALL_METRICS, (medium,))\n",
    "    weights = get_weights.(ALL_METRICS, (medium,))\n",
    "    outdir = get_data_path(joinpath(\"alphas\", name))\n",
    "    setup_training(config, outdir)\n",
    "\n",
    "    writer_workers = 2\n",
    "    writer = Channel(2 * writer_workers)\n",
    "    for _ = 1:writer_workers\n",
    "        Threads.@spawn hdf5_writer(writer)\n",
    "    end\n",
    "    Threads.@spawn begin\n",
    "        for epoch = 0:num_epochs-1\n",
    "            for (s, t) in\n",
    "                zip([validation_users, training_users], [\"validation\", \"training\"])\n",
    "                save_epoch(sentences, labels, weights, s, config, epoch, outdir, t, writer)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed78f3-32ec-4535-82e0-5d381b149393",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2207253-2413-4538-b1c5-0b8d7d09ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function model(users, items, user_cache, user_to_index)\n",
    "    p = zeros(Float32, length(users))\n",
    "    @showprogress for i = 1:length(p)\n",
    "        u = user_to_index[users[i]]\n",
    "        p[i] = user_cache[u][items[i]+1]\n",
    "    end\n",
    "    p\n",
    "end\n",
    "\n",
    "function get_cache(metric, embeddings, user_to_index, weights, biases)\n",
    "    cache = Dict()\n",
    "    E = weights[metric] * embeddings .+ biases[metric]\n",
    "    @showprogress for u in values(user_to_index)\n",
    "        e = E[:, u]\n",
    "        if metric in [\"watch\", \"plantowatch\"]\n",
    "            e = softmax(e)\n",
    "        elseif metric == \"drop\"\n",
    "            e = sigmoid(e)\n",
    "        end\n",
    "        cache[u] = e\n",
    "    end\n",
    "    cache\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab68379-dbc8-4bbe-98da-b3339700b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_alphas()\n",
    "    file = HDF5.h5open(get_data_path(joinpath(\"alphas\", name, \"embeddings.h5\")), \"r\")\n",
    "    embeddings = read(file[\"embedding\"])\n",
    "    users = read(file[\"users\"])\n",
    "    weights = Dict(x => read(file[\"$(medium)_$(x)_weight\"])' for x in ALL_METRICS)\n",
    "    biases = Dict(x => read(file[\"$(medium)_$(x)_bias\"]) for x in ALL_METRICS)\n",
    "    close(file)\n",
    "\n",
    "    user_to_index = Dict()\n",
    "    for (i, u) in Iterators.enumerate(users)\n",
    "        user_to_index[u] = i\n",
    "    end\n",
    "\n",
    "    for metric in ALL_METRICS\n",
    "        cache = get_cache(metric, embeddings, user_to_index, weights, biases)\n",
    "        write_alpha(\n",
    "            (users, items) -> model(users, items, cache, user_to_index),\n",
    "            medium,\n",
    "            \"$name/$metric\",\n",
    "            [\"test\", \"negative\"],\n",
    "        )\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750170ca-c881-4d9e-8510-ab67d2452d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_alphas()\n",
    "    for metric in ALL_METRICS\n",
    "        for split in [\"test\"]\n",
    "            val = compute_loss(metric, medium, [\"$name/$metric\"], split)\n",
    "            @info \"$metric $split loss = $val\"\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c19b7-612d-464b-a830-519ad2c4924a",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b33f5c-3a91-49d2-b202-d0dad70916da",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_epochs(16, pretrain_name);\n",
    "# wait until the first epoch is finished\n",
    "while !isdir(get_data_path(joinpath(\"alphas\", name, \"validation\", \"1\")))\n",
    "    sleep(1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9abf5e-ae0f-4e4c-967c-5058b90dcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(`python3 Pytorch.py --outdir $name --initialize $pretrain_name`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bac6a5-a8cb-4c03-a53d-5f2cb8a8637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_alphas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe2c46-60f5-41b6-a1ec-2d0c1177ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_alphas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
