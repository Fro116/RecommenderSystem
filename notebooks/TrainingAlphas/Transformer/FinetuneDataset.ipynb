{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37131d29-4151-410e-866f-761db4964bbb",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "* Finetunes a transformer model to predict recent watches\n",
    "* A recent watch is defined as an interaction that occurred within the past $D$ days and is one of the $N$ most recent interactions for that user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478f496-3095-4fc0-9db7-699277589073",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "medium = \"\"\n",
    "task = \"\"\n",
    "name = \"Transformer/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd56259-4513-4297-a978-99c765532231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrain_name = \"all/$name\"\n",
    "name = \"$medium/$task/$name\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90afc10-1ae5-4665-937e-d58a5859ac94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Data.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d60ae-6b55-4274-85b1-c0f9c9fc7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import HDF5\n",
    "import JSON\n",
    "import MLUtils\n",
    "import Random\n",
    "import SparseArrays: AbstractSparseArray, sparse, spzeros\n",
    "import StatsBase: mean, sample\n",
    "import ThreadPinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a7882c-990d-470e-a3d3-3ebc4d1c1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThreadPinning.pinthreads(:cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6434e07-f565-474a-ac05-12c409afe141",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed0909-a7b0-4b54-ae7b-20dd5b9b789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function featurize(sentences, labels, weights, medium, user, config, training::Bool)\n",
    "    if user in keys(sentences)\n",
    "        sentence = copy(sentences[user])\n",
    "    else\n",
    "        sentence = Vector{wordtype}()\n",
    "        push!(sentence, replace(config[\"cls_tokens\"], :user, user))\n",
    "    end\n",
    "    featurize(;\n",
    "        sentence = sentence,\n",
    "        labels = map(x -> x[:, user], labels),\n",
    "        weights = map(x -> x[:, user], weights),\n",
    "        medium = medium,\n",
    "        user = user,\n",
    "        max_seq_len = config[\"max_sequence_length\"],\n",
    "        vocab_sizes = config[\"base_vocab_sizes\"],\n",
    "        pad_tokens = config[\"pad_tokens\"],\n",
    "        cls_tokens = config[\"cls_tokens\"],\n",
    "        mask_tokens = config[\"mask_tokens\"],\n",
    "        empty_tokens = config[\"empty_tokens\"],\n",
    "        causal = config[\"causal\"],\n",
    "        training = training,\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002d542-1609-4c35-b8f3-32e0e5b16647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function featurize(;\n",
    "    sentence::Vector{wordtype},\n",
    "    labels,\n",
    "    weights,\n",
    "    medium,\n",
    "    user,\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    pad_tokens,\n",
    "    cls_tokens,\n",
    "    mask_tokens,\n",
    "    empty_tokens,\n",
    "    causal,\n",
    "    training,\n",
    ")\n",
    "    sentence = subset_sentence(\n",
    "        sentence,\n",
    "        min(length(sentence), max_seq_len - 1);\n",
    "        recent = true,\n",
    "        rng = nothing,\n",
    "    )\n",
    "\n",
    "    if causal\n",
    "        sentence[end] = replace(sentence[end], :timestamp, 1)\n",
    "    else\n",
    "        # add mask token  \n",
    "        if task == \"temporal_causal\"\n",
    "            masked_word = replace(mask_tokens, :timestamp, 1)\n",
    "            masked_word = replace(masked_word, :position, length(sentence))\n",
    "        else\n",
    "            @assert false\n",
    "        end\n",
    "        masked_word = replace(masked_word, :user, user)\n",
    "        push!(sentence, masked_word)\n",
    "    end\n",
    "\n",
    "    # get tokenized sentences\n",
    "    tokens =\n",
    "        vec.(\n",
    "            get_token_ids(\n",
    "                [sentence],\n",
    "                max_seq_len,\n",
    "                extract(vocab_sizes, :position),\n",
    "                pad_tokens,\n",
    "                cls_tokens,\n",
    "            ),\n",
    "        )\n",
    "    positions = [length(sentence)]\n",
    "\n",
    "    featurized_labels = Dict(\n",
    "        x => (\n",
    "            item = spzeros(Float32, num_items(x)),\n",
    "            rating = spzeros(Float32, num_items(x)),\n",
    "        ) for x in [\"anime\", \"manga\"]\n",
    "    )\n",
    "    featurized_labels[medium][:item] .= labels[1]\n",
    "    featurized_labels[medium][:rating] .= labels[2]\n",
    "\n",
    "    featurized_weights = Dict(\n",
    "        x => (\n",
    "            item = spzeros(Float32, num_items(x)),\n",
    "            rating = spzeros(Float32, num_items(x)),\n",
    "        ) for x in [\"anime\", \"manga\"]\n",
    "    )\n",
    "    featurized_weights[medium][:item] .= weights[1]\n",
    "    featurized_weights[medium][:rating] .= weights[2]\n",
    "\n",
    "    tokens, positions, featurized_labels, featurized_weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7ebe9-122d-4b77-a00b-87206686e4b3",
   "metadata": {},
   "source": [
    "# Data colleciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b753540-f3ec-4fda-af5d-9aaeb7d8a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_labels(task, content, medium)\n",
    "    df = cat(\n",
    "        get_split(\"validation\", task, content, medium),\n",
    "        get_split(\"test\", task, content, medium),\n",
    "    )\n",
    "    sparse(df.item, df.user, df.rating, num_items(medium), num_users())\n",
    "end\n",
    "\n",
    "function get_labels(task)\n",
    "    [get_labels(task, content, medium) for content in [\"implicit\", \"explicit\"]]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeaf61e-3c3e-420f-a95e-d7ddb96ad53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_weights(task, content, medium)\n",
    "    df = cat(\n",
    "        get_split(\"validation\", task, content, medium),\n",
    "        get_split(\"test\", task, content, medium),\n",
    "    )\n",
    "    w = vcat(\n",
    "        powerdecay(\n",
    "            get_counts(\"validation\", task, content, medium),\n",
    "            weighting_scheme(\"inverse\"),\n",
    "        ),\n",
    "        powerdecay(get_counts(\"test\", task, content, medium), weighting_scheme(\"inverse\")),\n",
    "    )\n",
    "\n",
    "    sparse(df.item, df.user, w, num_items(medium), num_users())\n",
    "end\n",
    "\n",
    "function get_weights(task)\n",
    "    [get_weights(task, content, medium) for content in [\"implicit\", \"explicit\"]]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2eeb2a-806a-4cbf-9c2e-8bbe5c17124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_users(task, medium)\n",
    "    training = collect(Set(get_split(\"validation\", task, \"implicit\", medium).user))\n",
    "    test = collect(\n",
    "        union(\n",
    "            [\n",
    "                Set(get_split(\"test\", task, x, medium; fields = [:user]).user) for\n",
    "                x in ALL_CONTENTS\n",
    "            ]...,\n",
    "        ),\n",
    "    )\n",
    "    training, test\n",
    "end\n",
    "\n",
    "function get_users(task)\n",
    "    get_users(task, medium)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed260a-b454-4690-ab8f-15ba4d686b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sentences(config, task, users)\n",
    "    get_training_data(\n",
    "        task,\n",
    "        config[\"media\"],\n",
    "        config[\"include_ptw_impressions\"],\n",
    "        config[\"cls_tokens\"],\n",
    "        config[\"empty_tokens\"],\n",
    "        config[\"causal\"];\n",
    "        chunks = 1,\n",
    "        users = users,\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50183c75-f670-425e-b08e-c72efe4e8988",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ac800-38d2-4ed9-9963-05c176baef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_rngs(seed)\n",
    "    rng = Random.Xoshiro(seed)\n",
    "    Random.seed!(rand(rng, UInt64))\n",
    "    rng\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb67b6-dabb-4db8-841d-c7ebf43f4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_training_config(pretrain_name)\n",
    "    file = joinpath(get_data_path(\"alphas/$pretrain_name\"), \"config.json\")\n",
    "    open(file) do f\n",
    "        d = JSON.parse(f)\n",
    "        d[\"mode\"] = \"finetune\"\n",
    "        return d\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c35ee-317c-4c0a-bd28-9c0462f9d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_epoch_size!(config, training_users, validation_users)\n",
    "    num_tokens = length(training_users) * config[\"max_sequence_length\"]\n",
    "    @info \"Number of training tokens: $(num_tokens)\"\n",
    "    @info \"Number of training sentences: $(length(training_users))\"\n",
    "    @info \"Number of validation sentences: $(length(validation_users))\"\n",
    "    config[\"training_epoch_size\"] = length(training_users)\n",
    "    config[\"validation_epoch_size\"] = length(validation_users)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31c5b2-42e2-455a-bd3f-0ca4eab8cb8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function setup_training(config, outdir)\n",
    "    if !isdir(outdir)\n",
    "        mkdir(outdir)\n",
    "    end\n",
    "    for x in readdir(outdir, join = true)\n",
    "        if isfile(x)\n",
    "            rm(x)\n",
    "        end\n",
    "    end\n",
    "    fn = joinpath(outdir, \"..\", \"config.json\")\n",
    "    open(fn, \"w\") do f\n",
    "        write(f, JSON.json(config))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fa056-8deb-486d-9854-6e0991bc4714",
   "metadata": {},
   "source": [
    "# Disk I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7abb52-f79b-4e89-8611-aa1506d2808e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function save_features(sentences, labels, weights, users, config, training, filename)\n",
    "    features = []\n",
    "    for x in users\n",
    "        push!(features, featurize(sentences, labels, weights, medium, x, config, training))\n",
    "    end\n",
    "\n",
    "    d = Dict{String,AbstractArray}()\n",
    "    d[\"causal\"] = [config[\"causal\"]]\n",
    "    collate = MLUtils.batch\n",
    "    embed_names = [\n",
    "        \"anime\",\n",
    "        \"manga\",\n",
    "        \"rating\",\n",
    "        \"timestamp\",\n",
    "        \"status\",\n",
    "        \"completion\",\n",
    "        \"user\",\n",
    "        \"position\",\n",
    "    ]\n",
    "    for (i, name) in Iterators.enumerate(embed_names)\n",
    "        d[name] = collate([x[1][i] for x in features])\n",
    "    end\n",
    "    d[\"positions\"] = collate([x[2] for x in features])\n",
    "\n",
    "    for medium in [\"anime\", \"manga\"]\n",
    "        for task in [\"item\", \"rating\"]\n",
    "            record_sparse_array!(\n",
    "                d,\n",
    "                \"labels_$(medium)_$(task)\",\n",
    "                collate([x[3][medium][Symbol(task)] for x in features]),\n",
    "                extract(config[\"vocab_sizes\"], Symbol(medium)),\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "    for medium in [\"anime\", \"manga\"]\n",
    "        for task in [\"item\", \"rating\"]\n",
    "            record_sparse_array!(\n",
    "                d,\n",
    "                \"weights_$(medium)_$(task)\",\n",
    "                collate([x[4][medium][Symbol(task)] for x in features]),\n",
    "                extract(config[\"vocab_sizes\"], Symbol(medium)),\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "\n",
    "    HDF5.h5open(filename, \"w\") do file\n",
    "        for (k, v) in d\n",
    "            write(file, k, v)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function record_sparse_array!(d::Dict, name::String, x::AbstractSparseArray, vocab_size)\n",
    "    i, j, v = SparseArrays.findnz(x)\n",
    "    d[name*\"_i\"] = i\n",
    "    d[name*\"_j\"] = j\n",
    "    d[name*\"_v\"] = v\n",
    "    d[name*\"_size\"] = [vocab_size, size(x)[2]]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18314729-7634-4b7e-a452-1c73a0922d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function advance!(filename)\n",
    "    # check to see if we should write the next shard\n",
    "    outdir = dirname(filename)\n",
    "    files = readdir(outdir)\n",
    "    suffix = basename(filename) * \".read\"\n",
    "    files = [x for x in files if occursin(suffix, basename(x))]\n",
    "    if length(files) == 0\n",
    "        return false\n",
    "    end\n",
    "    workers = Set(split(x, \".\")[end-1:end] for x in files)\n",
    "    @assert length(workers) == 1\n",
    "    world_size, num_workers = parse.(Int, first(workers))\n",
    "    advance = length(files) == world_size * num_workers\n",
    "    if advance\n",
    "        rm(\"$filename.complete\")\n",
    "        rm(filename)\n",
    "        for x in files\n",
    "            rm(joinpath(outdir, x))\n",
    "        end\n",
    "    end\n",
    "    advance\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8cc95c-b656-49d2-a65f-1504fbe4b50e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function spawn_feature_workers(\n",
    "    sentences,\n",
    "    labels,\n",
    "    weights,\n",
    "    users,\n",
    "    config,\n",
    "    rng,\n",
    "    training,\n",
    "    outdir,\n",
    ")\n",
    "    # writes data to \"$outdir/data.$worker.h5\" in a hot loop\n",
    "    # whenever that file disappears, we populate it with a new batch\n",
    "    chunk_size = config[\"chunk_size\"]\n",
    "    workers = training ? config[\"num_training_shards\"] : config[\"num_validation_shards\"]\n",
    "    stem = training ? \"training\" : \"validation\"\n",
    "    rngs = [Random.Xoshiro(rand(rng, UInt64)) for _ = 1:workers]\n",
    "    for (i, batch) in Iterators.enumerate(\n",
    "        Iterators.partition(users, div(length(users), workers, RoundUp)),\n",
    "    )\n",
    "        Threads.@spawn begin\n",
    "            rng = rngs[i]\n",
    "            while true\n",
    "                Random.shuffle!(rng, batch)\n",
    "                for (j, chunk) in\n",
    "                    Iterators.enumerate(Iterators.partition(batch, chunk_size))\n",
    "                    filename = joinpath(outdir, \"$stem.$i.h5\")\n",
    "                    save_features(\n",
    "                        sentences,\n",
    "                        labels,\n",
    "                        weights,\n",
    "                        chunk,\n",
    "                        config,\n",
    "                        training,\n",
    "                        filename,\n",
    "                    )\n",
    "                    open(\"$filename.complete\", \"w\") do f\n",
    "                        write(f, \"$j\")\n",
    "                    end\n",
    "                    GC.gc()\n",
    "                    while isdir(outdir) && !advance!(filename)\n",
    "                        sleep(1)\n",
    "                    end\n",
    "                    if !isdir(outdir)\n",
    "                        break\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c19b7-612d-464b-a830-519ad2c4924a",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bd779-8b8f-4027-ba80-34d147d90a4d",
   "metadata": {
    "papermill": {
     "duration": 1.708366,
     "end_time": "2023-05-15T05:04:49.317062",
     "exception": false,
     "start_time": "2023-05-15T05:04:47.608696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_checkpoint = nothing\n",
    "config_epoch = nothing\n",
    "reset_lr_schedule = true\n",
    "rng = set_rngs(20221221)\n",
    "config = create_training_config(pretrain_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece189f1-11f9-4774-b519-635444144d93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-05-15T05:04:49.320685",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@info \"loading data\"\n",
    "training_users, test_users = get_users(task)\n",
    "sentences = get_sentences(config, task, Set(vcat(training_users, test_users)))\n",
    "labels = get_labels(task)\n",
    "weights = get_weights(task)\n",
    "set_epoch_size!(config, training_users, test_users);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb0a73-ced6-4e7c-abe3-bfadf15a08b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "outdir = get_data_path(joinpath(\"alphas\", name, \"training\"))\n",
    "setup_training(config, outdir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b33f5c-3a91-49d2-b202-d0dad70916da",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDF5.h5open(joinpath(outdir, \"users.h5\"), \"w\") do file\n",
    "    write(file, \"training\", training_users)\n",
    "    write(file, \"test\", test_users)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40387edf-e126-4c06-a0ad-48b749fc0d35",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spawn_feature_workers(sentences, labels, weights, training_users, config, rng, true, outdir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa999b-7bc5-4f94-a562-cf0a773eaecc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spawn_feature_workers(sentences, labels, weights, test_users, config, rng, false, outdir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9abf5e-ae0f-4e4c-967c-5058b90dcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(`python3 Pytorch.py --outdir $name --initialize $pretrain_name --epochs 8`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184b49c-a8b8-44d5-8672-a6cce64fb09f",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe2c46-60f5-41b6-a1ec-2d0c1177ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = HDF5.h5open(get_data_path(joinpath(\"alphas\", name, \"embeddings.h5\")), \"r\")\n",
    "embeddings = read(file[\"embedding\"])\n",
    "users = read(file[\"users\"])\n",
    "item_weight = read(file[\"$(medium)_item_weight\"])'\n",
    "item_bias = read(file[\"$(medium)_item_bias\"])\n",
    "rating_weight = read(file[\"$(medium)_rating_weight\"])'\n",
    "rating_bias = read(file[\"$(medium)_rating_bias\"])\n",
    "close(file)\n",
    "\n",
    "user_to_index = Dict()\n",
    "for (i, u) in Iterators.enumerate(users)\n",
    "    user_to_index[u] = i\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2207253-2413-4538-b1c5-0b8d7d09ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we only record predictions for test users\n",
    "function model(users, items, user_cache)\n",
    "    ratings = zeros(Float32, length(users))\n",
    "    @showprogress for i = 1:length(ratings)\n",
    "        if users[i] ∉ keys(user_to_index)\n",
    "            continue\n",
    "        end\n",
    "        u = user_to_index[users[i]]\n",
    "        ratings[i] = user_cache[u][items[i]]\n",
    "    end\n",
    "    ratings\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd86902-af24-4e35-8a0a-4736a9e51695",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cache = Dict()\n",
    "@showprogress for u in values(user_to_index)\n",
    "    e = item_weight * embeddings[:, u] + item_bias\n",
    "    item_cache[u] = softmax(e)\n",
    "end\n",
    "write_alpha(\n",
    "    (users, items) -> model(users, items, item_cache),\n",
    "    medium,\n",
    "    joinpath(name, \"implicit\");\n",
    "    task = task,\n",
    "    log = true,\n",
    "    log_task = task,\n",
    "    log_content = \"implicit\",\n",
    "    log_alphas = String[],\n",
    "    log_splits = [\"test\"],\n",
    ")\n",
    "item_cache = nothing;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53445bf-ef56-4b21-9d80-fb1c1198fb8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rating_cache = Dict()\n",
    "@showprogress for u in values(user_to_index)\n",
    "    e = rating_weight * embeddings[:, u] + rating_bias\n",
    "    rating_cache[u] = e\n",
    "end\n",
    "user_cache = Dict()\n",
    "write_alpha(\n",
    "    (users, items) -> model(users, items, rating_cache),\n",
    "    medium,\n",
    "    joinpath(name, \"explicit\");\n",
    "    task = task,\n",
    "    log = true,\n",
    "    log_task = task,\n",
    "    log_content = \"explicit\",\n",
    "    log_alphas = String[],\n",
    "    log_splits = [\"test\"],\n",
    ")\n",
    "rating_cache = nothing;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3043c-3109-4972-b8ce-5dee407cfe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(outdir, recursive = true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
