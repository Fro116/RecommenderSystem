{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37131d29-4151-410e-866f-761db4964bbb",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "* Finetunes a transformer model to predict recent watches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90afc10-1ae5-4665-937e-d58a5859ac94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Data.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d60ae-6b55-4274-85b1-c0f9c9fc7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import H5Zblosc\n",
    "import HDF5\n",
    "import JSON\n",
    "import MLUtils\n",
    "import NNlib: sigmoid\n",
    "import Random\n",
    "import SparseArrays: AbstractSparseArray, sparse, spzeros\n",
    "import StatsBase: mean, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478f496-3095-4fc0-9db7-699277589073",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "medium = \"\";\n",
    "mode = \"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd56259-4513-4297-a978-99c765532231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "const version = \"v1\"\n",
    "const pretrain_name = \"all/Transformer/$version\"\n",
    "const name = \"$medium/Transformer/$version\"\n",
    "set_logging_outdir(name);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50183c75-f670-425e-b08e-c72efe4e8988",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e9a34-3d91-4b33-ab88-c39556b1a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_size(vocab_sizes, medium)\n",
    "    if medium == \"manga\"\n",
    "        return vocab_sizes[1]\n",
    "    elseif medium == \"anime\"\n",
    "        return vocab_sizes[2]\n",
    "    end\n",
    "end\n",
    "\n",
    "function SparseArrays.sparse(x::RatingsDataset, medium::String, vocab_sizes::Vector)\n",
    "    SparseArrays.sparse(\n",
    "        x.itemid .+ 1,\n",
    "        x.userid .+ 1,\n",
    "        x.metric,\n",
    "        get_size(vocab_sizes, medium),\n",
    "        num_users(),\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b753540-f3ec-4fda-af5d-9aaeb7d8a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_training_split(metric, medium, fields)\n",
    "    fields = vcat(fields, [:updated_at, :update_order])\n",
    "    _, df = training_test_split(get_split(\"training\", metric, medium, fields))\n",
    "    df = @set df.updated_at = []\n",
    "    df = @set df.update_order = []\n",
    "    df\n",
    "end\n",
    "\n",
    "function get_labels(split, metric, medium, vocab_sizes)\n",
    "    if split == \"training\"\n",
    "        df = get_training_split(metric, medium, [:userid, :itemid, :metric])\n",
    "    elseif split == \"test\"\n",
    "        df = get_split(\"test\", metric, medium, [:userid, :itemid, :metric])\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    sparse(df, medium, vocab_sizes)\n",
    "end\n",
    "\n",
    "function get_weights(split, metric, medium, vocab_sizes)\n",
    "    if split == \"training\"\n",
    "        df = get_training_split(metric, medium, [:userid, :itemid, :metric])\n",
    "    elseif split == \"test\"\n",
    "        df = get_split(\"test\", metric, medium, [:userid, :itemid, :metric])\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    df = @set df.metric = powerdecay(get_counts(df.userid), -1.0f0)\n",
    "    sparse(df, medium, vocab_sizes)\n",
    "end\n",
    "\n",
    "function get_users(split, medium)\n",
    "    if split == \"training\"\n",
    "        _, df = training_test_split(\n",
    "            get_raw_split(\n",
    "                \"training\",\n",
    "                medium,\n",
    "                [:userid, :updated_at, :update_order],\n",
    "                nothing,\n",
    "            ),\n",
    "        )\n",
    "    else\n",
    "        df = get_raw_split(split, medium, [:userid], nothing)\n",
    "    end\n",
    "    collect(Set(df.userid))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed0909-a7b0-4b54-ae7b-20dd5b9b789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function tokenize(sentences, labels, weights, medium, userid, config)\n",
    "    if userid in keys(sentences)\n",
    "        sentence = copy(sentences[userid])\n",
    "    else\n",
    "        sentence = Vector{wordtype}()\n",
    "        push!(sentence, replace(config[:cls_tokens], :userid, userid))\n",
    "    end\n",
    "    tokenize(;\n",
    "        sentence = sentence,\n",
    "        labels = map(x -> x[:, userid+1], labels),\n",
    "        weights = map(x -> x[:, userid+1], weights),\n",
    "        medium = medium,\n",
    "        userid = userid,\n",
    "        max_seq_len = config[:max_sequence_length],\n",
    "        vocab_sizes = config[:vocab_sizes],\n",
    "        pad_tokens = config[:pad_tokens],\n",
    "        cls_tokens = config[:cls_tokens],\n",
    "        mask_tokens = config[:mask_tokens],\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002d542-1609-4c35-b8f3-32e0e5b16647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function tokenize(;\n",
    "    sentence::Vector{wordtype},\n",
    "    labels,\n",
    "    weights,\n",
    "    medium,\n",
    "    userid,\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    pad_tokens,\n",
    "    cls_tokens,\n",
    "    mask_tokens,\n",
    ")\n",
    "    # get inputs\n",
    "    sentence =\n",
    "        subset_sentence(sentence, min(length(sentence), max_seq_len - 1); recent = true)\n",
    "    masked_word = mask_tokens\n",
    "    masked_word = replace(masked_word, :updated_at, 1)\n",
    "    masked_word = replace(masked_word, :position, length(sentence) - 1)\n",
    "    masked_word = replace(masked_word, :userid, userid)\n",
    "    push!(sentence, masked_word)\n",
    "    tokens = get_token_ids(sentence, max_seq_len, pad_tokens, false)\n",
    "\n",
    "    # get outputs\n",
    "    positions = [length(sentence) - 1]\n",
    "    tokenized_labels = Dict(\n",
    "        x => Dict(y => spzeros(Float32, get_size(vocab_sizes, x)) for y in ALL_METRICS)\n",
    "        for x in ALL_MEDIUMS\n",
    "    )\n",
    "    tokenized_weights = Dict(\n",
    "        x => Dict(y => spzeros(Float32, get_size(vocab_sizes, x)) for y in ALL_METRICS)\n",
    "        for x in ALL_MEDIUMS\n",
    "    )\n",
    "    for i = 1:length(ALL_METRICS)\n",
    "        tokenized_labels[medium][ALL_METRICS[i]] .= labels[i]\n",
    "        tokenized_weights[medium][ALL_METRICS[i]] .= weights[i]\n",
    "    end\n",
    "    tokens, positions, tokenized_labels, tokenized_weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdccd72-b45a-4987-aed5-4c2eb5a27a54",
   "metadata": {},
   "source": [
    "# Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b8550-0340-4abf-9782-ed908f206687",
   "metadata": {},
   "outputs": [],
   "source": [
    "function record_sparse_array!(d::Dict, name::String, x::AbstractSparseArray)\n",
    "    i, j, v = SparseArrays.findnz(x)\n",
    "    d[name*\"_i\"] = i .- 1\n",
    "    d[name*\"_j\"] = j .- 1\n",
    "    d[name*\"_v\"] = v\n",
    "    d[name*\"_size\"] = collect(size(x))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea2b0a-d24e-4712-a3cf-1b6c03e72ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_tokens(sentences, labels, weights, users, config, filename)\n",
    "    tokens = Any[nothing for _ = 1:length(users)]\n",
    "    Threads.@threads for i = 1:length(users)\n",
    "        tokens[i] = tokenize(sentences, labels, weights, medium, users[i], config)\n",
    "    end\n",
    "\n",
    "    d = Dict{String,AbstractArray}()\n",
    "    collate = MLUtils.batch\n",
    "    for (i, name) in Iterators.enumerate(config.vocab_names)\n",
    "        d[name] = collate([x[1][i] for x in tokens])\n",
    "    end\n",
    "    d[\"positions\"] = collate([x[2] for x in tokens])\n",
    "    for medium in ALL_MEDIUMS\n",
    "        for metric in ALL_METRICS\n",
    "            record_sparse_array!(\n",
    "                d,\n",
    "                \"labels_$(medium)_$(metric)\",\n",
    "                collate([x[3][medium][metric] for x in tokens]),\n",
    "            )\n",
    "            record_sparse_array!(\n",
    "                d,\n",
    "                \"weights_$(medium)_$(metric)\",\n",
    "                collate([x[4][medium][metric] for x in tokens]),\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "    HDF5.h5open(filename, \"w\") do f\n",
    "        for (k, v) in d\n",
    "            f[k, blosc = 3] = v\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11aa3e7-2ffd-4657-9622-ea9ed8cc325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_epoch(sentences, labels, weights, users, config, epoch, outdir, split)\n",
    "    outdir = joinpath(outdir, split, \"$epoch\")\n",
    "    mkpath(outdir)\n",
    "    Random.shuffle!(users)\n",
    "    num_sentences = 0\n",
    "    expected_num_sentences = getfield(config, Symbol(\"$(split)_epoch_size\"))\n",
    "    @showprogress enabled = epoch == 0 for (i, batch) in collect(\n",
    "        Iterators.enumerate(Iterators.partition(users, config.batch_size)),\n",
    "    )\n",
    "        num_sentences += length(batch)\n",
    "        save_tokens(\n",
    "            sentences,\n",
    "            labels,\n",
    "            weights,\n",
    "            batch,\n",
    "            config,\n",
    "            joinpath(outdir, \"$(i-1).h5\"),\n",
    "        )\n",
    "    end\n",
    "    @assert num_sentences == expected_num_sentences\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb3995-af54-4bbd-8e4c-694a56f45cfb",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb67b6-dabb-4db8-841d-c7ebf43f4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_training_config(pretrain_name, medium)\n",
    "    file = joinpath(get_data_path(\"alphas/$pretrain_name/0\"), \"config.json\")\n",
    "    open(file) do f\n",
    "        d = JSON.parse(f)\n",
    "        d[\"mode\"] = \"finetune\"\n",
    "        d[\"medium\"] = medium\n",
    "        for split in [\"training\", \"validation\"]\n",
    "            d[\"$(split)_epoch_size\"] = nothing\n",
    "            d[\"$(split)_epoch_tokens\"] = nothing\n",
    "        end\n",
    "        return NamedTuple(Symbol.(keys(d)) .=> values(d))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c35ee-317c-4c0a-bd28-9c0462f9d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_epoch_size(config, users)\n",
    "    for (name, u) in zip([\"training\", \"validation\"], users)\n",
    "        @info \"Number of $name sentences: $(length(u))\"\n",
    "    end\n",
    "    merge(\n",
    "        config,\n",
    "        (training_epoch_size = length(users[1]), validation_epoch_size = length(users[2])),\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bb2ec-1636-4880-9c87-3891e30db421",
   "metadata": {},
   "outputs": [],
   "source": [
    "function setup_training(config, outdir)\n",
    "    mkpath(outdir)\n",
    "    fn = joinpath(outdir, \"config.json\")\n",
    "    open(fn, \"w\") do f\n",
    "        write(f, JSON.json(config))\n",
    "    end\n",
    "    for split in [\"training\", \"validation\"]\n",
    "        fn = joinpath(outdir, split)\n",
    "        mkpath(fn)\n",
    "        for x in readdir(fn, join = true)\n",
    "            rm(x, recursive = true)\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e689ae-0270-4d04-944f-76a127a2c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_epochs(num_epochs, pretrain_name)\n",
    "    Random.seed!(20221221)\n",
    "    config = create_training_config(pretrain_name, medium)\n",
    "    @info \"loading data\"\n",
    "    users = get_users.([\"training\", \"test\"], (medium,))\n",
    "    sentences = [\n",
    "        get_training_data(\n",
    "            config[:cls_tokens],\n",
    "            config[:max_sequence_length],\n",
    "            nothing,\n",
    "            vcat(users...),\n",
    "            holdout,\n",
    "        ) for holdout in [true, false]\n",
    "    ]\n",
    "    config = set_epoch_size(config, users)\n",
    "    labels = [\n",
    "        [get_labels(s, m, medium, config.vocab_sizes) for m in ALL_METRICS] for\n",
    "        s in [\"training\", \"test\"]\n",
    "    ]\n",
    "    weights = [\n",
    "        [get_weights(s, m, medium, config.vocab_sizes) for m in ALL_METRICS] for\n",
    "        s in [\"training\", \"test\"]\n",
    "    ]\n",
    "    outdir = get_data_path(joinpath(\"alphas\", name, \"0\"))\n",
    "    setup_training(config, outdir)\n",
    "\n",
    "    for epoch = 0:num_epochs-1\n",
    "        for (s, l, w, u, t) in\n",
    "            zip(sentences, labels, weights, users, [\"training\", \"validation\"])\n",
    "            save_epoch(s, l, w, u, config, epoch, outdir, t)\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7677b-7a04-4357-ab52-403408ad916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function copy_epochs(num_epochs, num_source_epochs)\n",
    "    source_epoch = 0\n",
    "    for s in [\"training\", \"validation\"]\n",
    "        getpath(epoch) = get_data_path(joinpath(\"alphas\", name, \"0\", s, \"$epoch\"))\n",
    "        for i = num_source_epochs:num_epochs-1\n",
    "            src = getpath(source_epoch)\n",
    "            source_epoch = (source_epoch + 1) % num_source_epochs\n",
    "            dst = getpath(i)\n",
    "            mkdir(dst)\n",
    "            for basename in readdir(src)\n",
    "                cp(\"$src/$basename\", \"$dst/$basename\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed78f3-32ec-4535-82e0-5d381b149393",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23b440-80ac-40a9-8323-486dfe189e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "function model(users, items, cache)\n",
    "    p = zeros(Float32, length(users))\n",
    "    @showprogress for i = 1:length(p)\n",
    "        p[i] = cache[users[i]][items[i]+1]\n",
    "    end\n",
    "    p\n",
    "end\n",
    "\n",
    "function get_cache(metric::String, embeddings, user_to_index, seen)\n",
    "    cache = Dict()\n",
    "    @showprogress for (user, index) in user_to_index\n",
    "        e = embeddings[:, index]\n",
    "        if metric in [\"watch\", \"plantowatch\"]\n",
    "            e = exp.(e) # the model saves log-softmax values\n",
    "            e[seen[:, user+1].nzind] .= 0 # zero out watched items\n",
    "            e = e ./ sum(e)\n",
    "        elseif metric == \"drop\"\n",
    "            e = sigmoid.(e)\n",
    "        end\n",
    "        cache[user] = e\n",
    "    end\n",
    "    cache\n",
    "end;\n",
    "\n",
    "function get_cache(metric::String, medium::String)\n",
    "    if metric in [\"watch\", \"plantowatch\"]\n",
    "        df = get_raw_split(\"training\", medium, [:userid, :itemid], nothing)\n",
    "        users = Set(get_users(\"test\", medium))\n",
    "        df = filter(df, df.userid .∈ (users,))\n",
    "        df = @set df.metric = ones(Float32, length(df.userid))\n",
    "        seen = sparse(df, medium)\n",
    "    else\n",
    "        seen = nothing\n",
    "    end\n",
    "\n",
    "    cache = Dict()\n",
    "    shard = 0\n",
    "    while true\n",
    "        fn = get_data_path(joinpath(\"alphas\", name, \"embeddings.$shard.h5\"))\n",
    "        shard += 1\n",
    "        if !isfile(fn)\n",
    "            break\n",
    "        end\n",
    "        file = HDF5.h5open(fn, \"r\")\n",
    "        users = read(file[\"users\"])\n",
    "        user_to_index = Dict()\n",
    "        for (i, u) in Iterators.enumerate(users)\n",
    "            user_to_index[u] = i\n",
    "        end\n",
    "        preds = read(file[\"$(medium)_$(metric)\"])\n",
    "        cache = merge(cache, get_cache(metric, preds, user_to_index, seen))\n",
    "        close(file)\n",
    "    end\n",
    "    cache\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab68379-dbc8-4bbe-98da-b3339700b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_alphas()\n",
    "    for metric in ALL_METRICS\n",
    "        cache = get_cache(metric, medium)\n",
    "        write_alpha(\n",
    "            (users, items) -> model(users, items, cache),\n",
    "            medium,\n",
    "            \"$name/$metric\",\n",
    "            [\"test\", \"negative\"],\n",
    "        )\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750170ca-c881-4d9e-8510-ab67d2452d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_alphas()\n",
    "    for metric in ALL_METRICS\n",
    "        for split in [\"test\"]\n",
    "            val = compute_loss(metric, medium, [\"$name/$metric\"], split)\n",
    "            @info \"$metric $split loss = $val\"\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc3752-55b4-4d84-a2b6-ab23fc0768fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "function cleanup()\n",
    "    fn = get_data_path(joinpath(\"alphas\", name))\n",
    "    rm(joinpath(fn, \"0\"); recursive = true)\n",
    "    shard = 0\n",
    "    while true\n",
    "        fn = get_data_path(joinpath(\"alphas\", name, \"embeddings.$shard.h5\"))\n",
    "        if !isfile(fn)\n",
    "            break\n",
    "        end\n",
    "        rm(fn)\n",
    "        shard += 1\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c19b7-612d-464b-a830-519ad2c4924a",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b33f5c-3a91-49d2-b202-d0dad70916da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"dataset\"\n",
    "    save_epochs(4, pretrain_name)\n",
    "    copy_epochs(16, 4)\n",
    "elseif mode == \"train\"\n",
    "    run(`python3 Pytorch.py --outdir $name --initialize $pretrain_name`)\n",
    "    save_alphas()\n",
    "    log_alphas()\n",
    "    cleanup()\n",
    "else\n",
    "    @assert false\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
