{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478f496-3095-4fc0-9db7-699277589073",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "medium = \"anime\"\n",
    "task = \"temporal\"\n",
    "name = \"Transformer/v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd56259-4513-4297-a978-99c765532231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrain_name = \"all/$name\"\n",
    "name = \"$medium/$task/$name\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90afc10-1ae5-4665-937e-d58a5859ac94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"../Alpha.ipynb\")\n",
    "@nbinclude(\"Data.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d60ae-6b55-4274-85b1-c0f9c9fc7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux\n",
    "import HDF5\n",
    "import JSON\n",
    "import Random\n",
    "import SparseArrays: AbstractSparseArray, sparse, spzeros\n",
    "import StatsBase: mean, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6434e07-f565-474a-ac05-12c409afe141",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed0909-a7b0-4b54-ae7b-20dd5b9b789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function featurize(sentences, labels, weights, medium, user, config, training::Bool)\n",
    "    featurize(;\n",
    "        sentence = user in keys(sentences) ? sentences[user] : eltype(values(sentences))(),\n",
    "        labels = map(x -> x[:, user], labels),\n",
    "        weights = map(x -> x[:, user], weights),\n",
    "        medium = medium,\n",
    "        user = user,\n",
    "        max_seq_len = config[\"max_sequence_length\"],\n",
    "        vocab_sizes = config[\"base_vocab_sizes\"],\n",
    "        pad_tokens = config[\"pad_tokens\"],\n",
    "        cls_tokens = config[\"cls_tokens\"],\n",
    "        mask_tokens = config[\"mask_tokens\"],\n",
    "        empty_tokens = config[\"empty_tokens\"],\n",
    "        training = training,\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002d542-1609-4c35-b8f3-32e0e5b16647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function featurize(;\n",
    "    sentence::Vector{wordtype},\n",
    "    labels,\n",
    "    weights,\n",
    "    medium,\n",
    "    user,\n",
    "    max_seq_len,\n",
    "    vocab_sizes,\n",
    "    pad_tokens,\n",
    "    cls_tokens,\n",
    "    mask_tokens,\n",
    "    empty_tokens,\n",
    "    training,\n",
    ")\n",
    "    sentence = subset_sentence(\n",
    "        sentence,\n",
    "        min(length(sentence), max_seq_len - 1);\n",
    "        recent = true,\n",
    "        keep_first = false,\n",
    "        rng = nothing,\n",
    "    )\n",
    "\n",
    "    # add masking token    \n",
    "    if task == \"temporal\"\n",
    "        masked_word = replace(mask_tokens, :timestamp, 1)\n",
    "    elseif task == \"temporal_causal\"\n",
    "        masked_word = replace(mask_tokens, :timestamp, 1)\n",
    "        masked_word = replace(masked_word, :position, length(s))\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    masked_word = replace(masked_word, :user, user)\n",
    "    push!(sentence, masked_word)\n",
    "    masked_pos = length(sentence)\n",
    "    seq_len = max_seq_len\n",
    "\n",
    "    # get tokenized sentences\n",
    "    tokens =\n",
    "        vec.(\n",
    "            get_token_ids(\n",
    "                [sentence],\n",
    "                seq_len,\n",
    "                extract(vocab_sizes, :position),\n",
    "                pad_tokens,\n",
    "                cls_tokens,\n",
    "            ),\n",
    "        )\n",
    "    positions = [masked_pos]\n",
    "\n",
    "    featurized_labels = Dict(\n",
    "        x => (\n",
    "            item = spzeros(Float32, num_items(x)),\n",
    "            rating = spzeros(Float32, num_items(x)),\n",
    "        ) for x in [\"anime\", \"manga\"]\n",
    "    )\n",
    "    featurized_labels[medium][:item] .= labels[1]\n",
    "    featurized_labels[medium][:rating] .= labels[2]\n",
    "\n",
    "    featurized_weights = Dict(\n",
    "        x => (\n",
    "            item = spzeros(Float32, num_items(x)),\n",
    "            rating = spzeros(Float32, num_items(x)),\n",
    "        ) for x in [\"anime\", \"manga\"]\n",
    "    )\n",
    "    featurized_weights[medium][:item] .= weights[1]\n",
    "    featurized_weights[medium][:rating] .= weights[2]\n",
    "\n",
    "    tokens, positions, featurized_labels, featurized_weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7ebe9-122d-4b77-a00b-87206686e4b3",
   "metadata": {},
   "source": [
    "# Data colleciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b753540-f3ec-4fda-af5d-9aaeb7d8a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_labels(task, content, medium)\n",
    "    df = cat(\n",
    "        get_split(\"validation\", task, content, medium),\n",
    "        get_split(\"test\", task, content, medium),\n",
    "    )\n",
    "    if content == \"explicit\"\n",
    "        baseline = read_params(\"$medium/$task/ExplicitUserItemBiases\")\n",
    "        for i = 1:length(df.rating)\n",
    "            df.rating[i] -= baseline[\"u\"][df.user[i]] + baseline[\"a\"][df.item[i]]\n",
    "        end\n",
    "    end\n",
    "    sparse(df.item, df.user, df.rating, num_items(medium), num_users(medium))\n",
    "end\n",
    "\n",
    "function get_labels(task)\n",
    "    [get_labels(task, content, medium) for content in [\"implicit\", \"explicit\"]]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeaf61e-3c3e-420f-a95e-d7ddb96ad53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_weights(task, content, medium)\n",
    "    df = cat(\n",
    "        get_split(\"validation\", task, content, medium),\n",
    "        get_split(\"test\", task, content, medium),\n",
    "    )\n",
    "    w = vcat(\n",
    "        powerdecay(\n",
    "            get_counts(\"validation\", task, content, medium),\n",
    "            weighting_scheme(\"inverse\"),\n",
    "        ),\n",
    "        powerdecay(get_counts(\"test\", task, content, medium), weighting_scheme(\"inverse\")),\n",
    "    )\n",
    "\n",
    "    sparse(df.item, df.user, w, num_items(medium), num_users(medium))\n",
    "end\n",
    "\n",
    "function get_weights(task)\n",
    "    [get_weights(task, content, medium) for content in [\"implicit\", \"explicit\"]]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2eeb2a-806a-4cbf-9c2e-8bbe5c17124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_users(task, content, medium)\n",
    "    training = collect(Set(get_split(\"validation\", task, content, medium).user))\n",
    "    test = collect(Set(get_split(\"test\", task, content, medium).user))\n",
    "    training, test\n",
    "end\n",
    "\n",
    "function get_users(task)\n",
    "    get_users(task, \"implicit\", medium)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed260a-b454-4690-ab8f-15ba4d686b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sentences(config, task)\n",
    "    explicit_baseline = Dict(\n",
    "        medium => read_params(\"$medium/$task/ExplicitUserItemBiases\") for\n",
    "        medium in [\"anime\", \"manga\"]\n",
    "    )\n",
    "    get_training_data(\n",
    "        task,\n",
    "        config[\"media\"],\n",
    "        config[\"include_ptw_impressions\"],\n",
    "        config[\"cls_tokens\"],\n",
    "        config[\"empty_tokens\"],\n",
    "        explicit_baseline = explicit_baseline,\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50183c75-f670-425e-b08e-c72efe4e8988",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ac800-38d2-4ed9-9963-05c176baef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_rngs(seed)\n",
    "    rng = Random.Xoshiro(seed)\n",
    "    Random.seed!(rand(rng, UInt64))\n",
    "    rng\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb67b6-dabb-4db8-841d-c7ebf43f4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_training_config(pretrain_name)\n",
    "    file = joinpath(get_data_path(\"alphas/$pretrain_name\"), \"config.json\")\n",
    "    open(file) do f\n",
    "        d = JSON.parse(f)\n",
    "        d[\"mode\"] = \"finetune\"\n",
    "        return d\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c35ee-317c-4c0a-bd28-9c0462f9d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_epoch_size!(config, training_users, validation_users)\n",
    "    num_tokens = length(training_users) * config[\"max_sequence_length\"]\n",
    "    @info \"Number of training tokens: $(num_tokens)\"\n",
    "    @info \"Number of training sentences: $(length(training_users))\"\n",
    "    @info \"Number of validation sentences: $(length(validation_users))\"\n",
    "    config[\"training_epoch_size\"] = length(training_users)\n",
    "    config[\"validation_epoch_size\"] = length(validation_users)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31c5b2-42e2-455a-bd3f-0ca4eab8cb8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function setup_training(config, outdir)\n",
    "    if !isdir(outdir)\n",
    "        mkdir(outdir)\n",
    "    end\n",
    "    for x in readdir(outdir, join = true)\n",
    "        if isfile(x)\n",
    "            rm(x)\n",
    "        end\n",
    "    end\n",
    "    fn = joinpath(outdir, \"..\", \"config.json\")\n",
    "    open(fn * \"~\", \"w\") do f\n",
    "        write(f, JSON.json(config))\n",
    "    end\n",
    "    mv(fn * \"~\", fn, force = true)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fa056-8deb-486d-9854-6e0991bc4714",
   "metadata": {},
   "source": [
    "# Disk I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7abb52-f79b-4e89-8611-aa1506d2808e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function save_features(sentences, labels, weights, users, config, training, filename)\n",
    "    features = []\n",
    "    for x in users\n",
    "        push!(features, featurize(sentences, labels, weights, medium, x, config, training))\n",
    "    end\n",
    "\n",
    "    d = Dict{String,AbstractArray}()\n",
    "    collate = Flux.batch\n",
    "    embed_names = [\n",
    "        \"anime\",\n",
    "        \"manga\",\n",
    "        \"rating\",\n",
    "        \"timestamp\",\n",
    "        \"status\",\n",
    "        \"completion\",\n",
    "        \"user\",\n",
    "        \"position\",\n",
    "    ]\n",
    "    for (i, name) in Iterators.enumerate(embed_names)\n",
    "        d[name] = collate([x[1][i] for x in features])\n",
    "    end\n",
    "    d[\"positions\"] = collate([x[2] for x in features])\n",
    "\n",
    "    for medium in [\"anime\", \"manga\"]\n",
    "        for task in [\"item\", \"rating\"]\n",
    "            record_sparse_array!(\n",
    "                d,\n",
    "                \"labels_$(medium)_$(task)\",\n",
    "                collate([x[3][medium][Symbol(task)] for x in features]),\n",
    "                extract(config[\"vocab_sizes\"], Symbol(medium)),\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "    for medium in [\"anime\", \"manga\"]\n",
    "        for task in [\"item\", \"rating\"]\n",
    "            record_sparse_array!(\n",
    "                d,\n",
    "                \"weights_$(medium)_$(task)\",\n",
    "                collate([x[4][medium][Symbol(task)] for x in features]),\n",
    "                extract(config[\"vocab_sizes\"], Symbol(medium)),\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "\n",
    "    HDF5.h5open(filename, \"w\") do file\n",
    "        for (k, v) in d\n",
    "            write(file, k, v)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function record_sparse_array!(d::Dict, name::String, x::AbstractSparseArray, vocab_size)\n",
    "    i, j, v = SparseArrays.findnz(x)\n",
    "    d[name*\"_i\"] = i\n",
    "    d[name*\"_j\"] = j\n",
    "    d[name*\"_v\"] = v\n",
    "    d[name*\"_size\"] = [vocab_size, size(x)[2]]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8cc95c-b656-49d2-a65f-1504fbe4b50e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function spawn_feature_workers(\n",
    "    sentences,\n",
    "    labels,\n",
    "    weights,\n",
    "    users,\n",
    "    config,\n",
    "    rng,\n",
    "    training,\n",
    "    outdir,\n",
    ")\n",
    "    # writes data to \"$outdir/data.$worker.h5\" in a hot loop\n",
    "    # whenever that file disappears, we populate it with a new batch\n",
    "    chunk_size = config[\"chunk_size\"]\n",
    "    workers = config[\"num_workers\"]\n",
    "    stem = training ? \"training\" : \"validation\"\n",
    "    rngs = [Random.Xoshiro(rand(rng, UInt64)) for _ = 1:workers]\n",
    "    @sync for (i, batch) in Iterators.enumerate(\n",
    "        Iterators.partition(users, div(length(users), workers, RoundUp)),\n",
    "    )\n",
    "        Threads.@spawn begin\n",
    "            rng = rngs[i]\n",
    "            while true\n",
    "                Random.shuffle!(rng, batch)\n",
    "                for (j, chunk) in\n",
    "                    Iterators.enumerate(Iterators.partition(batch, chunk_size))\n",
    "                    GC.gc()\n",
    "                    filename = joinpath(outdir, \"$stem.$i.h5\")\n",
    "                    while isfile(filename) && isdir(outdir)\n",
    "                        sleep(1)\n",
    "                    end\n",
    "                    if !isdir(outdir)\n",
    "                        break\n",
    "                    end\n",
    "                    save_features(\n",
    "                        sentences,\n",
    "                        labels,\n",
    "                        weights,\n",
    "                        chunk,\n",
    "                        config,\n",
    "                        training,\n",
    "                        filename,\n",
    "                    )\n",
    "                    open(\"$filename.complete\", \"w\") do f\n",
    "                        write(f, \"$j\")\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c19b7-612d-464b-a830-519ad2c4924a",
   "metadata": {},
   "source": [
    "# State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bd779-8b8f-4027-ba80-34d147d90a4d",
   "metadata": {
    "papermill": {
     "duration": 1.708366,
     "end_time": "2023-05-15T05:04:49.317062",
     "exception": false,
     "start_time": "2023-05-15T05:04:47.608696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_checkpoint = nothing\n",
    "config_epoch = nothing\n",
    "reset_lr_schedule = true\n",
    "rng = set_rngs(20221221)\n",
    "config = create_training_config(pretrain_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece189f1-11f9-4774-b519-635444144d93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-05-15T05:04:49.320685",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@info \"loading data\"\n",
    "sentences = get_sentences(config, task)\n",
    "labels = get_labels(task)\n",
    "weights = get_weights(task)\n",
    "training_users, test_users = get_users(task)\n",
    "set_epoch_size!(config, training_users, test_users);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb0a73-ced6-4e7c-abe3-bfadf15a08b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "outdir = get_data_path(joinpath(\"alphas\", name, \"training\"))\n",
    "config[\"num_workers\"] = 4\n",
    "setup_training(config, outdir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40387edf-e126-4c06-a0ad-48b749fc0d35",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Threads.@spawn spawn_feature_workers(\n",
    "    sentences,\n",
    "    labels,\n",
    "    weights,\n",
    "    training_users,\n",
    "    config,\n",
    "    rng,\n",
    "    true,\n",
    "    outdir,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa999b-7bc5-4f94-a562-cf0a773eaecc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Threads.@spawn spawn_feature_workers(\n",
    "    sentences,\n",
    "    labels,\n",
    "    weights,\n",
    "    test_users,\n",
    "    config,\n",
    "    rng,\n",
    "    false,\n",
    "    outdir,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9abf5e-ae0f-4e4c-967c-5058b90dcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(`python3 Pytorch.py --outdir $name --model_checkpoint $pretrain_name`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3043c-3109-4972-b8ce-5dee407cfe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm(outdir, recursive=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0-rc2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
