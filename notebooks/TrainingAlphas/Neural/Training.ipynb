{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2503872c-79e8-405c-acbb-518416c5c360",
   "metadata": {},
   "source": [
    "## Training\n",
    "* Trains a neural network with the given hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d61e7743-4bc9-4f75-a38d-b5e02ed1e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_epoch!(m, ps, opt; split=\"training\")\n",
    "    epoch = get_epoch(split)\n",
    "    sampling_order = get_sampling_order(split)\n",
    "    batchloss(x, y, z, w) = training_loss(m, x, y, z, w)\n",
    "    nbatches = Int(ceil(length(sampling_order) / G.batch_size))\n",
    "    for iter = 1:nbatches\n",
    "        batch, _ = get_batch(epoch, iter, G.batch_size, sampling_order, split == \"training\")\n",
    "        Flux.train!(batchloss, ps, batch, opt)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56658d70-3401-48de-b20c-c60435852fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function apply_zero_gradient!(m, ps, opt, apply)\n",
    "    # simulate training over other users by passing a zero gradient\n",
    "    if apply\n",
    "        zerobatches =\n",
    "            Int(ceil(num_users() / G.batch_size)) - Int(ceil(G.num_users / G.batch_size))\n",
    "        zerograd = gradient(ps) do\n",
    "            sum(0 * m(1))\n",
    "        end\n",
    "        for _ = 1:zerobatches\n",
    "            Flux.update!(opt, ps, zerograd)\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d31c53db-950b-438f-9318-ee357294f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains a model with the given hyperparams and returns its validation loss\n",
    "function train_model(\n",
    "    hyp;\n",
    "    max_checkpoints = 1000,\n",
    "    epochs_per_checkpoint = 1,\n",
    "    patience = 0,\n",
    "    verbose = false,\n",
    "    init_model = nothing,\n",
    "    fine_tune_layers = nothing,\n",
    ")\n",
    "    global G = hyp\n",
    "    opt = get_optimizer(G.optimizer, G.learning_rate, G.regularization_params)\n",
    "    Random.seed!(G.seed)\n",
    "    if isnothing(init_model)\n",
    "        m = build_model() |> device\n",
    "    else\n",
    "        m = init_model |> device\n",
    "    end\n",
    "    best_model = m |> cpu\n",
    "    if isnothing(fine_tune_layers)\n",
    "        ps = Flux.params(m)\n",
    "    else\n",
    "        ps = Flux.params(m[fine_tune_layers])\n",
    "    end\n",
    "    stopper = early_stopper(max_iters = max_checkpoints, patience = patience)\n",
    "\n",
    "    losses = []\n",
    "    loss = Inf\n",
    "    while (!stop!(stopper, loss))\n",
    "        for i = 1:epochs_per_checkpoint\n",
    "            train_epoch!(m, ps, opt)\n",
    "            apply_zero_gradient!(m, ps, opt, !isnothing(fine_tune_layers))\n",
    "        end\n",
    "        loss = split_loss(m, \"validation\")\n",
    "        push!(losses, loss)\n",
    "        if loss == minimum(losses)\n",
    "            best_model = m |> cpu\n",
    "        end\n",
    "        if verbose\n",
    "            training_loss = split_loss(m, \"training\")\n",
    "            @info \"losses: $training_loss $loss\"\n",
    "        end\n",
    "    end\n",
    "    global G = nothing\n",
    "    epochs = stopper.iters - stopper.iters_without_improvement\n",
    "    best_model, epochs, minimum(losses)\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
