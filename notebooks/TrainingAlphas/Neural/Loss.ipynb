{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4988ea0-8a00-41f1-857f-6064ef3806a7",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "* The `model_loss` is either the crossentropy loss or squared error, depending on the input data\n",
    "    * Note that we take the sum over all items, so using a bigger batchsize will have a bigger `model_loss`\n",
    "* The `regularization_loss` depends on the model architecture, but is commonly an L2 loss\n",
    "* The `split_loss` is either the weighted average crossentropy loss or weighted mean squared error, depending on the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c6be0c-9646-4495-9bc5-bfd7b87ecc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "function model_loss(m, x, y, z, w)\n",
    "    p = m(x)\n",
    "    β = m[end].β\n",
    "    if G.implicit\n",
    "        β = sigmoid.(β)\n",
    "        q = softmax(p) .* (1 .- β) + z .* β\n",
    "        ϵ = eps(Float64)\n",
    "        return sum(w .* -y .* log.(q .+ ϵ))\n",
    "    else\n",
    "        q = p + z .* β\n",
    "        return sum(w .* (q - y) .^ 2)\n",
    "    end\n",
    "end\n",
    "\n",
    "function training_loss(m, x, y, z, w)\n",
    "    model_loss(m, x, y, z, w) + regularization_loss(m, x)\n",
    "end\n",
    "\n",
    "function uncalibrated_split_loss(m, split)\n",
    "    epoch = get_epoch(split)\n",
    "    loss = 0.0\n",
    "    weights = 0.0\n",
    "    for iter = 1:Int(ceil(epoch_size(epoch) / G.batch_size))\n",
    "        batch, _ = get_batch(epoch, iter, G.batch_size, false)\n",
    "        loss += model_loss(m, batch[1]...)\n",
    "        weights += sum(batch[1][end])\n",
    "    end\n",
    "    Float32(loss / weights)\n",
    "end\n",
    "\n",
    "# reregresses the residualization before computing loss\n",
    "function split_loss(m, split; rng=Random.GLOBAL_RNG)\n",
    "    β = m[end].β\n",
    "    m = Chain(m[1:end-1]..., ScalarLayer(1) |> device, StorageLayer(1) |> device)\n",
    "    m[end].β .*= β\n",
    "    ps = Flux.params(m[end-1:end])\n",
    "    opt = ADAM(0.01)\n",
    "    train_epoch!(m, ps, opt; split = split, rng=rng)\n",
    "    uncalibrated_split_loss(m, split)\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
