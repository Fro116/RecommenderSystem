{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbdd719b-7f8b-491e-bafa-8aa56a9545e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10731233-3a58-4b55-aa5d-26424fe031b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dict that maps a user to the list of items they have watched\n",
    "function user_to_items(users, items)\n",
    "    utoa = Dict()\n",
    "    # making this multithreaded is slower\n",
    "    @showprogress for j = 1:length(users)\n",
    "        u = users[j]\n",
    "        a = items[j]\n",
    "        if u âˆ‰ keys(utoa)\n",
    "            utoa[u] = []\n",
    "        end\n",
    "        push!(utoa[u], a)\n",
    "    end\n",
    "    utoa\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8966e09b-8e58-490d-a08a-2bce141e8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a ratings dataset of predicted ratings\n",
    "function evaluate!(hyp, m, users, items, ratings)\n",
    "    # get model inputs\n",
    "    global G = hyp\n",
    "    m = m |> device\n",
    "    utoa = user_to_items(users, items)\n",
    "    epoch = [get_epoch_inputs(G.input_data, G.implicit, G.num_users, G.input_alphas)]\n",
    "    activation = G.implicit ? softmax : identity\n",
    "\n",
    "    # reuse the input buffers for space efficiency\n",
    "    out_users = users\n",
    "    out_items = items\n",
    "    out_ratings = ratings\n",
    "    out_ratings .= NaN32\n",
    "    out_idx = 1\n",
    "\n",
    "    # compute predictions    \n",
    "    @showprogress for iter = 1:Int(ceil(G.num_users / G.batch_size))\n",
    "        batch, sampled_users = get_batch(epoch, iter, G.batch_size, false)\n",
    "        alpha = activation(m(batch[1][1])) |> cpu\n",
    "        for j = 1:length(sampled_users)\n",
    "            u = sampled_users[j]\n",
    "            if u in keys(utoa)\n",
    "                item_mask = utoa[u]\n",
    "                next_idx = out_idx + length(item_mask)\n",
    "                out_users[out_idx:next_idx-1] .= u\n",
    "                out_items[out_idx:next_idx-1] = item_mask\n",
    "                out_ratings[out_idx:next_idx-1] = alpha[item_mask, j]\n",
    "                out_idx = next_idx\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    global G = nothing\n",
    "    RatingsDataset(user = out_users, item = out_items, rating = out_ratings)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46d999f4-f2b4-4d71-944b-754aead7de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "function write_alpha(hyp::Hyperparams, m, outdir)\n",
    "    splits = reduce(cat, [get_split(split, hyp.implicit) for split in all_raw_splits])\n",
    "    preds = evaluate!(hyp, m, splits.user, splits.item, splits.rating)\n",
    "    sparse_preds = sparse(preds.user, preds.item, preds.rating)\n",
    "    write_alpha(sparse_preds, hyp.residual_alphas, hyp.implicit, outdir)\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
