{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6cbfe4-1de9-4b3e-8828-0d4812e8df3c",
   "metadata": {},
   "source": [
    "## Retrain User Embeddings\n",
    "* To minimize training/serving skew, we train the model the same\n",
    "  way we will train it during inference\n",
    "* This means reinitializing the user embeddings, freezing all other layers,\n",
    "  and fine-tuning the user embeddings\n",
    "* During serving, we will determine a new user's embedding\n",
    "  by training with the same hyperparameters and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9f7fdca-da93-4280-99cc-786fa01874ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_retrain_model(hyp, m)\n",
    "    global G = hyp\n",
    "    Random.seed!(G.seed)\n",
    "    if G.model == \"user_item_biases\"\n",
    "        embedding_size = 1\n",
    "        initfn = (x...) -> zeros(Float32, x...)\n",
    "    elseif startswith(G.model, \"matrix_factorization\")\n",
    "        embedding_size = parse(Int, split(G.model, \"_\")[end])\n",
    "        initfn = Flux.glorot_uniform\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    m = m |> cpu\n",
    "    Chain(Flux.Embedding(G.num_users => embedding_size, init = initfn), m[2:end]...)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6199097-9ed3-4a4b-8911-9f56c95d5748",
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_retraining_hyperparams(hyp, m, max_iters)\n",
    "    function nlopt_loss(λ, grad)\n",
    "        @assert length(hyp.regularization_params) == 2\n",
    "        probe_hyp = hyp\n",
    "        probe_hyp = @set probe_hyp.learning_rate = 0.001 * 10^λ[1]\n",
    "        probe_hyp = @set probe_hyp.regularization_params = [1e-5 * 10^λ[2], 0]\n",
    "        probe_m = build_retrain_model(probe_hyp, m)\n",
    "        _, _, loss = train_model(\n",
    "            probe_hyp;\n",
    "            epochs_per_checkpoint = get_epochs_per_checkpoint(hyp.model),\n",
    "            init_model = probe_m,\n",
    "            fine_tune_layers = 1,\n",
    "            patience = 0,\n",
    "        )\n",
    "        @info \"$λ $loss\"\n",
    "        loss\n",
    "    end\n",
    "    nlopt_optimize(nlopt_loss, 2; max_evals = max_iters, max_time = 3600)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f0b9540-77a7-4b1b-b43c-8bb4c8799df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function retrain_user_embeddings(hyp, m, max_iters)\n",
    "    optimize_hyp =\n",
    "        @set hyp.num_users = Int(round(num_users() * get_subsampling_factor(hyp.model)))\n",
    "    λ = optimize_retraining_hyperparams(optimize_hyp, m, max_iters)\n",
    "    retrain_hyp = @set hyp.learning_rate = 0.001 * 10^λ[1]\n",
    "    retrain_hyp = @set retrain_hyp.regularization_params = [1e-5 * 10^λ[2], 0]\n",
    "    m = build_retrain_model(retrain_hyp, m)\n",
    "    retrained_model = train_model(\n",
    "        retrain_hyp,\n",
    "        init_model = m,\n",
    "        fine_tune_layers = 1,\n",
    "        epochs_per_checkpoint = 1,\n",
    "        patience = get_epochs_per_checkpoint(hyp.model),\n",
    "    )\n",
    "    tuple(retrained_model..., retrain_hyp)\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
