{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a52cbc-01d6-487c-8293-702ef156698a",
   "metadata": {},
   "source": [
    "## Batching\n",
    "* Turns an epoch into minibatches\n",
    "* Each user will appear in a minibatch with a probability proportional to its sampling weight\n",
    "* There is logic to predicting masked out items within a minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f26d8e-43bc-48d0-ad0f-ef3066e58b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import StatsBase: wsample, Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a1df7-d57d-4b77-8bbf-d47838e31fc2",
   "metadata": {},
   "source": [
    "### Sample users to put in a minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203dd198-8047-4470-8e80-4640739dd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sampling_order(epoch, split::String, rng)\n",
    "    scheme = split == \"training\" ? G.user_sampling_scheme : \"constant\"\n",
    "    task = split == \"training\" ? \"all\" : G.task\n",
    "    if scheme == \"constant\"\n",
    "        return collect(\n",
    "            Set(filter_users(get_split(split, task, G.content), G.num_users).user),\n",
    "        )\n",
    "    else\n",
    "        weights = powerdecay(\n",
    "            get_counts(split, task, G.content; per_rating = true),\n",
    "            weighting_scheme(scheme),\n",
    "        )\n",
    "        N = epoch_size(epoch)\n",
    "        samples = N\n",
    "        if should_temporal_batch(G.model) && split == \"training\"\n",
    "            weights = get_temporal_sampling_order(weights, true, split, task, content)\n",
    "            samples = min(samples, sum(weights .> 0))\n",
    "        end\n",
    "        return wsample(rng, 1:N, weights[1:N], samples)\n",
    "    end\n",
    "end;\n",
    "\n",
    "@memoize function get_sampling_timestamps(\n",
    "    split::String,\n",
    "    task::String,\n",
    "    content::String,\n",
    "    per_rating::Bool,\n",
    ")\n",
    "    if per_rating\n",
    "        timestamps = get_split(split, task, content, fields = [:timestamp]).timestamp\n",
    "    else\n",
    "        df = get_split(split, task, content, fields = [:user, :timestamp])\n",
    "        timestamps = zeros(num_users(), Threads.nthreads())\n",
    "        @tprogress Threads.@threads for i = 1:length(df.user)\n",
    "            if df.timestamp[i] > timestamps[df.user[i], Threads.threadid()]\n",
    "                timestamps[df.user[i], Threads.threadid()] = df.timestamp[i]\n",
    "            end\n",
    "        end\n",
    "        timestamps = vec(maximum(timestamps; dims = 2))\n",
    "    end\n",
    "    timestamps\n",
    "end\n",
    "\n",
    "function get_temporal_sampling_order(\n",
    "    weights,\n",
    "    per_rating::Bool,\n",
    "    split::String,\n",
    "    task::String,\n",
    "    content::String,\n",
    ")\n",
    "    # zero out weights for users that have no items past the temporal holdout\n",
    "    timestamps = get_sampling_timestamps(split, task, content, per_rating)\n",
    "    weights .* (timestamps .>= G.temporal_holdout)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29632c-edd2-436c-a103-54182bcb21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice(x::Nothing, range) = nothing\n",
    "slice(x::AbstractVector, range) = x[range]\n",
    "slice(x::AbstractMatrix, range) = x[:, range]\n",
    "slice(x::Tuple, range) = slice.(x, (range,));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288d708-5e5a-48a2-9426-57f98713580c",
   "metadata": {},
   "source": [
    "### Mask out items within a minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7178e2f-0f72-4359-af2d-d8933060683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform emphasized denoising and data augmentation on each minibatch\n",
    "\n",
    "function holdout(x, mask)\n",
    "    x .* repeat(mask, size(x)[1] รท size(mask)[1])\n",
    "end\n",
    "\n",
    "function holdout(x::Tuple, mask)\n",
    "    holdout.(x, (mask,))\n",
    "end\n",
    "\n",
    "function holdout_allitems(batch, holdout_perc::Real, temporal_perc::Real, training::Bool, rng)\n",
    "    if !training\n",
    "        return batch\n",
    "    end\n",
    "    randfn = CUDA.functional() ? CUDA.rand : x -> rand(rng, x)\n",
    "    batch_size = size(batch[4])[2]\n",
    "\n",
    "    if isnan(temporal_perc)\n",
    "        # randomly drop holdout_perc percent of items from a user's list\n",
    "        entries_to_keep = randfn(num_items(), batch_size) .>= holdout_perc\n",
    "        entries_to_predict = 1 .- entries_to_keep\n",
    "    else\n",
    "        # use the first temporal_perc percent of items to predict the remaining 1-temporal_perc entries\n",
    "        temporal_entries_to_keep = batch[5] .<= temporal_perc\n",
    "\n",
    "        # randomly drop holdout_perc percent of items from a user's list\n",
    "        holdout_entries_to_keep = randfn(num_items(), batch_size) .>= holdout_perc\n",
    "\n",
    "        entries_to_keep = holdout_entries_to_keep .* temporal_entries_to_keep\n",
    "        entries_to_predict = 1 .- temporal_entries_to_keep\n",
    "    end\n",
    "\n",
    "    holdout(batch[1], entries_to_keep),\n",
    "    batch[2],\n",
    "    batch[3],\n",
    "    holdout(batch[4], entries_to_predict)\n",
    "end\n",
    "\n",
    "function postprocess_batch(batch, training::Bool, rng)\n",
    "    if !should_holdout_items(G.model)\n",
    "        return batch\n",
    "    end\n",
    "    holdout_allitems(batch, G.holdout, G.temporal_holdout, training, rng)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c453802-5b57-4acc-b76b-246ea459cfc2",
   "metadata": {},
   "source": [
    "### Construct a minibatch from an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f35efa-decd-42ed-bcae-f7ea7cab6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs the following steps\n",
    "# 1) shuffle the epoch by the sampling order\n",
    "# 2) split the epoch into minibatches of size batch_size\n",
    "# 3) return the iter-th minibatch\n",
    "function get_batch(\n",
    "    epoch,\n",
    "    iter::Int,\n",
    "    batch_size::Int,\n",
    "    sampling_order,\n",
    "    training::Bool,\n",
    "    rng = Random.GLOBAL_RNG,\n",
    ")\n",
    "    range =\n",
    "        sampling_order[(iter-1)*batch_size+1:min(iter * batch_size, length(sampling_order))]\n",
    "    process(x) = slice(x, range) |> device\n",
    "    batch = postprocess_batch(process.(epoch), training, rng)\n",
    "    (batch[1], batch[2], batch[3], batch[4]), range\n",
    "end;\n",
    "\n",
    "function get_batch(\n",
    "    epoch,\n",
    "    iter::Int,\n",
    "    batch_size::Int,\n",
    "    training::Bool,\n",
    "    rng = Random.GLOBAL_RNG,\n",
    ")\n",
    "    get_batch(epoch, iter, batch_size, 1:epoch_size(epoch), training, rng)\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
