{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a52cbc-01d6-487c-8293-702ef156698a",
   "metadata": {},
   "source": [
    "## Batching\n",
    "* Turns an epoch into minibatches\n",
    "* Each user will appear in a minibatch with a probability proportional to its sampling weight\n",
    "* There is logic to predicting masked out items within a minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f26d8e-43bc-48d0-ad0f-ef3066e58b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import StatsBase: wsample, Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a1df7-d57d-4b77-8bbf-d47838e31fc2",
   "metadata": {},
   "source": [
    "### Sample users to put in a minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203dd198-8047-4470-8e80-4640739dd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sampling_order(epoch, split::String, rng)\n",
    "    N = epoch_size(epoch)\n",
    "    samples = N\n",
    "    scheme = split == \"training\" ? G.user_sampling_scheme : \"constant\"\n",
    "    if scheme == \"constant\"\n",
    "        return shuffle(rng, 1:N)\n",
    "    else\n",
    "        if G.output_data == \"allitems\"\n",
    "            per_rating = false\n",
    "        elseif G.output_data == \"item\"\n",
    "            per_rating = true\n",
    "        else\n",
    "            @assert false\n",
    "        end\n",
    "        weights = powerdecay(\n",
    "            get_counts(split, G.content; per_rating = per_rating),\n",
    "            weighting_scheme(scheme),\n",
    "        )\n",
    "        if should_temporal_batch(G.model) && split == \"training\"\n",
    "            weights = get_temporal_sampling_order(weights, per_rating, split, content)\n",
    "            samples = min(samples, sum(weights .> 0))\n",
    "        end\n",
    "        return wsample(rng, 1:N, weights[1:N], samples)\n",
    "    end\n",
    "end;\n",
    "\n",
    "@memoize function get_sampling_timestamps(split, content, per_rating)\n",
    "    if per_rating\n",
    "        timestamps = get_split(split, content, fields = [:timestamp]).timestamp\n",
    "    else\n",
    "        df = get_split(split, content, fields = [:user, :timestamp])\n",
    "        timestamps = zeros(num_users(), Threads.nthreads())\n",
    "        @tprogress Threads.@threads for i = 1:length(df.user)\n",
    "            if df.timestamp[i] > timestamps[df.user[i], Threads.threadid()]\n",
    "                timestamps[df.user[i], Threads.threadid()] = df.timestamp[i]\n",
    "            end\n",
    "        end\n",
    "        timestamps = vec(maximum(timestamps; dims = 2))\n",
    "    end\n",
    "    timestamps\n",
    "end\n",
    "\n",
    "function get_temporal_sampling_order(weights, per_rating, split, content)\n",
    "    # zero out weights for users that have no items past the temporal holdout\n",
    "    timestamps = get_sampling_timestamps(split, content, per_rating)\n",
    "    weights .* (timestamps .>= G.temporal_holdout)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29632c-edd2-436c-a103-54182bcb21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function slice(x::Nothing, range)\n",
    "    nothing\n",
    "end\n",
    "\n",
    "function slice(x::AbstractVector, range)\n",
    "    x[range]\n",
    "end\n",
    "\n",
    "function slice(x::AbstractMatrix, range)\n",
    "    x[:, range]\n",
    "end\n",
    "\n",
    "function slice(x::Tuple, range)\n",
    "    if G.output_data == \"allitems\"\n",
    "        return slice.(x, (range,))\n",
    "    elseif G.output_data == \"item\"\n",
    "        # x is encoded as (user idx, user timestamps, user embeddings, item idx, item embeddings)\n",
    "        @assert length(x) == 5\n",
    "        uidxs = slice(x[1], range)\n",
    "        T = slice.(x[2], (uidxs,))\n",
    "        U = slice.(x[3], (uidxs,))\n",
    "        aidxs = slice(x[4], range)\n",
    "        A = slice.(x[5], (aidxs,))\n",
    "        return T, U, A\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288d708-5e5a-48a2-9426-57f98713580c",
   "metadata": {},
   "source": [
    "### Mask out items within a minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7178e2f-0f72-4359-af2d-d8933060683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform emphasized denoising and data augmentation on each minibatch\n",
    "\n",
    "function holdout(x, mask)\n",
    "    x .* repeat(mask, size(x)[1] รท size(mask)[1])\n",
    "end\n",
    "\n",
    "function holdout(x::Tuple, mask)\n",
    "    holdout.(x, (mask,))\n",
    "end\n",
    "\n",
    "function holdout_allitems(batch, holdout_perc::Real, temporal_perc::Real, training::Bool)\n",
    "    if !training\n",
    "        return batch\n",
    "    end\n",
    "    randfn = CUDA.functional() ? CUDA.rand : rand\n",
    "    batch_size = size(batch[4])[2]\n",
    "\n",
    "    if isnan(temporal_perc)\n",
    "        # randomly drop holdout_perc percent of items from a user's list\n",
    "        entries_to_keep = randfn(num_items(), batch_size) .>= holdout_perc\n",
    "        entries_to_predict = 1 .- entries_to_keep\n",
    "    else\n",
    "        # use the first temporal_perc percent of items to predict the remaining 1-temporal_perc entries\n",
    "        temporal_entries_to_keep = batch[5] .<= temporal_perc\n",
    "\n",
    "        # randomly drop holdout_perc percent of items from a user's list\n",
    "        holdout_entries_to_keep = randfn(num_items(), batch_size) .>= holdout_perc\n",
    "\n",
    "        entries_to_keep = holdout_entries_to_keep .* temporal_entries_to_keep\n",
    "        entries_to_predict = 1 .- temporal_entries_to_keep\n",
    "    end\n",
    "\n",
    "    holdout(batch[1], entries_to_keep),\n",
    "    batch[2],\n",
    "    batch[3],\n",
    "    holdout(batch[4], entries_to_predict)\n",
    "end\n",
    "\n",
    "function holdout_item(batch, holdout_perc::Real, temporal_perc::Real, training::Bool)\n",
    "    #  remove the item we're trying to predict\n",
    "    T, U, A = batch[1]\n",
    "    batch_size = size(T)[2]\n",
    "    U = holdout.(U, (1 .- A[1],))\n",
    "    # perform holdout on the rest\n",
    "    if training\n",
    "        randfn = CUDA.functional() ? CUDA.rand : rand\n",
    "        if isnan(temporal_perc)\n",
    "            holdout_entries_to_keep = randfn(num_items(), batch_size) .>= holdout_perc\n",
    "            entries_to_keep = holdout_entries_to_keep            \n",
    "        else\n",
    "            temporal_entries_to_keep = T .<= temporal_perc\n",
    "            holdout_entries_to_keep = randfn(num_items(), batch_size) .>= holdout_perc\n",
    "            entries_to_keep = temporal_entries_to_keep * holdout_entries_to_keep\n",
    "        end\n",
    "        U = holdout.(U, (1 .- entries_to_keep,))\n",
    "    end\n",
    "    (U, A), batch[2], batch[3], batch[4]\n",
    "end\n",
    "\n",
    "function postprocess_batch(batch, training::Bool, rng)\n",
    "    if !should_holdout_items(G.model)\n",
    "        return batch\n",
    "    end\n",
    "    if G.output_data == \"allitems\"\n",
    "        holdout_fn = holdout_allitems\n",
    "    elseif G.output_data == \"item\"\n",
    "        holdout_fn = holdout_item        \n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    holdout_fn(batch, G.holdout, G.temporal_holdout, training)    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c453802-5b57-4acc-b76b-246ea459cfc2",
   "metadata": {},
   "source": [
    "### Construct a minibatch from an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f35efa-decd-42ed-bcae-f7ea7cab6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs the following steps\n",
    "# 1) shuffle the epoch by the sampling order\n",
    "# 2) split the epoch into minibatches of size batch_size\n",
    "# 3) return the iter-th minibatch\n",
    "function get_batch(\n",
    "    epoch,\n",
    "    iter::Int,\n",
    "    batch_size::Int,\n",
    "    sampling_order,\n",
    "    training::Bool,\n",
    "    rng = Random.GLOBAL_RNG,\n",
    ")\n",
    "    range = sampling_order[(iter-1)*batch_size+1:min(iter * batch_size, length(sampling_order))]\n",
    "    process(x) = slice(x, range) |> device\n",
    "    batch = postprocess_batch(process.(epoch), training, rng)\n",
    "    [(batch[1], batch[2], batch[3], batch[4])], range\n",
    "end;\n",
    "\n",
    "function get_batch(\n",
    "    epoch,\n",
    "    iter::Int,\n",
    "    batch_size::Int,\n",
    "    training::Bool,\n",
    "    rng = Random.GLOBAL_RNG,\n",
    ")\n",
    "    get_batch(epoch, iter, batch_size, 1:epoch_size(epoch), training, rng)\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
