{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4988ea0-8a00-41f1-857f-6064ef3806a7",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "* The `model_loss` is either the crossentropy loss or squared error, depending on the input data\n",
    "    * Note that we take the sum over all items, so using a bigger batchsize will have a bigger `model_loss`\n",
    "* The `uncalibrated_split_loss` is the average model loss over the entire training/validation/test split\n",
    "* The `split_loss` is the same as the uncalibrated split loss, except we first train a linear regression \n",
    "  from the model's output to the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6be0c-9646-4495-9bc5-bfd7b87ecc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "function model_loss(m, x, y, z, w)\n",
    "    p = m(x)\n",
    "    β = m[end].β\n",
    "    if G.implicit\n",
    "        β = sigmoid.(β)\n",
    "        q = softmax(p) .* (1 .- β) + z .* β\n",
    "        ϵ = Float32(eps(Float64))\n",
    "        return sum(w .* -y .* log.(q .+ ϵ))\n",
    "    else\n",
    "        q = p + z .* β\n",
    "        return sum(w .* (q - y) .^ 2)\n",
    "    end\n",
    "end\n",
    "\n",
    "function uncalibrated_split_loss(m, split::String)\n",
    "    epoch = get_epoch(split)\n",
    "    loss = 0.0\n",
    "    weights = 0.0\n",
    "    for iter = 1:Int(ceil(epoch_size(epoch) / G.batch_size))\n",
    "        batch, _ = get_batch(epoch, iter, G.batch_size, false)\n",
    "        loss += model_loss(m, batch...)\n",
    "        weights += sum(batch[end])\n",
    "        device_free!(batch)\n",
    "    end\n",
    "    Float32(loss / weights)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbfac73-6777-4208-876c-193c2822d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reregress the residualization before computing loss\n",
    "\n",
    "function train_calibration_layers(m, split::String; rng=Random.GLOBAL_RNG)\n",
    "    β = m[end].β\n",
    "    m = Chain(m[1:end-1]..., ScalarLayer(1) |> device, StorageLayer(1) |> device)\n",
    "    m[end].β .*= β\n",
    "    ps = Flux.params(m[end-1:end])\n",
    "    opt = ADAM(0.01)\n",
    "    train_epoch!(m, ps, opt; split = split, rng=rng)\n",
    "    m\n",
    "end\n",
    "\n",
    "function train_calibration_layers(m, split::String, hyp::Hyperparams; rng=Random.GLOBAL_RNG)\n",
    "    m = m |> device    \n",
    "    global G = hyp\n",
    "    m = train_calibration_layers(m, split; rng=rng)\n",
    "    global G = nothing\n",
    "    m |> cpu\n",
    "end\n",
    "\n",
    "function split_loss(m, split::String; rng=Random.GLOBAL_RNG)\n",
    "    m = train_calibration_layers(m, split; rng=rng)\n",
    "    uncalibrated_split_loss(m, split)\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
