{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2503872c-79e8-405c-acbb-518416c5c360",
   "metadata": {},
   "source": [
    "## Training\n",
    "* Trains a neural network with the given hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e7743-4bc9-4f75-a38d-b5e02ed1e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_epoch!(m, ps, opt; split::String=\"training\", rng=Random.GLOBAL_RNG)\n",
    "    epoch = get_epoch(split)\n",
    "    sampling_order = get_sampling_order(epoch, split, rng)\n",
    "    batchloss(x, y, z, w) = model_loss(m, x, y, z, w)\n",
    "    nbatches = Int(ceil(length(sampling_order) / G.batch_size))\n",
    "    for iter = 1:nbatches\n",
    "        batch, _ = get_batch(epoch, iter, G.batch_size, sampling_order, split == \"training\")\n",
    "        Flux.train!(batchloss, ps, [batch], opt)\n",
    "        device_free!(batch)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ad182-17ca-4bba-ad27-03bd7f76da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains a model with the given hyperparams and returns its validation loss\n",
    "function train_model(\n",
    "    hyp::Hyperparams;\n",
    "    max_checkpoints::Int = 1000,\n",
    "    epochs_per_checkpoint::Int = 1,\n",
    "    patience::Int = 0,\n",
    "    verbose::String = \"\",\n",
    "    init_model = nothing,\n",
    ")\n",
    "    global G = hyp\n",
    "    opt = get_optimizer(G.optimizer, G.learning_rate, G.optimizer_weight_decay)\n",
    "    rng = Random.Xoshiro(G.seed)\n",
    "    Random.seed!(rand(rng, UInt64))\n",
    "    if CUDA.functional()\n",
    "        Random.seed!(CUDA.default_rng(), rand(rng, UInt64))\n",
    "        Random.seed!(CUDA.CURAND.default_rng(), rand(rng, UInt64))\n",
    "    end\n",
    "    \n",
    "    if isnothing(init_model)\n",
    "        m = build_model(rng=rng) |> device\n",
    "    else\n",
    "        m = init_model |> device\n",
    "    end\n",
    "    best_model = m |> cpu\n",
    "    ps = Flux.params(m)\n",
    "    stopper = early_stopper(max_iters = max_checkpoints, patience = patience, min_rel_improvement = 1e-3)\n",
    "    \n",
    "    losses = []\n",
    "    loss = Inf\n",
    "    while (!stop!(stopper, loss))\n",
    "        for i = 1:epochs_per_checkpoint\n",
    "            train_epoch!(m, ps, opt; rng=rng)\n",
    "        end\n",
    "        loss = split_loss(m, \"validation\"; rng=rng)\n",
    "        push!(losses, loss)\n",
    "        if loss == minimum(losses)\n",
    "            best_model = m |> cpu\n",
    "        end\n",
    "        if verbose == \"info\"\n",
    "            @info \"losses: $loss\"\n",
    "        elseif verbose == \"debug\"\n",
    "            training_loss = uncalibrated_split_loss(m, \"training\")\n",
    "            validation_loss = uncalibrated_split_loss(m, \"validation\")            \n",
    "            @info \"losses: $training_loss $validation_loss $loss\"\n",
    "        end\n",
    "    end\n",
    "    global G = nothing\n",
    "    epochs = stopper.iters - stopper.iters_without_improvement\n",
    "    best_model, epochs, minimum(losses)\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
