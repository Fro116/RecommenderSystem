{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2503872c-79e8-405c-acbb-518416c5c360",
   "metadata": {},
   "source": [
    "## Training\n",
    "* Trains a neural network with the given hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e7743-4bc9-4f75-a38d-b5e02ed1e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_epoch!(m, opt; split::String = \"training\", rng = Random.GLOBAL_RNG)\n",
    "    epoch = get_epoch(split)\n",
    "    sampling_order = get_sampling_order(epoch, split, rng)\n",
    "    nbatches = Int(ceil(length(sampling_order) / G.batch_size))\n",
    "    losses = []\n",
    "    @showprogress for iter = 1:nbatches\n",
    "        batch, _ = get_batch(epoch, iter, G.batch_size, sampling_order, split == \"training\")\n",
    "        loss, grads = Flux.withgradient(m) do model\n",
    "            model_loss(model, batch...)\n",
    "        end\n",
    "        device_free!(batch)        \n",
    "        Flux.update!(opt, m, grads[1])\n",
    "        push!(losses, loss)        \n",
    "    end\n",
    "    mean(losses)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ad182-17ca-4bba-ad27-03bd7f76da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains a model with the given hyperparams and returns its validation loss\n",
    "function train_model(\n",
    "    hyp::Hyperparams;\n",
    "    max_checkpoints::Int = 1000,\n",
    "    epochs_per_checkpoint::Int = 1,\n",
    "    patience::Int = 0,\n",
    "    verbose::String = \"\",\n",
    "    init_model = nothing,\n",
    ")\n",
    "    global G = hyp\n",
    "    opt_spec = get_optimizer(G.optimizer, G.learning_rate, G.optimizer_weight_decay)\n",
    "    rng = Random.Xoshiro(G.seed)\n",
    "    Random.seed!(rand(rng, UInt64))\n",
    "    if CUDA.functional()\n",
    "        Random.seed!(CUDA.default_rng(), rand(rng, UInt64))\n",
    "        Random.seed!(CUDA.CURAND.default_rng(), rand(rng, UInt64))\n",
    "    end\n",
    "\n",
    "    if isnothing(init_model)\n",
    "        m = build_model(rng = rng) |> device\n",
    "    else\n",
    "        m = init_model |> device\n",
    "    end\n",
    "    best_model = m |> cpu\n",
    "    stopper = early_stopper(\n",
    "        max_iters = max_checkpoints,\n",
    "        patience = patience,\n",
    "        min_rel_improvement = 1e-3,\n",
    "    )\n",
    "    opt = Optimisers.setup(opt_spec, m)\n",
    "\n",
    "    losses = []\n",
    "    loss = Inf\n",
    "    training_loss = Inf\n",
    "    while (!stop!(stopper, loss))\n",
    "        for i = 1:epochs_per_checkpoint\n",
    "            tloss = train_epoch!(m, opt; rng = rng)\n",
    "        end\n",
    "        loss = split_loss(m, \"validation\"; rng = rng)\n",
    "        push!(losses, loss)\n",
    "        if loss == minimum(losses)\n",
    "            best_model = m |> cpu\n",
    "            training_loss = tloss\n",
    "        end\n",
    "        if verbose == \"info\"\n",
    "            @info \"losses: $tloss $loss\"\n",
    "        end\n",
    "    end\n",
    "    global G = nothing\n",
    "    epochs = stopper.iters - stopper.iters_without_improvement\n",
    "    best_model, epochs, training_loss, minimum(losses)\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
