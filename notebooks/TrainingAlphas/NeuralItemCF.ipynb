{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ef15a8-4209-4fdc-afb6-ebcbd5e79a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NBInclude: @nbinclude\n",
    "@nbinclude(\"NeuralNetworkBase.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e0e2f-1993-4947-8cba-099ed285e953",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220621 00:46:03 Optimizing hyperparameters...\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:11:57\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:40\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:40\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220621 01:16:36 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] 1.5331082\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:51\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:40\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:40\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:40\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:40\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:39\u001b[39m\n",
      "\u001b[32mProgress:  33%|█████████████▌                           |  ETA: 0:04:25\u001b[39mIOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:34\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:35\u001b[39m\n",
      "\u001b[32mProgress:  67%|███████████████████████████▍             |  ETA: 0:02:12\u001b[39mIOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:35\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:34\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:06:34\u001b[39m\n",
      "\u001b[32mProgress:  81%|█████████████████████████████████▎       |  ETA: 0:01:14\u001b[39m"
     ]
    }
   ],
   "source": [
    "train_alpha(\n",
    "  create_hyperparams(\"item_based_collaborative_filtering\", false, [\"NeuralExplicitUserItemBiases\"]),\n",
    "    \"NeuralExplicitItemCF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a2c4c-f12e-498f-8fcf-d543dcd33793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyp = create_hyperparams(\n",
    "#     \"item_based_collaborative_filtering\",\n",
    "#     false,\n",
    "#     [\"NeuralExplicitUserItemBiases\"],\n",
    "# )\n",
    "# hyp = @set hyp.num_users = Int(round(num_users()))\n",
    "# hyp = @set hyp.regularization_params = [0, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4d138-c45a-4dbf-9c64-9515c532c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m, _, _ = train_model(\n",
    "#     hyp;\n",
    "#     epochs_per_checkpoint = 1,\n",
    "#     patience = 10,\n",
    "#     verbose = true,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c922b41-3b14-4426-86e0-50dae8a0f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5363433 is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098503c-8576-45aa-95b2-ff71cc9884c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_alpha(\n",
    "#   create_hyperparams(\"item_based_collaborative_filtering\", false, [\"NeuralExplicitUserItemBiases\"]),\n",
    "#     \"NeuralExplicitItemCF\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37200f4-8f83-4084-901c-44d297c60bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
