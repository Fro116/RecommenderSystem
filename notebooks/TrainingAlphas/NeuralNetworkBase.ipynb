{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3758ece-9b72-47b4-ae6a-aa364e0443e3",
   "metadata": {},
   "source": [
    "# Neural Network Base Class\n",
    "* This class contains infrastructure to train neural networks\n",
    "* The following algorithms are implemented:\n",
    "    * Baseline predictors\n",
    "* The following algoirthms will be implemented\n",
    "    * Item-based collaborative filtering\n",
    "    * Matrix Factorization\n",
    "    * Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d17593ac-3260-4c01-a5df-91edfcd17e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Random\n",
    "using SparseArrays\n",
    "using Statistics: var\n",
    "\n",
    "import CUDA\n",
    "import NBInclude: @nbinclude\n",
    "import NLopt\n",
    "import Setfield: @set\n",
    "@nbinclude(\"Alpha.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08801fd4-7057-49f0-9fa7-9854d17494cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function device(x)\n",
    "    gpu(x)\n",
    "end\n",
    "\n",
    "# efficiently convert a sparse cpu matrix into a dense CUDA array\n",
    "function device(x::AbstractSparseArray)\n",
    "    CUDA.functional() ? CUDA.CuArray(gpu(x)) : x\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11693af-0136-4b3e-a1bb-2806a172b5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030f566-08b7-46ad-9b69-182e559a9f89",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "* Contains all the information necessary to train a new model\n",
    "* The important hyperparameters will tuned via a derivative-free optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abba16e9-f406-49c4-8af1-6e0afde838c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_kw struct Hyperparams\n",
    "    # model\n",
    "    implicit::Bool\n",
    "    input_data::String\n",
    "    model::String\n",
    "    # batching\n",
    "    batch_size::Int\n",
    "    user_sampling_scheme::String # TODO convert to float32\n",
    "    # optimizer\n",
    "    learning_rate::Float32\n",
    "    optimizer::String\n",
    "    # training\n",
    "    seed::UInt64\n",
    "    num_users::Int\n",
    "    # loss\n",
    "    item_weight_decay::Float32\n",
    "    regularization_params::Vector{Float32}\n",
    "    residual_alphas::Vector{String}\n",
    "    residual_beta::Float32\n",
    "    user_weight_decay::Float32\n",
    "end\n",
    "\n",
    "function to_dict(x::Hyperparams)\n",
    "    Dict(string(key) => getfield(x, key) for key ∈ fieldnames(Hyperparams))\n",
    "end\n",
    "\n",
    "function Base.string(x::Hyperparams)\n",
    "    fields = [x for x in fieldnames(Hyperparams)]\n",
    "    max_field_size = maximum(length(string(k)) for k in fields)\n",
    "    ret = \"Hyperparameters:\\n\"\n",
    "    for f in fields\n",
    "        ret *= \"$(rpad(string(f), max_field_size)) => $(getfield(x, f))\\n\"\n",
    "    end\n",
    "    ret\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3fa63-a407-4e8d-86d2-b54b50e6d6ac",
   "metadata": {},
   "source": [
    "## Models\n",
    "* To define a new model, add the architecture to `build_model` and the regularization to `regularization_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd9bcd93-0801-4944-9e44-b2b25cc16934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that takes one input and splits it into many\n",
    "struct Split{T}\n",
    "    paths::T\n",
    "end\n",
    "Split(paths...) = Split(paths)\n",
    "Flux.@functor Split\n",
    "(m::Split)(x::AbstractArray) = map(f -> f(x), m.paths)\n",
    "\n",
    "# A layer that takes many inputs and joins them into one\n",
    "Join(combine, paths) = Parallel(combine, paths)\n",
    "Join(combine, paths...) = Join(combine, paths);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "321ca69a-0b42-4859-8594-af05d93e52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that adds a 1-D vector to the input\n",
    "struct BiasLayer\n",
    "    b::Any\n",
    "end\n",
    "BiasLayer(n::Integer; init = randn) = BiasLayer(init(Float32, n))\n",
    "(m::BiasLayer)(x) = x .+ m.b\n",
    "Flux.@functor BiasLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf80688-1e79-420a-9710-ae82a5b8184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements a baseline predictor given by R[i, j] = u[i] + a[j]\n",
    "function user_item_biases()\n",
    "    U = Flux.Embedding(G.num_users => 1)\n",
    "    A = BiasLayer(num_items())\n",
    "    m = Chain(U, A)\n",
    "end\n",
    "\n",
    "# regularization is λ_u variance(u) + λ_a variance(a)\n",
    "function user_item_biases_regularization(m)\n",
    "    var(m[1].weight) * G.regularization_params[1] + var(m[2].b) * G.regularization_params[2]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8db136-b854-4cf5-8e8e-8dd6fbc28081",
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_model()\n",
    "    if G.model == \"user_item_biases\"\n",
    "        return user_item_biases()\n",
    "    end\n",
    "    @assert false\n",
    "end\n",
    "\n",
    "function regularization_loss(m)\n",
    "    if G.model == \"user_item_biases\"\n",
    "        return user_item_biases_regularization(m)\n",
    "    end\n",
    "    @assert false\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b1aac-67d5-4dec-be8b-1e6c65b4d252",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "* An epoch is an efficient representation of all the models inputs, outputs, residualization, and weights\n",
    "* We generate one epoch per split and memoize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1c39d34-3212-4131-8e1d-68021044fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function one_hot_inputs(implicit, num_users)\n",
    "    collect(1:num_users)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c0cd51a-8bfe-498b-9185-c0ecaeb1a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_inputs(\n",
    "    input_data,\n",
    "    implicit,\n",
    "    num_users,\n",
    ")\n",
    "    if input_data == \"one_hot\"\n",
    "        X, Y = one_hot_inputs(implicit, num_users)\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "end\n",
    "\n",
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_outputs(split, implicit, num_users)\n",
    "    sparse(filter_users(get_split(split, implicit), num_users))\n",
    "end\n",
    "\n",
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_residuals(\n",
    "    split,\n",
    "    residual_alphas,\n",
    "    implicit,\n",
    "    num_users,\n",
    ")\n",
    "    residuals = filter_users(read_alpha(residual_alphas, split, implicit), num_users)\n",
    "    sparse(residuals)\n",
    "end\n",
    "\n",
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_weights(\n",
    "    split,\n",
    "    user_weight_decay,\n",
    "    item_weight_decay,\n",
    "    implicit,\n",
    "    num_users,\n",
    ")\n",
    "    if split == \"training\"\n",
    "        weights =\n",
    "            expdecay(get_counts(split, implicit), user_weight_decay) .*\n",
    "            expdecay(get_counts(split, implicit; by_item = true), item_weight_decay)\n",
    "    else\n",
    "        weights = expdecay(get_counts(split, implicit), weighting_scheme(\"inverse\"))\n",
    "    end\n",
    "\n",
    "    df = get_split(split, implicit)\n",
    "    df = filter_users(RatingsDataset(df.user, df.item, weights), num_users)\n",
    "    sparse(df)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "272ebd7c-a236-4a22-b0af-17759579f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns (X, Y, Z, W) = (inputs, outputs, residualization alpha, weights)\n",
    "function get_epoch(split)\n",
    "    X = get_epoch_inputs(G.input_data, G.implicit, G.num_users)\n",
    "    Y = get_epoch_outputs(split, G.implicit, G.num_users)\n",
    "    Z = get_epoch_residuals(split, G.residual_alphas, G.implicit, G.num_users)\n",
    "    W = get_epoch_weights(\n",
    "        split,\n",
    "        G.user_weight_decay,\n",
    "        G.item_weight_decay,\n",
    "        G.implicit,\n",
    "        G.num_users,\n",
    "    )\n",
    "    X, Y, Z, W\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a52cbc-01d6-487c-8293-702ef156698a",
   "metadata": {},
   "source": [
    "# Batching\n",
    "* Turns an epoch into minibatches\n",
    "* Each data point will appear in a minibatch with a probability proportional to its sampling weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2f87d3-1942-4abc-88ca-6b83433443c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "function SparseArrays.sparse(split::RatingsDataset)\n",
    "    sparse(split.item, split.user, split.rating, num_items(), G.num_users)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea29632c-edd2-436c-a103-54182bcb21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function slice(x::AbstractVector, range)\n",
    "    x[range]\n",
    "end\n",
    "\n",
    "function slice(x::AbstractMatrix, range)\n",
    "    x[:, range]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203dd198-8047-4470-8e80-4640739dd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sampling_order(split)\n",
    "    weighting_scheme = split == \"training\" ? G.user_sampling_scheme : \"constant\"\n",
    "    if weighting_scheme == \"constant\"\n",
    "        return shuffle(1:G.num_users)\n",
    "    else\n",
    "        weights = expdecay(\n",
    "            get_counts(split; per_rating = false),\n",
    "            weighting_scheme(G.user_sampling_scheme),\n",
    "        )\n",
    "        return sample(1:G.num_users, Weights(weights), G.num_users)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f35efa-decd-42ed-bcae-f7ea7cab6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs the following steps\n",
    "# 1) shuffle the epoch by the sampling order\n",
    "# 2) split the epoch into minibatches of size batch_size\n",
    "# 3) return the iter-th minibatch\n",
    "function get_batch(epoch, iter, batch_size, sampling_order)\n",
    "    sampling_order = 1:G.num_users\n",
    "    range = sampling_order[(iter-1)*batch_size+1:min(iter * batch_size, G.num_users)]\n",
    "    process(x) = slice(x, range) |> device\n",
    "    [process.(epoch)], range\n",
    "end;\n",
    "\n",
    "function get_batch(epoch, iter, batch_size)\n",
    "    sampling_order = 1:G.num_users\n",
    "    get_batch(epoch, iter, batch_size, sampling_order)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4988ea0-8a00-41f1-857f-6064ef3806a7",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "* The `model_loss` is either the crossentropy loss or squared error, depending on the input data\n",
    "    * Note that we take the sum over all items, so using a bigger batchsize will have a bigger `model_loss`\n",
    "* The `regularization_loss` depends on the model architecture, but is commonly an L2 loss\n",
    "* During training, the `model_loss` is scaled by a function of the weight decays. This keeps the magnitude of the loss function approximately the same, even if the weight decay constats change\n",
    "* The `split_loss` is either the weighted average crossentropy loss or weighted mean squared error, depending on the input datadepending on the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52c6be0c-9646-4495-9bc5-bfd7b87ecc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "function model_loss(m, x, y, z, w)\n",
    "    p = m(x)\n",
    "    if G.implicit\n",
    "        β = sigmoid(G.residual_beta)\n",
    "        q = softmax(p) * (1 - β) + z .* β\n",
    "        return sum(w .* -y .* log.(q))\n",
    "    else\n",
    "        q = p + z .* G.residual_beta\n",
    "        return sum(w .* (q - y) .^ 2)\n",
    "    end\n",
    "end\n",
    "\n",
    "function training_loss(m, x, y, z, w; model_loss_scale)\n",
    "    model_loss(m, x, y, z, w) * model_loss_scale + regularization_loss(m)\n",
    "end\n",
    "\n",
    "function split_loss(m, split)\n",
    "    epoch = get_epoch(split)\n",
    "    loss = 0.0\n",
    "    weights = 0.0\n",
    "    for iter = 1:Int(ceil(G.num_users / G.batch_size))\n",
    "        batch, _ = get_batch(epoch, iter, G.batch_size)\n",
    "        loss += model_loss(m, batch[1]...)\n",
    "        weights += sum(batch[1][end])\n",
    "    end\n",
    "    Float32(loss / weights)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503872c-79e8-405c-acbb-518416c5c360",
   "metadata": {},
   "source": [
    "## Training\n",
    "* Trains a neural network with the given hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463d43b8-c951-41dd-8457-50272235449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_optimizer(optimizer, learning_rate)\n",
    "    if optimizer == \"ADAM\"\n",
    "        return ADAMW(learning_rate, (0.9, 0.999), 0)\n",
    "    elseif optimizer == \"SGD\"\n",
    "        return Descent(learning_rate)\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d61e7743-4bc9-4f75-a38d-b5e02ed1e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_epoch!(m, ps, opt)\n",
    "    LinearAlgebra.BLAS.set_num_threads(Threads.nthreads())\n",
    "    epoch = get_epoch(\"training\")\n",
    "    sampling_order = get_sampling_order(\"training\")\n",
    "    # make the training loss invariant to the scale of the weight decays\n",
    "    model_loss_scale = G.num_users / sum(epoch[4])\n",
    "    batchloss(x, y, z, w) =\n",
    "        training_loss(m, x, y, z, w; model_loss_scale = model_loss_scale)\n",
    "\n",
    "    nbatches = Int(ceil(length(sampling_order) / G.batch_size))\n",
    "    for iter = 1:nbatches\n",
    "        batch, _ = get_batch(epoch, iter, G.batch_size, sampling_order)\n",
    "        Flux.train!(batchloss, ps, batch, opt)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d31c53db-950b-438f-9318-ee357294f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains a model with the given hyperparams and returns its validation loss\n",
    "function train_model(hyp; max_checkpoints = 10, epochs_per_checkpoint = 10, patience = 0)\n",
    "    global G = hyp\n",
    "    opt = get_optimizer(G.optimizer, G.learning_rate)\n",
    "    Random.seed!(G.seed)\n",
    "    m = build_model() |> device\n",
    "    best_model = m |> cpu\n",
    "    ps = Flux.params(m)\n",
    "    stopper = early_stopper(max_iters = max_checkpoints, patience = patience)\n",
    "\n",
    "    losses = []\n",
    "    loss = Inf\n",
    "    while (!stop!(stopper, loss))\n",
    "        for i = 1:epochs_per_checkpoint\n",
    "            train_epoch!(m, ps, opt)\n",
    "        end\n",
    "        loss = split_loss(m, \"validation\")\n",
    "        push!(losses, loss)\n",
    "        if loss == minimum(losses)\n",
    "            best_model = m |> cpu\n",
    "        end\n",
    "    end\n",
    "    global G = nothing\n",
    "    best_model, minimum(losses)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bee255-17b6-4bc1-85aa-f3b89e46346c",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "* A derivative free optimizer is used to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dadbdc6d-d986-4870-a4e6-6467ac3aea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "function num_tuneable_params(model)\n",
    "    num_model_params = 4\n",
    "    if model == \"user_item_biases\"\n",
    "        num_sampling_params = 0\n",
    "        num_regularization_params = 2\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    num_model_params, num_sampling_params, num_regularization_params\n",
    "end\n",
    "\n",
    "function create_hyperparams(hyp, λ)\n",
    "    _, num_sampling_params, num_regularization_params = num_tuneable_params(hyp.model)\n",
    "    hyp = @set hyp.learning_rate = 0.01 * exp(λ[1])\n",
    "    hyp = @set hyp.residual_beta = hyp.implicit ? λ[2] : 1 + λ[2]\n",
    "    hyp = @set hyp.user_weight_decay = λ[3]\n",
    "    hyp = @set hyp.item_weight_decay = λ[4]\n",
    "    if num_sampling_params == 1\n",
    "        hyp = @set hyp.user_sampling_scheme = λ[5]\n",
    "    end\n",
    "    hyp = @set hyp.regularization_params = exp.(λ[end-num_regularization_params+1:end])\n",
    "    hyp\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aebe789-d898-4872-b5e2-376ea1074cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_hyperparams(hyp; max_evals)\n",
    "    function nlopt_loss(λ, grad)\n",
    "        # nlopt internally converts to float64 because it calls a c library\n",
    "        λ = convert.(Float32, λ)\n",
    "        _, loss = train_model(create_hyperparams(hyp, λ))\n",
    "        @info \"$λ $loss\"\n",
    "        loss\n",
    "    end\n",
    "    num_variables = sum(num_tuneable_params(hyp.model))\n",
    "    opt = NLopt.Opt(:LN_NELDERMEAD, num_variables)\n",
    "    opt.initial_step = 1\n",
    "    opt.maxeval = max_evals\n",
    "    opt.min_objective = nlopt_loss\n",
    "    minf, λ, ret = NLopt.optimize(opt, zeros(Float32, num_variables))\n",
    "    numevals = opt.numevals\n",
    "\n",
    "    @info (\n",
    "        \"found minimum $minf at point $λ after $numevals function calls \" *\n",
    "        \"(ended because $ret) and saved model at\"\n",
    "    )\n",
    "    λ\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cbfe4-1de9-4b3e-8828-0d4812e8df3c",
   "metadata": {},
   "source": [
    "## Retrain User Embeddings\n",
    "* To minimize training/serving skew, we train the model the same\n",
    "  way we will train it during inference\n",
    "* This means reinitializing the user embeddings, freezing all other layers,\n",
    "  and fine-tuning the user embeddings\n",
    "* During serving, we will determine a new user's embedding\n",
    "  by training with the same hyperparameters and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b2f716d-246d-4d28-a684-b0e7621e9bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function retrain_user_embeddings(hyp, m)\n",
    "    if hyp.model == \"user_item_biases\"\n",
    "        global G = hyp\n",
    "        opt = get_optimizer(G.optimizer, G.learning_rate)\n",
    "        Random.seed!(G.seed)\n",
    "        m = m |> cpu\n",
    "        m[1].weight .= Flux.Embedding(hyp.num_users => 1).weight\n",
    "        m = m |> device\n",
    "        best_model = m |> cpu\n",
    "        ps = Flux.params(m[1])\n",
    "        stopper = early_stopper(max_iters = 100, patience = 10)\n",
    "\n",
    "        losses = []\n",
    "        loss = Inf\n",
    "        while (!stop!(stopper, loss))\n",
    "            train_epoch!(m, ps, opt)\n",
    "            loss = split_loss(m, \"validation\")\n",
    "            push!(losses, loss)\n",
    "            if loss == minimum(losses)\n",
    "                best_model = m |> cpu\n",
    "            end\n",
    "        end\n",
    "        epochs = stopper.iters - stopper.iters_without_improvement\n",
    "        global G = nothing\n",
    "        return best_model, epochs\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd719b-7f8b-491e-bafa-8aa56a9545e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10731233-3a58-4b55-aa5d-26424fe031b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dict that maps a user to the list of items they have watched\n",
    "function user_to_items(users, items)\n",
    "    utoa = [Dict() for t = 1:Threads.nthreads()]\n",
    "    @tprogress Threads.@threads for j = 1:length(users)\n",
    "        u = users[j]\n",
    "        a = items[j]\n",
    "        t = Threads.threadid()\n",
    "        if u ∉ keys(utoa[t])\n",
    "            utoa[t][u] = []\n",
    "        end\n",
    "        push!(utoa[t][u], a)\n",
    "    end\n",
    "    merge(vcat, utoa...)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8966e09b-8e58-490d-a08a-2bce141e8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a ratings dataset of predicted ratings\n",
    "function evaluate(hyp, m, users, items)\n",
    "    # get model inputs\n",
    "    global G = hyp\n",
    "    m = m |> device\n",
    "    utoa = user_to_items(users, items)\n",
    "    epoch = [get_epoch_inputs(G.input_data, G.implicit, G.num_users)]\n",
    "    activation = G.implicit ? softmax : identity\n",
    "\n",
    "    # allocate outputs\n",
    "    out_users = Array{eltype(users)}(undef, length(users))\n",
    "    out_items = Array{eltype(items)}(undef, length(users))\n",
    "    out_ratings = Array{Float32}(undef, length(users))\n",
    "    out_idx = 1\n",
    "\n",
    "    # compute predictions    \n",
    "    @showprogress for iter = 1:Int(ceil(G.num_users / G.batch_size))\n",
    "        batch, sampled_users = get_batch(epoch, iter, G.batch_size)\n",
    "        alpha = activation(m(batch[1][1])) |> cpu\n",
    "        for j = 1:length(sampled_users)\n",
    "            u = sampled_users[j]\n",
    "            if u in keys(utoa)\n",
    "                item_mask = utoa[u]\n",
    "                next_idx = out_idx + length(item_mask)\n",
    "                out_users[out_idx:next_idx-1] .= u\n",
    "                out_items[out_idx:next_idx-1] = item_mask\n",
    "                out_ratings[out_idx:next_idx-1] = alpha[item_mask, j]\n",
    "                out_idx = next_idx\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    global G = nothing\n",
    "    RatingsDataset(user = out_users, item = out_items, rating = out_ratings)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46d999f4-f2b4-4d71-944b-754aead7de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "function write_alpha(hyp::Hyperparams, m, outdir)\n",
    "    splits = reduce(cat, [get_split(split, hyp.implicit) for split in all_splits])\n",
    "    preds = evaluate(hyp, m, splits.user, splits.item)\n",
    "    sparse_preds = sparse(preds.user, preds.item, preds.rating)\n",
    "\n",
    "\n",
    "    function model(users, items)\n",
    "        r = zeros(length(users))\n",
    "        @tprogress Threads.@threads for j = 1:length(r)\n",
    "            r[j] = sparse_preds[users[j], items[j]]\n",
    "        end\n",
    "        r\n",
    "    end\n",
    "\n",
    "    write_alpha(model, hyp.residual_alphas, hyp.implicit; outdir = outdir)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd693480-3562-4f60-91a2-395544381999",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_alpha(hyp, outdir)\n",
    "    # optimize hyperparameters\n",
    "    @info \"Optimizing hyperparameters...\"    \n",
    "    hyp_subset = @set hyp.num_users = Int(round(num_users() * 0.1))\n",
    "    λ = optimize_hyperparams(hyp_subset; max_evals = 100)\n",
    "\n",
    "    # train with the full dataset and with looser early-stopping rules\n",
    "    @info \"Training model...\"        \n",
    "    m, validation_loss = train_model(\n",
    "        create_hyperparams(hp, λ);\n",
    "        max_checkpoints = 100,\n",
    "        epochs_per_checkpoint = 1,\n",
    "        patience = 10,\n",
    "    )\n",
    "    @info \"Trained model loss: $validation_loss\"\n",
    "\n",
    "    @info \"Retraining user embeddings...\"            \n",
    "    hyp = create_hyperparams(hyp, λ)\n",
    "    m, epochs = retrain_user_embeddings(hyp, m)\n",
    "\n",
    "    @info \"Writing alpha...\"     \n",
    "    write_params(\n",
    "        Dict(\"m\" => m, \"epochs\" => epochs, \"λ\" => λ, \"hyp\" => hyp),\n",
    "        outdir = outdir,\n",
    "    )    \n",
    "    write_alpha(hyp, m, outdir)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88ceb699-dc26-4548-b8f5-4db2f081df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp = Hyperparams(\n",
    "#     # model\n",
    "#     implicit = false,\n",
    "#     model = \"user_item_biases\",\n",
    "#     # batching\n",
    "#     # 1024 is the smallest batch size that saturates the gpu\n",
    "#     batch_size = 1024,\n",
    "#     input_data = \"one_hot\",\n",
    "#     user_sampling_scheme = \"constant\",\n",
    "#     # optimizer\n",
    "#     learning_rate = 0.01,\n",
    "#     optimizer = \"ADAM\",\n",
    "#     # training\n",
    "#     seed = 20220524,\n",
    "#     num_users = num_users(),\n",
    "#     # loss\n",
    "#     item_weight_decay = 0,\n",
    "#     regularization_params = Float32[1, 1],\n",
    "#     residual_alphas = [],\n",
    "#     residual_beta = 0,\n",
    "#     user_weight_decay = 0,\n",
    "# )\n",
    "\n",
    "# λ = Float32[\n",
    "#     -0.45587917457999283,\n",
    "#     1.59948091680668,\n",
    "#     -0.4203013509949342,\n",
    "#     -0.025724981261365888,\n",
    "#     4.4043160918290605,\n",
    "#     2.42710246614073,\n",
    "# ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f56a14a5-cfef-40aa-9567-815ff5a371d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = read_params(name)\n",
    "# hyp = params[\"hyp\"]\n",
    "# m = params[\"m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d0bd3cc-15dc-4530-a391-3e0efc6cab7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write_alpha(hyp, m, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
