{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3758ece-9b72-47b4-ae6a-aa364e0443e3",
   "metadata": {},
   "source": [
    "# Neural Network Base Class\n",
    "* This class contains infrastructure to train neural networks\n",
    "* The following algorithms are implemented:\n",
    "    * Baseline predictors\n",
    "* The following algoirthms will be implemented\n",
    "    * Item-based collaborative filtering\n",
    "    * Matrix Factorization\n",
    "    * Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d17593ac-3260-4c01-a5df-91edfcd17e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Random\n",
    "using SparseArrays\n",
    "\n",
    "import CUDA\n",
    "import NBInclude: @nbinclude\n",
    "import NLopt\n",
    "import Setfield: @set\n",
    "@nbinclude(\"Alpha.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08801fd4-7057-49f0-9fa7-9854d17494cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# support both gpu and cpu training\n",
    "\n",
    "function device(x)\n",
    "    gpu(x)\n",
    "end\n",
    "\n",
    "# efficiently convert a sparse cpu matrix into a dense CUDA array\n",
    "function device(x::AbstractSparseArray)\n",
    "    CUDA.functional() ? CUDA.CuArray(gpu(x)) : x\n",
    "end\n",
    "\n",
    "if !CUDA.functional()\n",
    "    LinearAlgebra.BLAS.set_num_threads(Threads.nthreads())\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030f566-08b7-46ad-9b69-182e559a9f89",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "* Contains all the information necessary to train a new model\n",
    "* The important hyperparameters will tuned via a derivative-free optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abba16e9-f406-49c4-8af1-6e0afde838c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_kw struct Hyperparams\n",
    "    # model\n",
    "    implicit::Bool\n",
    "    input_data::String\n",
    "    model::String\n",
    "    # batching\n",
    "    batch_size::Int\n",
    "    user_sampling_scheme::Union{String,Float32}\n",
    "    # optimizer\n",
    "    learning_rate::Float32\n",
    "    optimizer::String\n",
    "    # training\n",
    "    seed::UInt64\n",
    "    num_users::Int\n",
    "    # loss\n",
    "    item_weight_decay::Float32\n",
    "    regularization_params::Vector{Float32}\n",
    "    residual_alphas::Vector{String}\n",
    "    residual_beta::Float32\n",
    "    user_weight_decay::Float32\n",
    "end\n",
    "\n",
    "function to_dict(x::Hyperparams)\n",
    "    Dict(string(key) => getfield(x, key) for key ∈ fieldnames(Hyperparams))\n",
    "end\n",
    "\n",
    "function Base.string(x::Hyperparams)\n",
    "    fields = [x for x in fieldnames(Hyperparams)]\n",
    "    max_field_size = maximum(length(string(k)) for k in fields)\n",
    "    ret = \"Hyperparameters:\\n\"\n",
    "    for f in fields\n",
    "        ret *= \"$(rpad(string(f), max_field_size)) => $(getfield(x, f))\\n\"\n",
    "    end\n",
    "    ret\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3fa63-a407-4e8d-86d2-b54b50e6d6ac",
   "metadata": {},
   "source": [
    "## Models\n",
    "* To define a new model, add the architecture to `build_model` and the regularization to `regularization_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9bcd93-0801-4944-9e44-b2b25cc16934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that takes one input and splits it into many\n",
    "struct Split{T}\n",
    "    paths::T\n",
    "end\n",
    "Split(paths...) = Split(paths)\n",
    "Flux.@functor Split\n",
    "(m::Split)(x::AbstractArray) = map(f -> f(x), m.paths)\n",
    "\n",
    "# A layer that takes many inputs and joins them into one\n",
    "Join(combine, paths) = Parallel(combine, paths)\n",
    "Join(combine, paths...) = Join(combine, paths);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "321ca69a-0b42-4859-8594-af05d93e52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that adds a 1-D vector to the input\n",
    "struct BiasLayer\n",
    "    b::Any\n",
    "end\n",
    "BiasLayer(n::Integer; init = zeros) = BiasLayer(init(Float32, n))\n",
    "(m::BiasLayer)(x) = x .+ m.b\n",
    "Flux.@functor BiasLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf80688-1e79-420a-9710-ae82a5b8184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements a baseline predictor given by R[i, j] = u[i] + a[j]\n",
    "function user_item_biases()\n",
    "    U = Flux.Embedding(G.num_users => 1, init = (x...) -> zeros(Float32, x...))\n",
    "    A = BiasLayer(num_items())\n",
    "    B = BiasLayer(1) # unregularized constant so that U, A can centered at 0\n",
    "    m = Chain(U, A, B)\n",
    "end\n",
    "\n",
    "# regularization is λ_u ||u|| + λ_a ||a||\n",
    "function user_item_biases_regularization(m, x)\n",
    "    sum(m[1](x) .^2) * G.regularization_params[1] + sum(m[2].b .^2) * G.regularization_params[2]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a4037-6c15-4839-9054-24d6703d00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements a baseline predictor given by R[i, j] = U[:, i] * A[:, j]\n",
    "function matrix_factorization(K)\n",
    "    U = Flux.Embedding(G.num_users => K, init = Flux.glorot_uniform)\n",
    "    A = Dense(K, num_items(), bias = false)\n",
    "    m = Chain(U, A)\n",
    "end\n",
    "\n",
    "# regularization is λ_u ||U|| + λ_a ||a||\n",
    "function matrix_factorization_regularization(m, x)\n",
    "    sum(m[1](x) .^ 2) * G.regularization_params[1] +\n",
    "    sum(m[2].weight .^ 2) * G.regularization_params[2]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8db136-b854-4cf5-8e8e-8dd6fbc28081",
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_model()\n",
    "    if G.model == \"user_item_biases\"\n",
    "        return user_item_biases()\n",
    "    elseif startswith(G.model, \"matrix_factorization\")\n",
    "        K = parse(Int, split(G.model, \"_\")[end])\n",
    "        return matrix_factorization(K)\n",
    "    end\n",
    "    @assert false\n",
    "end\n",
    "\n",
    "function regularization_loss(m, x)\n",
    "    if G.model == \"user_item_biases\"\n",
    "        return user_item_biases_regularization(m, x)\n",
    "    elseif startswith(G.model, \"matrix_factorization\")\n",
    "        return matrix_factorization_regularization(m, x)\n",
    "    end\n",
    "    @assert false\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b1aac-67d5-4dec-be8b-1e6c65b4d252",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "* An epoch is an efficient representation of all the models inputs, outputs, residualization, and weights\n",
    "* We generate one epoch per split and memoize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1c39d34-3212-4131-8e1d-68021044fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function one_hot_inputs(implicit, num_users)\n",
    "    convert.(Int32, collect(1:num_users))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c0cd51a-8bfe-498b-9185-c0ecaeb1a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_inputs(\n",
    "    input_data,\n",
    "    implicit,\n",
    "    num_users,\n",
    ")\n",
    "    if input_data == \"one_hot\"\n",
    "        return one_hot_inputs(implicit, num_users)\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "end\n",
    "\n",
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_outputs(split, implicit, num_users)\n",
    "    sparse(filter_users(get_split(split, implicit), num_users))\n",
    "end\n",
    "\n",
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_residuals(\n",
    "    split,\n",
    "    residual_alphas,\n",
    "    implicit,\n",
    "    num_users,\n",
    ")\n",
    "    residuals = filter_users(read_alpha(residual_alphas, split, implicit), num_users)\n",
    "    sparse(residuals)\n",
    "end\n",
    "\n",
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_weights(\n",
    "    split,\n",
    "    user_weight_decay,\n",
    "    item_weight_decay,\n",
    "    implicit,\n",
    "    num_users,\n",
    ")\n",
    "    if split == \"training\"\n",
    "        weights =\n",
    "            expdecay(get_counts(split, implicit), user_weight_decay) .*\n",
    "            expdecay(get_counts(split, implicit; by_item = true), item_weight_decay)\n",
    "    else\n",
    "        weights = expdecay(get_counts(split, implicit), weighting_scheme(\"inverse\"))\n",
    "    end\n",
    "\n",
    "    df = get_split(split, implicit)\n",
    "    df = filter_users(RatingsDataset(df.user, df.item, weights), num_users)\n",
    "    sparse(df)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "272ebd7c-a236-4a22-b0af-17759579f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns (X, Y, Z, W) = (inputs, outputs, residualization alpha, weights)\n",
    "function get_epoch(split)\n",
    "    X = get_epoch_inputs(G.input_data, G.implicit, G.num_users)\n",
    "    Y = get_epoch_outputs(split, G.implicit, G.num_users)\n",
    "    Z = get_epoch_residuals(split, G.residual_alphas, G.implicit, G.num_users)\n",
    "    W = get_epoch_weights(\n",
    "        split,\n",
    "        G.user_weight_decay,\n",
    "        G.item_weight_decay,\n",
    "        G.implicit,\n",
    "        G.num_users,\n",
    "    )\n",
    "    X, Y, Z, W\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a52cbc-01d6-487c-8293-702ef156698a",
   "metadata": {},
   "source": [
    "# Batching\n",
    "* Turns an epoch into minibatches\n",
    "* Each data point will appear in a minibatch with a probability proportional to its sampling weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2f87d3-1942-4abc-88ca-6b83433443c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "function SparseArrays.sparse(split::RatingsDataset)\n",
    "    sparse(split.item, split.user, split.rating, num_items(), G.num_users)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea29632c-edd2-436c-a103-54182bcb21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function slice(x::AbstractVector, range)\n",
    "    x[range]\n",
    "end\n",
    "\n",
    "function slice(x::AbstractMatrix, range)\n",
    "    x[:, range]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203dd198-8047-4470-8e80-4640739dd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sampling_order(split)\n",
    "    weighting_scheme = split == \"training\" ? G.user_sampling_scheme : \"constant\"\n",
    "    if weighting_scheme == \"constant\"\n",
    "        return shuffle(1:G.num_users)\n",
    "    else\n",
    "        weights = expdecay(\n",
    "            get_counts(split; per_rating = false),\n",
    "            weighting_scheme(G.user_sampling_scheme),\n",
    "        )\n",
    "        return sample(1:G.num_users, Weights(weights), G.num_users)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f35efa-decd-42ed-bcae-f7ea7cab6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs the following steps\n",
    "# 1) shuffle the epoch by the sampling order\n",
    "# 2) split the epoch into minibatches of size batch_size\n",
    "# 3) return the iter-th minibatch\n",
    "function get_batch(epoch, iter, batch_size, sampling_order)\n",
    "    sampling_order = 1:G.num_users\n",
    "    range = sampling_order[(iter-1)*batch_size+1:min(iter * batch_size, G.num_users)]\n",
    "    process(x) = slice(x, range) |> device\n",
    "    [process.(epoch)], range\n",
    "end;\n",
    "\n",
    "function get_batch(epoch, iter, batch_size)\n",
    "    sampling_order = 1:G.num_users\n",
    "    get_batch(epoch, iter, batch_size, sampling_order)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4988ea0-8a00-41f1-857f-6064ef3806a7",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "* The `model_loss` is either the crossentropy loss or squared error, depending on the input data\n",
    "    * Note that we take the sum over all items, so using a bigger batchsize will have a bigger `model_loss`\n",
    "* The `regularization_loss` depends on the model architecture, but is commonly an L2 loss\n",
    "* During training, the `model_loss` is scaled by a function of the weight decays. This keeps the magnitude of the loss function approximately the same, even if the weight decay constats change\n",
    "* The `split_loss` is either the weighted average crossentropy loss or weighted mean squared error, depending on the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52c6be0c-9646-4495-9bc5-bfd7b87ecc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "function model_loss(m, x, y, z, w)\n",
    "    p = m(x)\n",
    "    if G.implicit\n",
    "        q = softmax(p) .* (1 - G.residual_beta) + z .* G.residual_beta\n",
    "        return sum(w .* -y .* log.(q))\n",
    "    else\n",
    "        q = p + z .* G.residual_beta\n",
    "        return sum(w .* (q - y) .^ 2)\n",
    "    end\n",
    "end\n",
    "\n",
    "function training_loss(m, x, y, z, w)\n",
    "    model_loss(m, x, y, z, w) + regularization_loss(m, x)\n",
    "end\n",
    "\n",
    "function split_loss(m, split)\n",
    "    epoch = get_epoch(split)\n",
    "    loss = 0.0\n",
    "    weights = 0.0\n",
    "    for iter = 1:Int(ceil(G.num_users / G.batch_size))\n",
    "        batch, _ = get_batch(epoch, iter, G.batch_size)\n",
    "        loss += model_loss(m, batch[1]...)\n",
    "        weights += sum(batch[1][end])\n",
    "    end\n",
    "    Float32(loss / weights)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503872c-79e8-405c-acbb-518416c5c360",
   "metadata": {},
   "source": [
    "## Training\n",
    "* Trains a neural network with the given hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463d43b8-c951-41dd-8457-50272235449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_optimizer(optimizer, learning_rate)\n",
    "    if optimizer == \"ADAM\"\n",
    "        return ADAMW(learning_rate, (0.9, 0.999), 0)\n",
    "    elseif optimizer == \"SGD\"\n",
    "        return Descent(learning_rate)\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d61e7743-4bc9-4f75-a38d-b5e02ed1e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_epoch!(m, ps, opt)\n",
    "    epoch = get_epoch(\"training\")\n",
    "    sampling_order = get_sampling_order(\"training\")\n",
    "    batchloss(x, y, z, w) = training_loss(m, x, y, z, w)\n",
    "\n",
    "    nbatches = Int(ceil(length(sampling_order) / G.batch_size))\n",
    "    for iter = 1:nbatches\n",
    "        batch, _ = get_batch(epoch, iter, G.batch_size, sampling_order)\n",
    "        Flux.train!(batchloss, ps, batch, opt)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d31c53db-950b-438f-9318-ee357294f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains a model with the given hyperparams and returns its validation loss\n",
    "function train_model(\n",
    "    hyp;\n",
    "    max_checkpoints = 50,\n",
    "    epochs_per_checkpoint = 10,\n",
    "    patience = 0,\n",
    "    verbose = false,\n",
    "    init_model = nothing,\n",
    ")\n",
    "    global G = hyp\n",
    "    opt = get_optimizer(G.optimizer, G.learning_rate)\n",
    "    Random.seed!(G.seed)\n",
    "    if isnothing(init_model)\n",
    "        m = build_model() |> device\n",
    "    else\n",
    "        m = init_model |> device\n",
    "    end\n",
    "    best_model = m |> cpu\n",
    "    ps = Flux.params(m)\n",
    "    stopper = early_stopper(max_iters = max_checkpoints, patience = patience)\n",
    "\n",
    "    losses = []\n",
    "    loss = Inf\n",
    "    while (!stop!(stopper, loss))\n",
    "        for i = 1:epochs_per_checkpoint\n",
    "            train_epoch!(m, ps, opt)\n",
    "        end\n",
    "        loss = split_loss(m, \"validation\")\n",
    "        push!(losses, loss)\n",
    "        if loss == minimum(losses)\n",
    "            best_model = m |> cpu\n",
    "        end\n",
    "        if verbose\n",
    "            training_loss = split_loss(m, \"training\")\n",
    "            @info \"losses: $training_loss $loss\"\n",
    "        end\n",
    "    end\n",
    "    global G = nothing\n",
    "    best_model, minimum(losses)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bee255-17b6-4bc1-85aa-f3b89e46346c",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "* A derivative free optimizer is used to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbdc6d-d986-4870-a4e6-6467ac3aea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "function num_tuneable_params(model)\n",
    "    num_model_params = 4\n",
    "    if (model == \"user_item_biases\") || startswith(model, \"matrix_factorization\")\n",
    "        num_sampling_params = 0\n",
    "        num_regularization_params = 2\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    num_model_params, num_sampling_params, num_regularization_params\n",
    "end\n",
    "\n",
    "function create_hyperparams(hyp::Hyperparams, λ)\n",
    "    _, num_sampling_params, num_regularization_params = num_tuneable_params(hyp.model)\n",
    "    hyp = @set hyp.learning_rate = 1e-3 * 10 ^ (λ[1])\n",
    "    hyp = @set hyp.residual_beta = hyp.implicit ? sigmoid(λ[2]) : exp(λ[2])\n",
    "    hyp = @set hyp.user_weight_decay = λ[3]\n",
    "    hyp = @set hyp.item_weight_decay = λ[4]\n",
    "    if num_sampling_params == 1\n",
    "        hyp = @set hyp.user_sampling_scheme = λ[5]\n",
    "    end\n",
    "    hyp =\n",
    "        @set hyp.regularization_params = 1e-4 .* 10 .^ (λ[end-num_regularization_params+1:end])\n",
    "    hyp\n",
    "end\n",
    "\n",
    "function create_hyperparams(model::String, implicit, residual_alphas)\n",
    "    if (model == \"user_item_biases\") || startswith(model, \"matrix_factorization\")\n",
    "        input_data = \"one_hot\"\n",
    "        user_sampling_scheme = \"constant\"\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    hyp = Hyperparams(\n",
    "        implicit = implicit,\n",
    "        model = model,\n",
    "        batch_size = 1024,\n",
    "        input_data = input_data,\n",
    "        user_sampling_scheme = user_sampling_scheme,\n",
    "        learning_rate = NaN,\n",
    "        optimizer = \"ADAM\",\n",
    "        seed = 20220524,\n",
    "        num_users = num_users(),\n",
    "        item_weight_decay = NaN,\n",
    "        regularization_params = fill(NaN, sum(num_tuneable_params(model))),\n",
    "        residual_alphas = residual_alphas,\n",
    "        residual_beta = NaN,\n",
    "        user_weight_decay = NaN,\n",
    "    )\n",
    "    create_hyperparams(hyp, zeros(Float32, sum(num_tuneable_params(model))))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aebe789-d898-4872-b5e2-376ea1074cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_hyperparams(hyp; max_evals)\n",
    "    function nlopt_loss(λ, grad)\n",
    "        # nlopt internally converts to float64 because it calls a c library\n",
    "        λ = convert.(Float32, λ)\n",
    "        _, loss = train_model(create_hyperparams(hyp, λ))\n",
    "        @info \"$λ $loss\"\n",
    "        loss\n",
    "    end\n",
    "    num_variables = sum(num_tuneable_params(hyp.model))\n",
    "    opt = NLopt.Opt(:LN_NELDERMEAD, num_variables)\n",
    "    opt.initial_step = 1\n",
    "    opt.maxeval = max_evals\n",
    "    opt.min_objective = nlopt_loss\n",
    "    minf, λ, ret = NLopt.optimize(opt, zeros(Float32, num_variables))\n",
    "    numevals = opt.numevals\n",
    "\n",
    "    @info (\n",
    "        \"found minimum $minf at point $λ after $numevals function calls \" *\n",
    "        \"(ended because $ret)\"\n",
    "    )\n",
    "    λ\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cbfe4-1de9-4b3e-8828-0d4812e8df3c",
   "metadata": {},
   "source": [
    "## Retrain User Embeddings\n",
    "* To minimize training/serving skew, we train the model the same\n",
    "  way we will train it during inference\n",
    "* This means reinitializing the user embeddings, freezing all other layers,\n",
    "  and fine-tuning the user embeddings\n",
    "* During serving, we will determine a new user's embedding\n",
    "  by training with the same hyperparameters and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b2f716d-246d-4d28-a684-b0e7621e9bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function retrain_user_embeddings(hyp, m)\n",
    "    global G = hyp\n",
    "    if G.model == \"user_item_biases\"\n",
    "        embedding_size = 1\n",
    "        initfn = (x...) -> zeros(Float32, x...)\n",
    "    elseif startswith(G.model, \"matrix_factorization\")\n",
    "        embedding_size = parse(Int, split(G.model, \"_\")[end])\n",
    "        initfn = Flux.glorot_uniform\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    opt = get_optimizer(G.optimizer, G.learning_rate)\n",
    "    Random.seed!(G.seed)\n",
    "    m = m |> cpu\n",
    "    m[1].weight .=\n",
    "        Flux.Embedding(hyp.num_users => embedding_size, init = initfn).weight\n",
    "    m = m |> device\n",
    "    best_model = m |> cpu\n",
    "    ps = Flux.params(m[1])\n",
    "    stopper = early_stopper(max_iters = 1000, patience = 10)\n",
    "\n",
    "    losses = []\n",
    "    loss = Inf\n",
    "    while (!stop!(stopper, loss))\n",
    "        train_epoch!(m, ps, opt)\n",
    "        loss = split_loss(m, \"validation\")\n",
    "        push!(losses, loss)\n",
    "        if loss == minimum(losses)\n",
    "            best_model = m |> cpu\n",
    "        end\n",
    "    end\n",
    "    epochs = stopper.iters - stopper.iters_without_improvement\n",
    "    global G = nothing\n",
    "    best_model, epochs, loss\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd719b-7f8b-491e-bafa-8aa56a9545e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10731233-3a58-4b55-aa5d-26424fe031b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dict that maps a user to the list of items they have watched\n",
    "function user_to_items(users, items)\n",
    "    utoa = [Dict() for t = 1:Threads.nthreads()]\n",
    "    @tprogress Threads.@threads for j = 1:length(users)\n",
    "        u = users[j]\n",
    "        a = items[j]\n",
    "        t = Threads.threadid()\n",
    "        if u ∉ keys(utoa[t])\n",
    "            utoa[t][u] = []\n",
    "        end\n",
    "        push!(utoa[t][u], a)\n",
    "    end\n",
    "    merge(vcat, utoa...)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8966e09b-8e58-490d-a08a-2bce141e8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a ratings dataset of predicted ratings\n",
    "function evaluate(hyp, m, users, items)\n",
    "    # get model inputs\n",
    "    global G = hyp\n",
    "    m = m |> device\n",
    "    utoa = user_to_items(users, items)\n",
    "    epoch = [get_epoch_inputs(G.input_data, G.implicit, G.num_users)]\n",
    "    activation = G.implicit ? softmax : identity\n",
    "\n",
    "    # allocate outputs\n",
    "    out_users = Array{eltype(users)}(undef, length(users))\n",
    "    out_items = Array{eltype(items)}(undef, length(users))\n",
    "    out_ratings = Array{Float32}(undef, length(users))\n",
    "    out_idx = 1\n",
    "\n",
    "    # compute predictions    \n",
    "    @showprogress for iter = 1:Int(ceil(G.num_users / G.batch_size))\n",
    "        batch, sampled_users = get_batch(epoch, iter, G.batch_size)\n",
    "        alpha = activation(m(batch[1][1])) |> cpu\n",
    "        for j = 1:length(sampled_users)\n",
    "            u = sampled_users[j]\n",
    "            if u in keys(utoa)\n",
    "                item_mask = utoa[u]\n",
    "                next_idx = out_idx + length(item_mask)\n",
    "                out_users[out_idx:next_idx-1] .= u\n",
    "                out_items[out_idx:next_idx-1] = item_mask\n",
    "                out_ratings[out_idx:next_idx-1] = alpha[item_mask, j]\n",
    "                out_idx = next_idx\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    global G = nothing\n",
    "    RatingsDataset(user = out_users, item = out_items, rating = out_ratings)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46d999f4-f2b4-4d71-944b-754aead7de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "function write_alpha(hyp::Hyperparams, m, outdir)\n",
    "    splits = reduce(cat, [get_split(split, hyp.implicit) for split in all_raw_splits])\n",
    "    preds = evaluate(hyp, m, splits.user, splits.item)\n",
    "    sparse_preds = sparse(preds.user, preds.item, preds.rating)\n",
    "    write_alpha(sparse_preds, hyp.residual_alphas, hyp.implicit, outdir)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd693480-3562-4f60-91a2-395544381999",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_alpha(hyp, outdir; tune_hyperparams = true)\n",
    "    set_logging_outdir(outdir)\n",
    "\n",
    "    if tune_hyperparams\n",
    "        @info \"Optimizing hyperparameters...\"\n",
    "        subsampling_factor = 0.01\n",
    "        hyp_subset = @set hyp.num_users = Int(round(num_users() * subsampling_factor))\n",
    "        λ = optimize_hyperparams(hyp_subset; max_evals = 100)\n",
    "    else\n",
    "        λ = zeros(Float32, sum(num_tuneable_params(hyp.model)))\n",
    "    end\n",
    "    hyp = create_hyperparams(hyp, λ)\n",
    "\n",
    "    @info \"Training model...\"\n",
    "    learning_rate_decay = exp(-1)\n",
    "    m = nothing\n",
    "    validation_loss = Inf\n",
    "    hyp = @set hyp.learning_rate /= learning_rate_decay\n",
    "    while true\n",
    "        # we have more training data, so we can afford a lower learning rate\n",
    "        # we exponentially decay the learning rate whenever we hit a plateau\n",
    "        hyp2 = @set hyp.learning_rate *= learning_rate_decay\n",
    "        m2, validation_loss2 = train_model(\n",
    "            hyp2;\n",
    "            max_checkpoints = 1000,\n",
    "            epochs_per_checkpoint = 1,\n",
    "            patience = 10,\n",
    "            init_model = m,\n",
    "        )\n",
    "        @info \"loss: $validation_loss2 learning_rate: $(hyp2.learning_rate)\"\n",
    "        tolerance = 1e-4\n",
    "        if validation_loss2 >= validation_loss * (1 - tolerance)\n",
    "            break\n",
    "        end\n",
    "        m = m2\n",
    "        validation_loss = validation_loss2\n",
    "        hyp = hyp2\n",
    "    end\n",
    "    @info \"Trained model loss: $validation_loss\"\n",
    "\n",
    "    @info \"Retraining user embeddings...\"\n",
    "    m, epochs, retrain_loss = retrain_user_embeddings(hyp, m)\n",
    "    @info \"Retrained user embeddings with loss: $retrain_loss\"\n",
    "\n",
    "\n",
    "    @info \"Writing alpha...\"\n",
    "    write_params(Dict(\"m\" => m, \"epochs\" => epochs, \"λ\" => λ, \"hyp\" => hyp), outdir)\n",
    "    write_alpha(hyp, m, outdir)\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
