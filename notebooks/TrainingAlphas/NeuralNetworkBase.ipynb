{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3758ece-9b72-47b4-ae6a-aa364e0443e3",
   "metadata": {},
   "source": [
    "# Neural Network Base Class\n",
    "* This class contains infrastructure to train neural networks\n",
    "* The following algorithms are implemented:\n",
    "    * Baseline predictors\n",
    "* The following algoirthms will be implemented\n",
    "    * Item-based collaborative filtering\n",
    "    * Matrix Factorization\n",
    "    * Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d17593ac-3260-4c01-a5df-91edfcd17e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Random\n",
    "using SparseArrays\n",
    "using Statistics: var\n",
    "\n",
    "import BSON\n",
    "import CUDA\n",
    "import NBInclude: @nbinclude\n",
    "import NLopt\n",
    "import Setfield: @set\n",
    "@nbinclude(\"Alpha.ipynb\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08801fd4-7057-49f0-9fa7-9854d17494cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function device(x)\n",
    "    gpu(x)\n",
    "end\n",
    "\n",
    "# efficiently convert a sparse cpu matrix into a dense CUDA array\n",
    "function device(x::AbstractSparseArray)\n",
    "    CUDA.functional() ? CUDA.CuArray(gpu(x)) : x\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11693af-0136-4b3e-a1bb-2806a172b5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030f566-08b7-46ad-9b69-182e559a9f89",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "* Contains all the information necessary to train a new model\n",
    "* The important hyperparameters will tuned via a derivative-free optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abba16e9-f406-49c4-8af1-6e0afde838c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_kw struct Hyperparams\n",
    "    # model\n",
    "    implicit::Bool\n",
    "    input_data::String\n",
    "    model::String\n",
    "    # batching\n",
    "    batch_size::Int\n",
    "    user_sampling_scheme::String # TODO convert to float32\n",
    "    # optimizer\n",
    "    learning_rate::Float32\n",
    "    optimizer::String\n",
    "    # training\n",
    "    seed::UInt64\n",
    "    num_users::Int\n",
    "    # loss\n",
    "    item_weight_decay::Float32\n",
    "    regularization_params::Vector{Float32}\n",
    "    residual_alphas::Vector{String}\n",
    "    residual_beta::Float32\n",
    "    user_weight_decay::Float32\n",
    "end\n",
    "\n",
    "function to_dict(x::Hyperparams)\n",
    "    Dict(string(key) => getfield(x, key) for key ∈ fieldnames(Hyperparams))\n",
    "end\n",
    "\n",
    "function Base.string(x::Hyperparams)\n",
    "    fields = [x for x in fieldnames(Hyperparams)]\n",
    "    max_field_size = maximum(length(string(k)) for k in fields)\n",
    "    ret = \"Hyperparameters:\\n\"\n",
    "    for f in fields\n",
    "        ret *= \"$(rpad(string(f), max_field_size)) => $(getfield(x, f))\\n\"\n",
    "    end\n",
    "    ret\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3fa63-a407-4e8d-86d2-b54b50e6d6ac",
   "metadata": {},
   "source": [
    "## Models\n",
    "* To define a new model, add the architecture to `build_model` and the regularization to `regularization_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd9bcd93-0801-4944-9e44-b2b25cc16934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that takes one input and splits it into many\n",
    "struct Split{T}\n",
    "    paths::T\n",
    "end\n",
    "Split(paths...) = Split(paths)\n",
    "Flux.@functor Split\n",
    "(m::Split)(x::AbstractArray) = map(f -> f(x), m.paths)\n",
    "\n",
    "# A layer that takes many inputs and joins them into one\n",
    "Join(combine, paths) = Parallel(combine, paths)\n",
    "Join(combine, paths...) = Join(combine, paths);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "321ca69a-0b42-4859-8594-af05d93e52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that adds a 1-D vector to the input\n",
    "struct BiasLayer\n",
    "    b::Any\n",
    "end\n",
    "BiasLayer(n::Integer; init = randn) = BiasLayer(init(Float32, n))\n",
    "(m::BiasLayer)(x) = x .+ m.b\n",
    "Flux.@functor BiasLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf80688-1e79-420a-9710-ae82a5b8184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements a baseline predictor given by R[i, j] = u[i] + a[j]\n",
    "function user_item_biases()\n",
    "    U = Flux.Embedding(G.num_users => 1)\n",
    "    A = BiasLayer(num_items())\n",
    "    m = Chain(U, A) |> device\n",
    "end\n",
    "\n",
    "# regularization is λ_u variance(u) + λ_a variance(a)\n",
    "function user_item_biases_regularization(m)\n",
    "    var(m[1].weight) * G.regularization_params[1] + var(m[2].b) * G.regularization_params[2]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8db136-b854-4cf5-8e8e-8dd6fbc28081",
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_model()\n",
    "    if G.model == \"user_item_biases\"\n",
    "        return user_item_biases()\n",
    "    end\n",
    "    @assert false\n",
    "end\n",
    "\n",
    "function regularization_loss(m)\n",
    "    if G.model == \"user_item_biases\"\n",
    "        return user_item_biases_regularization(m)\n",
    "    end\n",
    "    @assert false\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b1aac-67d5-4dec-be8b-1e6c65b4d252",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "* An epoch is an efficient representation of all the models inputs, outputs, residualization, and weights\n",
    "* We generate one epoch per split and memoize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1c39d34-3212-4131-8e1d-68021044fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function one_hot_inputs(split, implicit, num_users)\n",
    "    collect(1:num_users)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c0cd51a-8bfe-498b-9185-c0ecaeb1a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_inputs(\n",
    "    split,\n",
    "    input_data,\n",
    "    implicit,\n",
    "    num_users,\n",
    ")\n",
    "    if input_data == \"one_hot\"\n",
    "        X, Y = one_hot_inputs(split, implicit, num_users)\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "end\n",
    "\n",
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_outputs(split, implicit, num_users)\n",
    "    sparse(filter_users(get_split(split, implicit), num_users))\n",
    "end\n",
    "\n",
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_residuals(\n",
    "    split,\n",
    "    residual_alphas,\n",
    "    implicit,\n",
    "    num_users,\n",
    ")\n",
    "    residuals = filter_users(read_alpha(residual_alphas, split, implicit), num_users)\n",
    "    sparse(residuals)\n",
    "end\n",
    "\n",
    "@memoize LRU{Any,Any}(maxsize = 2) function get_epoch_weights(\n",
    "    split,\n",
    "    user_weight_decay,\n",
    "    item_weight_decay,\n",
    "    implicit,\n",
    "    num_users,\n",
    ")\n",
    "    if split == \"training\"\n",
    "        weights =\n",
    "            expdecay(get_counts(split, implicit), user_weight_decay) .*\n",
    "            expdecay(get_counts(split, implicit; by_item = true), item_weight_decay)\n",
    "    else\n",
    "        weights = expdecay(get_counts(split, implicit), weighting_scheme(\"inverse\"))\n",
    "    end\n",
    "\n",
    "    df = get_split(split, implicit)\n",
    "    df = filter_users(RatingsDataset(df.user, df.item, weights), num_users)\n",
    "    sparse(df)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "272ebd7c-a236-4a22-b0af-17759579f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns (X, Y, Z, W) = (inputs, outputs, residualization alpha, weights)\n",
    "function get_epoch(split)\n",
    "    X = get_epoch_inputs(split, G.input_data, G.implicit, G.num_users)\n",
    "    Y = get_epoch_outputs(split, G.implicit, G.num_users)\n",
    "    Z = get_epoch_residuals(split, G.residual_alphas, G.implicit, G.num_users)\n",
    "    W = get_epoch_weights(\n",
    "        split,\n",
    "        G.user_weight_decay,\n",
    "        G.item_weight_decay,\n",
    "        G.implicit,\n",
    "        G.num_users,\n",
    "    )\n",
    "    X, Y, Z, W\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a52cbc-01d6-487c-8293-702ef156698a",
   "metadata": {},
   "source": [
    "# Batching\n",
    "* Turns an epoch into minibatches\n",
    "* Each data point will appear in a minibatch with a probability proportional to its sampling weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2f87d3-1942-4abc-88ca-6b83433443c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "function SparseArrays.sparse(split::RatingsDataset)\n",
    "    sparse(split.item, split.user, split.rating, num_items(), G.num_users)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea29632c-edd2-436c-a103-54182bcb21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function slice(x::AbstractVector, range)\n",
    "    x[range]\n",
    "end\n",
    "\n",
    "function slice(x::AbstractMatrix, range)\n",
    "    x[:, range]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203dd198-8047-4470-8e80-4640739dd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_sampling_order(split)\n",
    "    weighting_scheme = split == \"training\" ? G.user_sampling_scheme : \"constant\"\n",
    "    if weighting_scheme == \"constant\"\n",
    "        return shuffle(1:G.num_users)\n",
    "    else\n",
    "        weights = expdecay(\n",
    "            get_counts(split; per_rating = false),\n",
    "            weighting_scheme(G.user_sampling_scheme),\n",
    "        )\n",
    "        return sample(1:G.num_users, Weights(weights), G.num_users)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f35efa-decd-42ed-bcae-f7ea7cab6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs the following steps\n",
    "# 1) shuffle the epoch by the sampling order\n",
    "# 2) split the epoch into minibatches of size batch_size\n",
    "# 3) return the iter-th minibatch\n",
    "function get_batch(epoch, iter, batch_size, sampling_order)\n",
    "    sampling_order = 1:G.num_users\n",
    "    range = sampling_order[(iter-1)*batch_size+1:min(iter * batch_size, G.num_users)]\n",
    "    process(x) = slice(x, range) |> device\n",
    "    [process.(epoch)], range\n",
    "end;\n",
    "\n",
    "function get_batch(epoch, iter, batch_size)\n",
    "    sampling_order = 1:G.num_users\n",
    "    get_batch(epoch, iter, batch_size, sampling_order)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4988ea0-8a00-41f1-857f-6064ef3806a7",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "* The `model_loss` is either the crossentropy loss or squared error, depending on the input data\n",
    "    * Note that we take the sum over all items, so using a bigger batchsize will have a bigger `model_loss`\n",
    "* The `regularization_loss` depends on the model architecture, but is commonly an L2 loss\n",
    "* During training, the `model_loss` is scaled by a function of the weight decays. This keeps the magnitude of the loss function approximately the same, even if the weight decay constats change\n",
    "* The `split_loss` is either the weighted average crossentropy loss or weighted mean squared error, depending on the input datadepending on the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52c6be0c-9646-4495-9bc5-bfd7b87ecc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "function model_loss(m, x, y, z, w)\n",
    "    p = m(x)\n",
    "    if G.implicit\n",
    "        β = sigmoid(G.residual_beta)\n",
    "        q = softmax(p) * (1 - β) + z .* β\n",
    "        return sum(w .* -y .* log.(q))\n",
    "    else\n",
    "        q = p + z .* G.residual_beta\n",
    "        return sum(w .* (q - y) .^ 2)\n",
    "    end\n",
    "end\n",
    "\n",
    "function training_loss(m, x, y, z, w; model_loss_scale)\n",
    "    model_loss(m, x, y, z, w) * model_loss_scale + regularization_loss(m)\n",
    "end\n",
    "\n",
    "function split_loss(m, split)\n",
    "    epoch = get_epoch(split)\n",
    "    loss = 0.0\n",
    "    weights = 0.0\n",
    "    for iter = 1:Int(ceil(G.num_users / G.batch_size))\n",
    "        batch, _ = get_batch(epoch, iter, G.batch_size)\n",
    "        loss += model_loss(m, batch[1]...)\n",
    "        weights += sum(batch[1][end])\n",
    "    end\n",
    "    Float32(loss / weights)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503872c-79e8-405c-acbb-518416c5c360",
   "metadata": {},
   "source": [
    "## Training\n",
    "* Trains a neural network with the given hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463d43b8-c951-41dd-8457-50272235449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_optimizer(optimizer, learning_rate)\n",
    "    if optimizer == \"ADAM\"\n",
    "        return ADAMW(learning_rate, (0.9, 0.999), 0)\n",
    "    elseif optimizer == \"SGD\"\n",
    "        return Descent(learning_rate)\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d61e7743-4bc9-4f75-a38d-b5e02ed1e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_epoch!(m, ps, opt)\n",
    "    LinearAlgebra.BLAS.set_num_threads(Threads.nthreads())\n",
    "    epoch = get_epoch(\"training\")\n",
    "    sampling_order = get_sampling_order(\"training\")\n",
    "    # make the training loss invariant to the scale of the weight decays\n",
    "    model_loss_scale = G.num_users / sum(epoch[4])\n",
    "    batchloss(x, y, z, w) =\n",
    "        training_loss(m, x, y, z, w; model_loss_scale = model_loss_scale)\n",
    "\n",
    "    nbatches = Int(ceil(length(sampling_order) / G.batch_size))\n",
    "    for iter = 1:nbatches\n",
    "        batch, _ = get_batch(epoch, iter, G.batch_size, sampling_order)\n",
    "        Flux.train!(batchloss, ps, batch, opt)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d31c53db-950b-438f-9318-ee357294f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains a model with the given hyperparams and returns its validation loss\n",
    "function train_model(hyp; max_checkpoints = 10, epochs_per_checkpoint = 10, patience = 0)\n",
    "    global G = hyp\n",
    "    opt = get_optimizer(G.optimizer, G.learning_rate)\n",
    "    Random.seed!(G.seed)\n",
    "    m = build_model()\n",
    "    best_model = m |> cpu\n",
    "    ps = Flux.params(m)\n",
    "    stopper = early_stopper(max_iters = max_checkpoints, patience = patience)\n",
    "\n",
    "    losses = []\n",
    "    loss = Inf\n",
    "    while (!stop!(stopper, loss))\n",
    "        for i = 1:epochs_per_checkpoint\n",
    "            train_epoch!(m, ps, opt)\n",
    "        end\n",
    "        loss = split_loss(m, \"validation\")\n",
    "        push!(losses, loss)\n",
    "        if loss == minimum(losses)\n",
    "            best_model = m |> cpu\n",
    "        end\n",
    "    end\n",
    "    global G = nothing\n",
    "    best_model, minimum(losses)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bee255-17b6-4bc1-85aa-f3b89e46346c",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "* A derivative free optimizer is used to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dadbdc6d-d986-4870-a4e6-6467ac3aea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "function num_tuneable_params(model)\n",
    "    num_model_params = 4\n",
    "    if model == \"user_item_biases\"\n",
    "        num_sampling_params = 0\n",
    "        num_regularization_params = 2\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "    num_model_params, num_sampling_params, num_regularization_params\n",
    "end\n",
    "\n",
    "function create_hyperparams(hyp, λ)\n",
    "    _, num_sampling_params, num_regularization_params = num_tuneable_params(hyp.model)\n",
    "    hyp = @set hyp.learning_rate = 0.01 * exp(λ[1])\n",
    "    hyp = @set hyp.residual_beta = hyp.implicit ? λ[2] : 1 + λ[2]\n",
    "    hyp = @set hyp.user_weight_decay = λ[3]\n",
    "    hyp = @set hyp.item_weight_decay = λ[4]\n",
    "    if num_sampling_params == 1\n",
    "        hyp = @set hyp.user_sampling_scheme = λ[5]\n",
    "    end\n",
    "    hyp = @set hyp.regularization_params = exp.(λ[end-num_regularization_params+1:end])\n",
    "    hyp\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aebe789-d898-4872-b5e2-376ea1074cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_hyperparams(hyp; max_evals)\n",
    "    function nlopt_loss(λ, grad)\n",
    "        # nlopt internally converts to float64 because it calls a c library\n",
    "        λ = convert.(Float32, λ)\n",
    "        _, loss = train_model(create_hyperparams(hyp, λ))\n",
    "        @info \"$λ $loss\"\n",
    "        loss\n",
    "    end\n",
    "    num_variables = sum(num_tuneable_params(hyp.model))\n",
    "    opt = NLopt.Opt(:LN_NELDERMEAD, num_variables)\n",
    "    opt.initial_step = 1\n",
    "    opt.maxeval = max_evals\n",
    "    opt.min_objective = nlopt_loss\n",
    "    minf, λ, ret = NLopt.optimize(opt, zeros(Float32, num_variables))\n",
    "    numevals = opt.numevals\n",
    "\n",
    "    @info (\n",
    "        \"found minimum $minf at point $λ after $numevals function calls \" *\n",
    "        \"(ended because $ret) and saved model at\"\n",
    "    )\n",
    "    λ\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88ceb699-dc26-4548-b8f5-4db2f081df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = Hyperparams(\n",
    "    # model\n",
    "    implicit = false,\n",
    "    model = \"user_item_biases\",\n",
    "    # batching\n",
    "    # 1024 is the smallest batch size that saturates the gpu\n",
    "    batch_size = 1024,\n",
    "    input_data = \"one_hot\",\n",
    "    user_sampling_scheme = \"constant\",\n",
    "    # optimizer\n",
    "    learning_rate = 0.01,\n",
    "    optimizer = \"ADAM\",\n",
    "    # training\n",
    "    seed = 20220524,\n",
    "    num_users = num_users(),\n",
    "    # loss\n",
    "    item_weight_decay = 0,\n",
    "    regularization_params = Float32[1, 1],\n",
    "    residual_alphas = [],\n",
    "    residual_beta = 0,\n",
    "    user_weight_decay = 0,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5fc00-e357-4789-8d7a-4b011db000ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:01 ( 0.12 μs/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:04 (31.77 ns/it)\u001b[39m\n",
      "\u001b[32mProgress: 100%|███████████████████████████| Time: 0:00:04 (36.33 ns/it)\u001b[39m\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:26:43 Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0] 1.8194634\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:27:16 Float32[1.0, 0.0, 0.0, 0.0, 0.0, 0.0] 1.8205892\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:28:24 Float32[0.0, 1.0, 0.0, 0.0, 0.0, 0.0] 1.8194634\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:29:59 Float32[0.0, 1.0, 1.0, 0.0, 0.0, 0.0] 1.834349\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:31:11 Float32[0.0, 1.0, 0.0, 1.0, 0.0, 0.0] 1.8874848\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:32:23 Float32[0.0, 1.0, 0.0, 0.0, 1.0, 0.0] 1.8164439\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:33:30 Float32[0.0, 1.0, 0.0, 0.0, 1.0, 1.0] 1.8163617\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:34:32 Float32[0.33333334, 0.33333334, 0.33333334, -1.0, 0.6666667, 0.33333334] 2.0664165\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:35:56 Float32[0.083333336, 0.8333333, 0.083333336, 0.5, 0.16666667, 0.083333336] 1.8405867\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:37:21 Float32[0.25, 0.5, 0.25, -0.5, 0.5, 0.25] 1.8696747\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:38:57 Float32[0.125, 0.75, 0.125, 0.25, 0.25, 0.125] 1.8246605\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:40:22 Float32[0.375, 0.25, -0.9583333, 0.083333336, 0.75, 0.375] 1.8195416\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:41:46 Float32[0.33333334, 0.33333334, -0.44444445, -0.22222222, 0.6666667, 0.33333334] 1.8266193\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:43:20 Float32[0.17708333, 0.6458333, -0.017361112, 0.13194445, 0.35416666, 0.17708333] 1.8205378\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:45:40 Float32[-0.8159722, 1.2986112, -0.3252315, 0.07175926, 1.0347222, 0.5173611] 1.8176365\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:47:38 Float32[-0.32407406, 0.8703704, -0.41049382, -0.08024691, 0.9074074, 0.4537037] 1.818745\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:49:58 Float32[-0.75501543, 1.4729939, 0.71309155, -0.08616255, 0.5640432, 0.2820216] 1.8248117\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:51:35 Float32[0.09249614, 0.55574846, -0.5404771, 0.040959362, 0.7035108, 0.3517554] 1.8173286\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:53:31 Float32[-0.34918338, 0.57491, -0.4254008, 0.010823903, 1.5485468, 0.7742734] 1.8163278\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:55:40 Float32[-0.5237751, 0.36236498, -0.6381012, 0.016235854, 2.3228202, 1.1614101] 1.8154224\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:57:21 Float32[-0.5237751, 1.6956983, -0.6381012, 0.016235854, 2.3228202, 1.1614101] 1.8154224\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 18:59:20 Float32[-0.26626801, 1.1004373, -0.3034765, 0.12864369, 1.887217, 0.9436085] 1.8168136\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:00:43 Float32[0.4088649, 0.6061385, -0.3814872, -0.004401006, 2.0440671, 1.0220336] 1.8152602\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:01:45 Float32[1.0212834, 0.25990227, -0.40961507, -0.04248114, 2.5487397, 1.2743698] 1.8149065\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:03:42 Float32[-0.19000772, 1.2503858, -0.12262089, -0.001414609, 2.990355, 1.4951775] 1.8071893\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:06:01 Float32[-0.33125964, 1.5977045, 0.08630723, -0.022601595, 4.133777, 2.0668886] 1.8237187\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:07:26 Float32[0.19417652, 0.75567985, -0.29933628, -0.1324517, 2.1743612, 1.0871806] 1.8170483\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:09:14 Float32[-0.15115687, 1.0142479, -0.30244145, 0.06336984, 1.9590031, 0.97950155] 1.8150897\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:11:00 Float32[-0.1224771, 0.8608664, -0.7036266, 0.017315267, 3.381246, 2.3572898] 1.8133885\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:12:44 Float32[-0.16330281, 0.81448853, -0.9381688, 0.023087023, 4.174995, 1.8097196] 1.8144155\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:14:06 Float32[0.4806297, 1.6028315, -0.40009013, 0.009134892, 3.469566, 1.8644127] 1.8085654\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:15:17 Float32[0.8154313, 0.23854248, -0.3207531, 0.006767905, 3.8518147, 2.0987468] 1.8056681\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:16:17 Float32[1.4850345, -0.4900354, -0.16207905, 0.0020339305, 4.616312, 2.5674152] 1.8135927\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:17:40 Float32[0.76500916, 0.6614244, -0.6625167, -0.059233394, 4.846569, 2.653737] 1.8072895\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:19:58 Float32[-0.49285594, 1.5496107, -0.63964367, 0.0410335, 5.0227757, 2.8186579] 1.8068901\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:21:17 Float32[0.58187926, 1.2400652, -0.011581579, -0.018552503, 3.6791139, 2.619621] 1.8093703\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:22:38 Float32[0.77583903, 1.320087, -0.015442106, -0.02473667, 4.572152, 2.1594946] 1.8245869\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:24:11 Float32[0.10210193, 0.9756715, -0.5315805, 0.0068022828, 3.6789725, 2.3078408] 1.8093445\n",
      "\u001b[38;5;6m\u001b[1m[ \u001b[22m\u001b[39m\u001b[38;5;6m\u001b[1mInfo: \u001b[22m\u001b[39m20220604 19:25:55 Float32[-0.08844312, 0.8527569, -0.8808201, 0.019582694, 4.2742367, 1.7932367] 1.8129133\n"
     ]
    }
   ],
   "source": [
    "hp_subset = @set hp.num_users = Int(round(num_users() * 0.1))\n",
    "λ = optimize_hyperparams(hp_subset; max_evals = 100)\n",
    "@info \"THE BEST HYPERPARAMETERS ARE $λ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb07646-cedd-432e-89d3-b4b1c20af139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with the full dataset and with looser early-stopping rules\n",
    "m, loss = train_model(create_hyperparams(hp, λ); max_checkpoints = 100, epochs_per_checkpoint = 1, patience = 10)\n",
    "@info loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cbfe4-1de9-4b3e-8828-0d4812e8df3c",
   "metadata": {},
   "source": [
    "## Retrain User Embeddings\n",
    "* To minimize training/serving skew, we train the model the same\n",
    "  way we will train it during inference\n",
    "* This means reinitializing the user embeddings, freezing all other layers,\n",
    "  and fine-tuning the user embeddings\n",
    "* During serving, we will determine a new user's embedding\n",
    "  by training with the same hyperparameters and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f716d-246d-4d28-a684-b0e7621e9bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function retrain_user_embeddings!(hyp, m)\n",
    "    if hyp.model == \"user_item_biases\"\n",
    "        m[1].weight .= Flux.Embedding(hyp.num_users => 1).weight\n",
    "        ps = Flux.params(m[1])\n",
    "        stopper = early_stopper(max_iters = 100, patience = 10)\n",
    "        loss = Inf\n",
    "\n",
    "        global G = hyp\n",
    "        while (!stop!(stopper, loss))\n",
    "            train_epoch!(m, ps, opt)\n",
    "            loss = split_loss(m, \"validation\")\n",
    "            @info loss\n",
    "        end\n",
    "        global G = nothing\n",
    "\n",
    "        epochs = stopper.iters - stopper.iters_without_improvement\n",
    "    else\n",
    "        @assert false\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d76ef5-0654-448e-9080-5f3dcd553d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_model(hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd719b-7f8b-491e-bafa-8aa56a9545e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c1cdd-01f6-49da-b0d4-241c68909355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # returns the preimage of the index -> split.user[index] mapping\n",
    "# # this is primarily a performance optimization\n",
    "# @memoize function user_to_output_indices(split)\n",
    "#     users = filter_users(get_split(split, implicit), G.num_users).user\n",
    "#     user_to_output_idxs = [Dict() for t = 1:Threads.nthreads()]\n",
    "#     @tprogress Threads.@threads for j = 1:length(users)\n",
    "#         u = users[j]\n",
    "#         t = Threads.threadid()\n",
    "#         if u ∉ keys(user_to_output_idxs[t])\n",
    "#             user_to_output_idxs[t][u] = []\n",
    "#         end\n",
    "#         push!(user_to_output_idxs[t][u], j)\n",
    "#     end\n",
    "#     merge(vcat, user_to_output_idxs...)\n",
    "# end;\n",
    "\n",
    "# # returns a ratins dataset of predicted ratings\n",
    "# function evaluate(m, split)\n",
    "#     # get model inputs\n",
    "#     user_to_output_idxs = user_to_output_indices(split)\n",
    "#     df = filter_users(get_split(split, implicit), G.num_users)\n",
    "#     users = df.user\n",
    "#     items = df.item\n",
    "#     epoch = get_epoch(split)\n",
    "\n",
    "#     # compute predictions    \n",
    "#     batch_size = G.batch_size\n",
    "#     activation = G.implicit ? softmax : identity\n",
    "#     ratings = zeros(Float32, length(users))\n",
    "#     @showprogress for iter = 1:Int(ceil(G.num_users / batch_size))\n",
    "#         batch, sampled_users = get_batch(epoch, iter, batch_size)\n",
    "#         alpha = activation(m(batch[1][1])) |> cpu\n",
    "\n",
    "#         for j = 1:length(sampled_users)\n",
    "#             u = sampled_users[j]\n",
    "#             if u in keys(user_to_output_idxs)\n",
    "#                 for output_idx in user_to_output_idxs[u]\n",
    "#                     ratings[output_idx] = alpha[items[output_idx], j]\n",
    "#                 end\n",
    "#             end\n",
    "#         end\n",
    "#     end\n",
    "\n",
    "#     RatingsDataset(user = users, item = items, rating = ratings)\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327642b-9991-44d5-ab0c-7ce6033d461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function make_prediction(sparse_preds, users, items)\n",
    "#     preds = zeros(length(users))\n",
    "#     @tprogress Threads.@threads for j = 1:length(preds)\n",
    "#         preds[j] = sparse_preds[users[j], items[j]]\n",
    "#     end\n",
    "#     preds\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d999f4-f2b4-4d71-944b-754aead7de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function save_model(model_path, hyperparams::Hyperparams, outdir)\n",
    "#     global G = hyperparams\n",
    "#     BSON.@load model_path m\n",
    "#     training = evaluate(m, \"training\")\n",
    "#     validation = evaluate(m, \"validation\")\n",
    "#     test = evaluate(m, \"test\")\n",
    "#     df = reduce(cat, [training, validation, test])\n",
    "#     sparse_preds = sparse(df.user, df.item, df.rating)\n",
    "\n",
    "#     write_predictions(\n",
    "#         (users, items) -> make_prediction(sparse_preds, users, items),\n",
    "#         residual_alphas = G.validation_residuals,\n",
    "#         outdir = outdir,\n",
    "#         implicit = G.train_implicit_model,\n",
    "#     )\n",
    "#     params = to_dict(G)\n",
    "#     params[\"model\"] = model_path\n",
    "#     write_params(params, outdir = outdir)\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b887e-756b-440e-bbd6-b2842fd184b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function fit(hyperparams::Hyperparams, outdir)\n",
    "#     redirect_logging(\"../../data/alphas/$outdir\")\n",
    "#     @info string(hyperparams)\n",
    "#     model_path = train_model(hyperparams)\n",
    "#     save_model(model_path, hyperparams, outdir)\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44a095-a0c3-43b0-ae5b-a1a4f76de27a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# hyperparams = Hyperparams(\n",
    "#     use_derived_features = true,\n",
    "#     train_implicit_model = true,\n",
    "#     activation = \"relu\",\n",
    "#     autoencode = true,\n",
    "#     batch_size = 128,\n",
    "#     dropout_perc = 0.5,\n",
    "#     dropout_rescale = false,\n",
    "#     layers = [512, 256, 128, 256, 512],\n",
    "#     l2penalty = 1e-5,\n",
    "#     learning_rate = 0.001,\n",
    "#     optimizer = \"ADAM\",\n",
    "#     patience = 10,\n",
    "#     sampling_weight_scheme = \"linear\",\n",
    "#     training_residuals = [\"UserItemBiases\"],\n",
    "#     training_weight_scheme = \"linear\",\n",
    "#     use_residualized_validation_loss = false,\n",
    "#     validation_residuals = [\"UserItemBiases\"],\n",
    "#     validation_weight_scheme = \"constant\",\n",
    "#     seed = 20220501 * hash(name),\n",
    "# )\n",
    "\n",
    "# #fit(hyperparams, \"GNN.Rating.Test.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a5b32-b6a4-4871-ae16-9bd0397aa247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
