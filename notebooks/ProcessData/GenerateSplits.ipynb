{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf03b87-063f-46fe-b392-0d6399dca361",
   "metadata": {},
   "source": [
    "# Generate Splits\n",
    "* We split the dataset into explicit interactions, implicit interactions, and plan-to-watch interactions\n",
    "* Each of the above splits is further separated into a training, validation, and test split\n",
    "* If a user has over 20 interactions, then all but the most recent 10 will be in the training split\n",
    "* If a user has fewer than 20 interactions, then half of their interactions will be in the training split\n",
    "* The remaining interactions are randomly split betweeen the validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1ceb4-d65a-410a-b876-93c491c0cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a0836-bb2b-4956-b075-49dbf38aa079",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"../../data/processed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a8f49-53b5-4106-8fe5-84ba572eead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"../../data/splits\"\n",
    "os.makedirs(outdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0e857-4540-4732-a475-9f88b8c79fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa6376f-09f3-4b44-8117-9771371572d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_by_user(file, num_shards):\n",
    "    try:\n",
    "        outfiles = []\n",
    "        for i in range(num_shards):\n",
    "            outfiles.append(open(f\"{file}.shard.{i}\", \"w\"))\n",
    "        with open(file, \"r\") as in_file:\n",
    "            header = False\n",
    "            for line in tqdm(in_file):\n",
    "                if not header:\n",
    "                    header = True\n",
    "                    for f in outfiles:\n",
    "                        f.write(line)\n",
    "                    continue\n",
    "                fields = line.strip().split(\",\")\n",
    "                user = fields[0]\n",
    "                outfiles[hash(user) % num_shards].write(line)\n",
    "    finally:\n",
    "        for f in outfiles:\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a1ed8-3bf5-4571-b02d-b2494298e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_sort(input_fn, output_fn):\n",
    "    user_anime_lists = pd.read_csv(input_fn)\n",
    "    user_anime_lists[\"rng\"] = [random.random() for _ in range(len(user_anime_lists))]\n",
    "    user_anime_lists = user_anime_lists.sort_values(\n",
    "        by=[\"username\", \"timestamp\", \"rng\"]\n",
    "    ).reset_index(drop=True)\n",
    "    user_anime_lists = user_anime_lists.drop(\"rng\", axis=1)\n",
    "    user_anime_lists[\"count\"] = 1\n",
    "    user_anime_lists[\"order\"] = user_anime_lists.groupby(\"username\")[\"count\"].cumsum()\n",
    "    user_anime_lists = user_anime_lists.drop(\"count\", axis=1)\n",
    "    user_anime_lists.to_csv(output_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf60acd-b530-448d-bd57-581c4009664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharded_temporal_sort(input_fn, output_fn, num_shards=16):\n",
    "    shard_by_user(input_fn, num_shards)\n",
    "    for i in tqdm(range(num_shards)):\n",
    "        temporal_sort(f\"{input_fn}.shard.{i}\", f\"{output_fn}.shard.{i}\")\n",
    "        os.remove(f\"{input_fn}.shard.{i}\")\n",
    "    with open(output_fn, \"w\") as outfile:\n",
    "        for i in tqdm(range(num_shards)):\n",
    "            fn = f\"{output_fn}.shard.{i}\"\n",
    "            with open(fn, \"r\") as infile:\n",
    "                header = False\n",
    "                for line in infile:\n",
    "                    if not header:\n",
    "                        header = True\n",
    "                        if i == 0:\n",
    "                            outfile.write(line)\n",
    "                        continue\n",
    "                    outfile.write(line)\n",
    "            os.remove(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec474dc-e9c2-49d1-a912-07a02a85dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_counts(input_fn):\n",
    "    user_to_count = {}\n",
    "    with open(input_fn, \"r\") as in_file:\n",
    "        header = False\n",
    "        for line in tqdm(in_file):\n",
    "            if not header:\n",
    "                header = True\n",
    "                continue\n",
    "            fields = line.strip().split(\",\")\n",
    "            user = fields[0]\n",
    "            if user not in user_to_count:\n",
    "                user_to_count[user] = 0\n",
    "            user_to_count[user] += 1\n",
    "    return user_to_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f40a8-8747-40a9-92da-8c0778b33a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the input file into a training set and a test set\n",
    "def temporal_split(input_fn, train_fn, test_fn, test_samples_per_user, p_rampup):\n",
    "    user_counts = get_user_counts(input_fn)\n",
    "    seen_counts = {u: 0 for u in user_counts}\n",
    "    with open(input_fn, \"r\") as in_file, open(train_fn, \"w\") as training, open(\n",
    "        test_fn, \"w\"\n",
    "    ) as test:\n",
    "        header = False\n",
    "        for line in tqdm(in_file):\n",
    "            if not header:\n",
    "                header = True\n",
    "                training.write(line)\n",
    "                test.write(line)\n",
    "                continue\n",
    "            username = line.strip().split(\",\")[0]\n",
    "            if user_counts[username] * (1 - p_rampup) >= test_samples_per_user:\n",
    "                to_training_split = (\n",
    "                    seen_counts[username] + test_samples_per_user\n",
    "                    < user_counts[username]\n",
    "                )\n",
    "            else:\n",
    "                to_training_split = (\n",
    "                    seen_counts[username] + random.random()\n",
    "                    < user_counts[username] * p_rampup\n",
    "                )\n",
    "            if to_training_split:\n",
    "                training.write(line)\n",
    "            else:\n",
    "                test.write(line)\n",
    "            seen_counts[username] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d872c37-a89e-4efb-8189-58e59c943da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(input_fn, train_fn, test_fn, p_training):\n",
    "    with open(input_fn, \"r\") as in_file, open(train_fn, \"w\") as training, open(\n",
    "        test_fn, \"w\"\n",
    "    ) as test:\n",
    "        header = False\n",
    "        for line in tqdm(in_file):\n",
    "            if not header:\n",
    "                header = True\n",
    "                training.write(line)\n",
    "                test.write(line)\n",
    "                continue\n",
    "            if random.random() < p_training:\n",
    "                training.write(line)\n",
    "            else:\n",
    "                test.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5209b3-e097-4796-9dab-683971484de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scans every line of the input file. If the line satisfies the\n",
    "# condition, then it is removed from the input file and written\n",
    "# to the output file\n",
    "def subset(input_fn, output_fn, condition):\n",
    "    with open(input_fn, \"r\") as in_file, open(output_fn, \"w\") as out_file:\n",
    "        header = False\n",
    "        for line in tqdm(in_file):\n",
    "            if not header:\n",
    "                header = True\n",
    "                out_file.write(line)\n",
    "                continue\n",
    "            if condition(line):\n",
    "                out_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519843b7-f8c2-4e8a-be36-016de0b8f68a",
   "metadata": {},
   "source": [
    "## Construct training/validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80de47-da9d-429e-984b-2e21fdc7f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporal_splits():\n",
    "    outfn = \"user_anime_list_sorted\"\n",
    "    sharded_temporal_sort(\n",
    "        os.path.join(source_dir, \"user_anime_list.csv\"),\n",
    "        os.path.join(outdir, f\"{outfn}.csv\"),\n",
    "    )\n",
    "    temporal_split(\n",
    "        os.path.join(outdir, f\"{outfn}.csv\"),\n",
    "        os.path.join(outdir, f\"{outfn}_training.csv\"),\n",
    "        os.path.join(outdir, f\"{outfn}_valtest.csv\"),\n",
    "        10,\n",
    "        0.5,\n",
    "    )\n",
    "    random_split(\n",
    "        os.path.join(outdir, f\"{outfn}_valtest.csv\"),\n",
    "        os.path.join(outdir, f\"{outfn}_validation.csv\"),\n",
    "        os.path.join(outdir, f\"{outfn}_test.csv\"),\n",
    "        0.5,\n",
    "    )\n",
    "    os.remove(os.path.join(outdir, f\"{outfn}_valtest.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ef704-e06a-45a0-987c-263721df50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_splits(split):\n",
    "    with open(os.path.join(outdir, f\"user_anime_list_sorted_{split}.csv\")) as f:\n",
    "        first_line = f.readline()\n",
    "    fields = first_line.strip().split(\",\")\n",
    "    score_col = fields.index(\"score\")\n",
    "    status_col = fields.index(\"status\")\n",
    "    subset(\n",
    "        os.path.join(outdir, f\"user_anime_list_sorted_{split}.csv\"),\n",
    "        os.path.join(outdir, f\"ptw_{split}.csv\"),\n",
    "        lambda x: x.strip().split(\",\")[status_col] == \"1\",\n",
    "    )\n",
    "    subset(\n",
    "        os.path.join(outdir, f\"user_anime_list_sorted_{split}.csv\"),\n",
    "        os.path.join(outdir, f\"implicit_{split}.csv\"),\n",
    "        lambda x: float(x.strip().split(\",\")[score_col]) == 0\n",
    "        and x.strip().split(\",\")[status_col] != \"1\",\n",
    "    )\n",
    "    subset(\n",
    "        os.path.join(outdir, f\"user_anime_list_sorted_{split}.csv\"),\n",
    "        os.path.join(outdir, f\"explicit_{split}.csv\"),\n",
    "        lambda x: float(x.strip().split(\",\")[score_col]) != 0\n",
    "        and x.strip().split(\",\")[status_col] != \"1\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66573a69-5238-4e89-b19c-fbeff9bfea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_ids(input_fn):\n",
    "    max_userid = -1\n",
    "    max_itemid = -1\n",
    "    with open(input_fn, \"r\") as in_file:\n",
    "        header = False\n",
    "        for line in tqdm(in_file):\n",
    "            if not header:\n",
    "                header = True\n",
    "                continue\n",
    "            fields = line.strip().split(\",\")\n",
    "            userid = fields[0]\n",
    "            itemid = fields[1]\n",
    "            userid, itemid = int(userid), int(itemid)\n",
    "            max_userid = max(max_userid, int(userid))\n",
    "            max_itemid = max(max_itemid, int(itemid))\n",
    "    return max_userid, max_itemid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935eac6b-2160-4c75-b8fa-4428af4a1425",
   "metadata": {},
   "source": [
    "## Write splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65525017-7a5e-444b-8c96-6204cacf2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_temporal_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5df27b-f77e-4b9a-a6c1-10a39ead09b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for split in [\"training\", \"validation\", \"test\"]:\n",
    "    generate_content_splits(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24475ca-b926-4edf-a1d7-13dc972fd71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_userid, max_itemid = get_max_ids(os.path.join(outdir, \"user_anime_list_sorted.csv\"))\n",
    "with open(os.path.join(source_dir, \"uid_encoding.csv\"), \"w\") as out_file:\n",
    "    out_file.write(f\"max_userid,{max_userid}\\n\")\n",
    "    out_file.write(f\"max_itemid,{max_itemid}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
