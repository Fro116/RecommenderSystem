{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item Correlations\n",
    "* Computes the item correlation matrix and saves it to disk\n",
    "* The correlation two items is given by the adjusted cosine similarity of user scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../data/processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c07c9074f57e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user_anime_lists.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"anime_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"username\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"score\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'anime_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "df = pickle.load(open(\"user_anime_lists.pkl\", \"rb\"))\n",
    "df = df[[\"anime_id\", \"username\", \"score\"]]\n",
    "df = df.set_index('anime_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache()\n",
    "def adj_cos_corr_denominator():\n",
    "    return df.groupby(\"anime_id\").apply(\n",
    "        lambda x: np.sqrt(np.dot(x[\"score\"], x[\"score\"]))\n",
    "    )\n",
    "\n",
    "\n",
    "def get_corrs(df, anime_ids):\n",
    "    item_subset = df.loc[anime_ids].reset_index().merge(df.reset_index(), on=\"username\")\n",
    "\n",
    "    # compute the adjusted cosine correlation\n",
    "    item_subset[\"adj_cor_numerator\"] = item_subset[\"score_x\"] * item_subset[\"score_y\"]\n",
    "    adj_cos_corr_numerator = item_subset.groupby([\"anime_id_x\", \"anime_id_y\"])[\n",
    "        \"adj_cor_numerator\"\n",
    "    ].sum()\n",
    "    x_length = adj_cos_corr_denominator()[anime_ids]\n",
    "    x_length.index.rename(\"anime_id_x\", inplace=True)\n",
    "    y_length = adj_cos_corr_denominator()\n",
    "    y_length.index.rename(\"anime_id_y\", inplace=True)\n",
    "    adj_cos_corr = adj_cos_corr_numerator / x_length / y_length\n",
    "    adj_cos_corr = pd.DataFrame(adj_cos_corr, columns=[\"corr\"]).dropna()\n",
    "\n",
    "    # We approximate the variance as the variance for pearson correlation.\n",
    "    # see https://www.jstor.org/stable/2277400?seq=1\n",
    "    corr_sizes = item_subset.groupby([\"anime_id_x\", \"anime_id_y\"]).size()\n",
    "    corrs = adj_cos_corr.merge(\n",
    "        pd.DataFrame(corr_sizes, columns=[\"size\"]), on=[\"anime_id_x\", \"anime_id_y\"]\n",
    "    )\n",
    "    corrs = corrs.loc[lambda x: x[\"size\"] > 2]\n",
    "    corrs[\"corr_var\"] = (1 - corrs[\"corr\"] * corrs[\"corr\"]) ** 2 / (corrs[\"size\"] - 2)\n",
    "    corrs = corrs.dropna()\n",
    "    return corrs[[\"corr\", \"corr_var\", \"size\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output directory\n",
    "outdir = \"item_correlations\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 632/632 [3:35:46<00:00, 20.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Store correlations on disk. We need to do this in chunks to prevent running out of memory\n",
    "# Note: The first iterations take longer than later iterations because of cache warming\n",
    "# Note: increasing parallelism will make the program run faster but will use more memory\n",
    "anime_ids = sorted(list(set(df.index)))\n",
    "parallelism = 25\n",
    "chunks = np.array_split(anime_ids, int(len(anime_ids) / parallelism))\n",
    "for i, chunk in tqdm(enumerate(chunks), total=len(chunks)):\n",
    "    get_corrs(df, chunk).to_pickle(os.path.join(outdir, f\"{i}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 632/632 [00:16<00:00, 38.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# combine all chunks into a single file\n",
    "corr_dfs = []\n",
    "for item in tqdm(os.listdir(outdir)):\n",
    "    if not re.match(\"[0-9]+.pkl\", item):\n",
    "        print(f\"Found unrecognized file {item} in {outdir}\")\n",
    "        continue\n",
    "    path = os.path.join(outdir, item)\n",
    "    corr_dfs.append(pickle.load(open(path, \"rb\")))\n",
    "corr_df = pd.concat(corr_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = corr_df.astype(np.float32)\n",
    "corr_df = corr_df.loc[\n",
    "        lambda x: x.index.get_level_values(\"anime_id_x\")\n",
    "        != x.index.get_level_values(\"anime_id_y\")\n",
    "]\n",
    "corr_df['similarity'] = corr_df['corr'].abs()\n",
    "corr_df = corr_df.sort_values(by='similarity')\n",
    "corr_df.to_pickle(\"item_correlations.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
